Sys.sleep(0.25)
print(record_df$EIS.Number[i])
htmlNodes = record_df$eis_url[i] %>% read_html()  %>% html_nodes('a')
file_url = grep('downloadAttachment',htmlNodes %>% html_attr('href') ,value=T)
file_names = {htmlNodes %>% html_text()}[grepl('downloadAttachment',htmlNodes %>% html_attr('href'))]
file_names = gsub('~|/','-',file_names)
file_names = gsub('\\s{1,}','_',file_names)
file_names
file_url
file.info(file_url)
url_ok(file_url[1])
paste0(base_page,file_url[1])
url_ok(paste0(base_page,file_url[1]))
url_ok(paste0(base_page,file_url[2]))
file_url = paste0(base_page,file_url[2])
curl_cmd = paste('curl -X HEAD -i', file_url)
system_cmd = paste(curl_cmd, '|grep Content-Length |cut -d : -f 2')
system_cmd
b <- system(system_cmd, intern = TRUE)
b
j = 1
subd = str_extract(record_df$EIS.Number[i],'^[0-9]{4}')
if(file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
tdf
file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))
!file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))
paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_'))
!file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))
temp_name = paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_'))
download = tryCatch(httr::GET(paste0(base_page,file_url[j]), verbose(),write_disk(temp_name), overwrite=TRUE),error=function(e) NULL)
file.size(temp_name)
file_names
file_names[j]
file_names[j] %in% file_names[-j]
i = 1670
Sys.sleep(0.25)
print(record_df$EIS.Number[i])
htmlNodes = record_df$eis_url[i] %>% read_html()  %>% html_nodes('a')
file_url = grep('downloadAttachment',htmlNodes %>% html_attr('href') ,value=T)
file_names = {htmlNodes %>% html_text()}[grepl('downloadAttachment',htmlNodes %>% html_attr('href'))]
file_names = gsub('~|/','-',file_names)
file_names = gsub('\\s{1,}','_',file_names)
if(length(file_url)==0){next}
for (j in 1:length(file_url)){
subd = str_extract(record_df$EIS.Number[i],'^[0-9]{4}')
if(!dir.exists(paste0(file_storage,subd))){dir.create(paste0(file_storage,subd))}
if(file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
temp_name = paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_'))
download = tryCatch(httr::GET(paste0(base_page,file_url[j]), verbose(),write_disk(temp_name), overwrite=TRUE),error=function(e) NULL)
temp_info = file.info(temp_name)
if(is.null(download)){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!is.null(download)){
temp_info$size = file.info(temp_name)
if(file.size(temp_name)==0&all(duplicated(file_names))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(file.size(temp_name)==0&file_names[j] %in% file_names[-j]){
next
}
if(file.size(temp_name)>0){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
}
}
doc_df = rbind(doc_df,tdf,use.names = T,fill = T)
}
for (i in 16701)#:nrow(record_df))
{
Sys.sleep(0.25)
print(record_df$EIS.Number[i])
htmlNodes = record_df$eis_url[i] %>% read_html()  %>% html_nodes('a')
file_url = grep('downloadAttachment',htmlNodes %>% html_attr('href') ,value=T)
file_names = {htmlNodes %>% html_text()}[grepl('downloadAttachment',htmlNodes %>% html_attr('href'))]
file_names = gsub('~|/','-',file_names)
file_names = gsub('\\s{1,}','_',file_names)
if(length(file_url)==0){next}
for (j in 1:length(file_url)){
subd = str_extract(record_df$EIS.Number[i],'^[0-9]{4}')
if(!dir.exists(paste0(file_storage,subd))){dir.create(paste0(file_storage,subd))}
if(file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
temp_name = paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_'))
download = tryCatch(httr::GET(paste0(base_page,file_url[j]), verbose(),write_disk(temp_name), overwrite=TRUE),error=function(e) NULL)
temp_info = file.info(temp_name)
if(is.null(download)){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!is.null(download)){
temp_info$size = file.info(temp_name)
if(file.size(temp_name)==0&all(duplicated(file_names))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(file.size(temp_name)==0&file_names[j] %in% file_names[-j]){
file.remove(temp_name)
next
}
if(file.size(temp_name)>0){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
}
}
doc_df = rbind(doc_df,tdf,use.names = T,fill = T)
}
write.csv(x = doc_df,file = paste0('enepa_repository/meta_data/eis_document_record','.csv'),row.names = F)
}
i = 1670
Sys.sleep(0.25)
print(record_df$EIS.Number[i])
htmlNodes = record_df$eis_url[i] %>% read_html()  %>% html_nodes('a')
file_url = grep('downloadAttachment',htmlNodes %>% html_attr('href') ,value=T)
file_names = {htmlNodes %>% html_text()}[grepl('downloadAttachment',htmlNodes %>% html_attr('href'))]
file_names = gsub('~|/','-',file_names)
file_names = gsub('\\s{1,}','_',file_names)
if(length(file_url)==0){next}
for (j in 1:length(file_url)){
subd = str_extract(record_df$EIS.Number[i],'^[0-9]{4}')
if(!dir.exists(paste0(file_storage,subd))){dir.create(paste0(file_storage,subd))}
if(file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
temp_name = paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_'))
download = tryCatch(httr::GET(paste0(base_page,file_url[j]), verbose(),write_disk(temp_name), overwrite=TRUE),error=function(e) NULL)
temp_info = file.info(temp_name)
if(is.null(download)){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!is.null(download)){
temp_info$size = file.info(temp_name)
if(file.size(temp_name)==0&all(duplicated(file_names))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(file.size(temp_name)==0&file_names[j] %in% file_names[-j]){
file.remove(temp_name)
next
}
if(file.size(temp_name)>0){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
}
}
doc_df = rbind(doc_df,tdf,use.names = T,fill = T)
}
write.csv(x = doc_df,file = paste0('enepa_repository/meta_data/eis_document_record','.csv'),row.names = F)
i = which(record_df$EIS.Number=='20150273')
record_df[i,]
i = which(record_df$EIS.Number=='20150287')
record_df[i,]
i = which(record_df$EIS.Number=='20150287')
Sys.sleep(0.25)
print(record_df$EIS.Number[i])
htmlNodes = record_df$eis_url[i] %>% read_html()  %>% html_nodes('a')
file_url = grep('downloadAttachment',htmlNodes %>% html_attr('href') ,value=T)
file_names = {htmlNodes %>% html_text()}[grepl('downloadAttachment',htmlNodes %>% html_attr('href'))]
file_names = gsub('~|/','-',file_names)
file_names = gsub('\\s{1,}','_',file_names)
file_names
for (j in 1:length(file_url)){
subd = str_extract(record_df$EIS.Number[i],'^[0-9]{4}')
if(!dir.exists(paste0(file_storage,subd))){dir.create(paste0(file_storage,subd))}
if(file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
temp_name = paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_'))
download = tryCatch(httr::GET(paste0(base_page,file_url[j]), verbose(),write_disk(temp_name), overwrite=TRUE),error=function(e) NULL)
temp_info = file.info(temp_name)
if(is.null(download)){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!is.null(download)){
temp_info$size = file.info(temp_name)
if(file.size(temp_name)==0&all(duplicated(file_names))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(file.size(temp_name)==0&file_names[j] %in% file_names[-j]){
file.remove(temp_name)
next
}
if(file.size(temp_name)>0){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
}
}
doc_df = rbind(doc_df,tdf,use.names = T,fill = T)
}
write.csv(x = doc_df,file = paste0('enepa_repository/meta_data/eis_document_record','.csv'),row.names = F)
i = which(record_df$EIS.Number=='20150287')
Sys.sleep(0.25)
print(record_df$EIS.Number[i])
htmlNodes = record_df$eis_url[i] %>% read_html()  %>% html_nodes('a')
file_url = grep('downloadAttachment',htmlNodes %>% html_attr('href') ,value=T)
file_names = {htmlNodes %>% html_text()}[grepl('downloadAttachment',htmlNodes %>% html_attr('href'))]
file_names = gsub('~|/','-',file_names)
file_names = gsub('\\s{1,}','_',file_names)
if(length(file_url)==0){next}
for (j in 1:length(file_url)){
subd = str_extract(record_df$EIS.Number[i],'^[0-9]{4}')
if(!dir.exists(paste0(file_storage,subd))){dir.create(paste0(file_storage,subd))}
if(file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
temp_name = paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_'))
download = tryCatch(httr::GET(paste0(base_page,file_url[j]), verbose(),write_disk(temp_name), overwrite=TRUE),error=function(e) NULL)
temp_info = file.info(temp_name)
if(is.null(download)){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!is.null(download)){
temp_info$size = file.info(temp_name)
if(file.size(temp_name)==0&all(duplicated(file_names))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(file.size(temp_name)==0&file_names[j] %in% file_names[-j]){
file.remove(temp_name)
next
}
if(file.size(temp_name)>0){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
}
}
doc_df = rbind(doc_df,tdf,use.names = T,fill = T)
}
write.csv(x = doc_df,file = paste0('enepa_repository/meta_data/eis_document_record','.csv'),row.names = F)
finfo = file.size(pdf_list)
if(!require(data.table)){install.packages('data.table');require(data.table)}
if(!require(stringr)){install.packages('stringr');require(stringr)}
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(doParallel)){install.packages('doParallel');require(doParallel)}
if(!require(pdftools)){install.packages('pdftools');require(pdftools)}
floc = 'enepa_repository/documents/'
dir.create('enepa_repository/text_as_datatable')
tfloc = 'enepa_repository/text_as_datatable/'
docs = list.files(floc,recursive = T,full.names = T)
text_docs = list.files(tfloc,recursive = T,full.names = T)
doc_record = fread('enepa_repository/meta_data/eis_document_record.csv')
mcores = detectCores() / 2
doc_record = doc_record[PDF==T&!BAD_FILE,]
doc_record$subd = str_extract(doc_record$File_Name,'^[0-9]{4}')
text_names = paste0(tfloc,'/',doc_record$subd,'/',gsub('PDF$|pdf$','txt',doc_record$File_Name))
flt = list.files('enepa_repository/text_as_datatable/2013',full.names = T)
finfo = file.info(flt)
library(lubridate)
old = month(ymd_hms(finfo$mtime))!=5
pdf_list = list.files('enepa_repository/documents/',recursive = T,full.names = T,pattern = '^2020|^201[3-9]')
#pdf_list = pdf_list[grepl('^201[3-9]',basename(pdf_list))]
pdf_list = gsub('_{2,}','_',pdf_list)
text_list = list.files('enepa_repository/text_as_datatable/',recursive = T,full.names = T)
text_list = text_list[grepl('^2020|^201[3-9]',basename(text_list))]
#docs_unconverted = doc_record[!file.exists(text_names),]
#docs_unconverted[EIS.Number=='201600457',]
docs_unconverted <- pdf_list[!gsub('pdf$|PDF$','txt',basename(pdf_list)) %in% basename(text_list)]
docs_unconverted = docs_unconverted[grepl('2020',basename(docs_unconverted))]
finfo = file.size(pdf_list)
file.remove(pdf_list[finfo==0])
#docs_unconverted = doc_record[subd %in% 2013:2018,]
#mclapply(which(!test_names %in% flt),function(i) {
mclapply(1:length(docs_unconverted),function(i) {
rm(id);rm(temp_text);rm(temp_page)
#i = which(documents$FILE_NAME=='41909_95853_FSPLT3_1658799.pdf')
fname = docs_unconverted[i]
tname = gsub('enepa_repository/documents','enepa_repository/text_as_datatable',fname)
tname = gsub('pdf$|PDF$','txt',tname)
#if(file.exists(tname)){next}#temp = fread(tname);temp[,text:=NULL];all_page_dt = rbind(all_page_dt,temp,use.names=T,fill=T);next}
print(i)
temp_text = tryCatch({pdftools::pdf_text(fname)},error = function(e) NULL)
if(!is.null(temp_text) & length(temp_text)>0 & any(temp_text!='')){
temp_page = unlist(sapply(temp_text,function(x) x))
temp_page = gsub('\\s{1,}',' ',temp_page)
temp = data.table::data.table(Page = seq_along(temp_page),text = temp_page,stringsAsFactors = F)
temp$text[nchar(temp$text)>10000] <- ''
#temp = temp[!grepl('\\.{10,}',temp$text),]
temp$text = textclean::replace_non_ascii(temp$text)
#temp = temp[!grepl('^Figure [0-9]',temp$text),]
temp$text = gsub('\\s{1,}$','',temp$text)
#temp = temp[nchar(temp$text)>500,]
temp = temp[!duplicated(text)&text!='',]
#  if(file.exists(tname)&nrow(temp)==0){file.remove(tname)}
if(!file.exists(tname)|file.exists(tname)){fwrite(x = temp,file = tname,sep = '\t')}
}
},mc.cores = mcores,mc.cleanup = T,mc.preschedule = T)
if(!require(data.table)){install.packages('data.table');require(data.table)}
if(!require(stringr)){install.packages('stringr');require(stringr)}
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(doParallel)){install.packages('doParallel');require(doParallel)}
if(!require(pdftools)){install.packages('pdftools');require(pdftools)}
floc = 'enepa_repository/documents/'
dir.create('enepa_repository/text_as_datatable')
tfloc = 'enepa_repository/text_as_datatable/'
docs = list.files(floc,recursive = T,full.names = T)
text_docs = list.files(tfloc,recursive = T,full.names = T)
doc_record = fread('enepa_repository/meta_data/eis_document_record.csv')
mcores = detectCores() / 2
doc_record = doc_record[PDF==T&!BAD_FILE,]
doc_record$subd = str_extract(doc_record$File_Name,'^[0-9]{4}')
text_names = paste0(tfloc,'/',doc_record$subd,'/',gsub('PDF$|pdf$','txt',doc_record$File_Name))
flt = list.files('enepa_repository/text_as_datatable/2013',full.names = T)
finfo = file.info(flt)
library(lubridate)
old = month(ymd_hms(finfo$mtime))!=5
pdf_list = list.files('enepa_repository/documents/',recursive = T,full.names = T,pattern = '^2020|^201[3-9]')
#pdf_list = pdf_list[grepl('^201[3-9]',basename(pdf_list))]
pdf_list = gsub('_{2,}','_',pdf_list)
text_list = list.files('enepa_repository/text_as_datatable/',recursive = T,full.names = T)
text_list = text_list[grepl('^2020|^201[3-9]',basename(text_list))]
#docs_unconverted = doc_record[!file.exists(text_names),]
#docs_unconverted[EIS.Number=='201600457',]
docs_unconverted <- pdf_list[!gsub('pdf$|PDF$','txt',basename(pdf_list)) %in% basename(text_list)]
docs_unconverted = docs_unconverted[grepl('2020',basename(docs_unconverted))]
finfo = file.size(pdf_list)
file.remove(pdf_list[finfo==0])
#docs_unconverted = doc_record[subd %in% 2013:2018,]
#mclapply(which(!test_names %in% flt),function(i) {
mclapply(1:length(docs_unconverted),function(i) {
rm(id);rm(temp_text);rm(temp_page)
#i = which(documents$FILE_NAME=='41909_95853_FSPLT3_1658799.pdf')
fname = docs_unconverted[i]
tname = gsub('enepa_repository/documents','enepa_repository/text_as_datatable',fname)
tname = gsub('pdf$|PDF$','txt',tname)
#if(file.exists(tname)){next}#temp = fread(tname);temp[,text:=NULL];all_page_dt = rbind(all_page_dt,temp,use.names=T,fill=T);next}
print(i)
temp_text = tryCatch({pdftools::pdf_text(fname)},error = function(e) NULL)
if(!is.null(temp_text) & length(temp_text)>0 & any(temp_text!='')){
temp_page = unlist(sapply(temp_text,function(x) x))
temp_page = gsub('\\s{1,}',' ',temp_page)
temp = data.table::data.table(Page = seq_along(temp_page),text = temp_page,stringsAsFactors = F)
temp$text[nchar(temp$text)>10000] <- ''
#temp = temp[!grepl('\\.{10,}',temp$text),]
temp$text = textclean::replace_non_ascii(temp$text)
#temp = temp[!grepl('^Figure [0-9]',temp$text),]
temp$text = gsub('\\s{1,}$','',temp$text)
#temp = temp[nchar(temp$text)>500,]
temp = temp[!duplicated(text)&text!='',]
#  if(file.exists(tname)&nrow(temp)==0){file.remove(tname)}
if(!file.exists(tname)|file.exists(tname)){fwrite(x = temp,file = tname,sep = '\t')}
}
},mc.cores = mcores,mc.cleanup = T,mc.preschedule = T)
# ###two packages that will do most of what you need
library(tidyverse)
library(rvest)
library(rvest)
library(data.table)
library(Rcrawler)
library(tokenizers)
library(rvest)
library(readxl)
library(XML)
library(data.table)
require(RSelenium)
require(seleniumPipes)
library(data.table)
library(RSelenium)
library(httr)
# #progress bar lapply, useful for gauging how long/bad your code is in an apply loop
library(pbapply)
# #page listing forest urls
nfs_site_list = "https://web.archive.org/web/20190701045610/https://www.fs.fed.us/recreation/map/state_list.shtml"
sites = nfs_site_list %>% read_html() %>% html_nodes('a')
site_matches = grep('National Forest$|National Forests$|in Texas$|Management Area|National Gra[s]{1,}land|National Tallgrass Prairie',sites %>% html_text(trim=T),value=F)
text = sites %>% html_text(trim=T)
hrefs = sites %>% html_attr('href')
site_dfs = data.frame(Forest = text[site_matches],url = hrefs[site_matches],stringsAsFactors = F) %>% filter(!duplicated(.))
site_dfs$url = str_extract(site_dfs$url,'[^^]http.+')
#site_dfs$url[!grepl('^http',site_dfs$url)] <- paste0('http://www.fs.fed.us',site_dfs$url[!grepl('^http',site_dfs$url)])
site_dfs = site_dfs[!duplicated(site_dfs),]
site_dfs$url = gsub('^\\/','',site_dfs$url)
site_dfs$url = gsub('http:','https:',site_dfs$url)
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r9/white/" ] <- 'https://www.fs.usda.gov/land/whitemountain'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r5/sequoia" ] <- 'https://www.fs.usda.gov/sequoia'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r3/sfe/"  ] <- 'https://www.fs.usda.gov/santafe'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r1/clearwater/"  ] <- 'https://www.fs.usda.gov/clearwater'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r10/tongass/"   ] <- 'https://www.fs.usda.gov/tongass'
site_dfs$url[site_dfs$url ==  "https://www.fs.fed.us/r1/helena/"  ] <- 'https://www.fs.usda.gov/helena'
# # #some urls redirect to a different page, so use session to find redirect
fsites = site_dfs$url
forest_names_url = pblapply(fsites, function(x) html_session(x)$url)
# #some redirects result in duplicates
forest_names_url = unique(unlist(forest_names_url))
# #
# #
forest_names_url = forest_names_url[!duplicated(gsub('\\/$','',forest_names_url))]
# ##isolate names used on project page urls
forest_name_urls = gsub("\\/","",gsub('main\\/|\\/home|land\\/','',gsub("https://www.fs.usda.gov/",'',forest_names_url)))
forest_name_urls = append(forest_name_urls,'ochoco')
prefix = "https://www.fs.usda.gov/wps/portal/fsinternet/cs/projects/"
suffix = "/landmanagement/projects?archive=1&sortby=1"
proj_archives = paste0(prefix,forest_name_urls,suffix)
# ##isolate names used on project page urls
prefix = "https://www.fs.usda.gov/projects/"
suffix = "/landmanagement/projects"
proj_current = paste0(prefix,forest_name_urls,suffix)
proj_url = data.frame(project_urls = c(proj_archives,proj_current),status = c(rep('Archived',length(proj_archives)),
rep('Current',length(proj_current))),stringsAsFactors = F)
proj_url$nf = gsub('https://www.fs.usda.gov/projects/','',proj_url$project_urls,fixed=T)
proj_url$nf = gsub('/landmanagement/projects','',proj_url$nf,fixed=T)
proj_url$nf = gsub('https://www.fs.usda.gov/wps/portal/fsinternet/cs/projects/','',proj_url$nf,fixed=T)
proj_url$nf = gsub('?archive=1&sortby=1','',proj_url$nf,fixed=T)
base = 'https://www.fs.fed.us/nepa/nepa_home.php'
port = 4567
pJS <- wdman::phantomjs()
#pJS <- wdman::phantomjs()
Sys.sleep(5) # give the binary a moment
remDr <- remoteDriver(port=port)
remDr$open()
remDr$navigate(base)
nf_options = remDr$findElement(using = 'name','forest')
nf_options
sites = nfs_site_list %>% read_html() %>% html_nodes('a')
site_matches = grep('National Forest$|National Forests$|in Texas$|Management Area|National Gra[s]{1,}land|National Tallgrass Prairie',sites %>% html_text(trim=T),value=F)
text = sites %>% html_text(trim=T)
hrefs = sites %>% html_attr('href')
site_dfs = data.frame(Forest = text[site_matches],url = hrefs[site_matches],stringsAsFactors = F) %>% filter(!duplicated(.))
site_dfs$url = str_extract(site_dfs$url,'[^^]http.+')
site_dfs$url
str_extract(site_dfs$url,'[^^]http.+')
str_extract(site_dfs$url,'[^^]http.+')
site_dfs$url = str_extract(site_dfs$url,'[^^]http.+')
#site_dfs$url[!grepl('^http',site_dfs$url)] <- paste0('http://www.fs.fed.us',site_dfs$url[!grepl('^http',site_dfs$url)])
site_dfs = site_dfs[!duplicated(site_dfs),]
site_dfs$url = gsub('^\\/','',site_dfs$url)
site_dfs$url = gsub('http:','https:',site_dfs$url)
site_dfs$url
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r9/white/" ] <- 'https://www.fs.usda.gov/land/whitemountain'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r5/sequoia" ] <- 'https://www.fs.usda.gov/sequoia'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r3/sfe/"  ] <- 'https://www.fs.usda.gov/santafe'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r1/clearwater/"  ] <- 'https://www.fs.usda.gov/clearwater'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r10/tongass/"   ] <- 'https://www.fs.usda.gov/tongass'
site_dfs$url[site_dfs$url ==  "https://www.fs.fed.us/r1/helena/"  ] <- 'https://www.fs.usda.gov/helena'
# # #some urls redirect to a different page, so use session to find redirect
fsites = site_dfs$url
forest_names_url = pblapply(fsites, function(x) html_session(x)$url)
# ###two packages that will do most of what you need
library(tidyverse)
library(rvest)
library(rvest)
library(data.table)
library(Rcrawler)
library(tokenizers)
library(rvest)
library(readxl)
library(XML)
library(data.table)
require(RSelenium)
require(seleniumPipes)
library(data.table)
library(RSelenium)
library(httr)
# #progress bar lapply, useful for gauging how long/bad your code is in an apply loop
library(pbapply)
# #page listing forest urls
nfs_site_list = "https://web.archive.org/web/20190701045610/https://www.fs.fed.us/recreation/map/state_list.shtml"
sites = nfs_site_list %>% read_html() %>% html_nodes('a')
site_matches = grep('National Forest$|National Forests$|in Texas$|Management Area|National Gra[s]{1,}land|National Tallgrass Prairie',sites %>% html_text(trim=T),value=F)
text = sites %>% html_text(trim=T)
hrefs = sites %>% html_attr('href')
site_dfs = data.frame(Forest = text[site_matches],url = hrefs[site_matches],stringsAsFactors = F) %>% filter(!duplicated(.))
site_dfs$url = str_extract(site_dfs$url,'[^^]http.+')
#site_dfs$url[!grepl('^http',site_dfs$url)] <- paste0('http://www.fs.fed.us',site_dfs$url[!grepl('^http',site_dfs$url)])
site_dfs = site_dfs[!duplicated(site_dfs),]
site_dfs$url = gsub('^\\/','',site_dfs$url)
site_dfs$url = gsub('http:','https:',site_dfs$url)
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r9/white/" ] <- 'https://www.fs.usda.gov/land/whitemountain'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r5/sequoia" ] <- 'https://www.fs.usda.gov/sequoia'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r3/sfe/"  ] <- 'https://www.fs.usda.gov/santafe'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r1/clearwater/"  ] <- 'https://www.fs.usda.gov/clearwater'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r10/tongass/"   ] <- 'https://www.fs.usda.gov/tongass'
site_dfs$url[site_dfs$url ==  "https://www.fs.fed.us/r1/helena/"  ] <- 'https://www.fs.usda.gov/helena'
# # #some urls redirect to a different page, so use session to find redirect
fsites = site_dfs$url
forest_names_url = pblapply(fsites, function(x) html_session(x)$url)
# #some redirects result in duplicates
forest_names_url = unique(unlist(forest_names_url))
# #
# #
forest_names_url = forest_names_url[!duplicated(gsub('\\/$','',forest_names_url))]
# ##isolate names used on project page urls
forest_name_urls = gsub("\\/","",gsub('main\\/|\\/home|land\\/','',gsub("https://www.fs.usda.gov/",'',forest_names_url)))
forest_name_urls = append(forest_name_urls,'ochoco')
prefix = "https://www.fs.usda.gov/wps/portal/fsinternet/cs/projects/"
suffix = "/landmanagement/projects?archive=1&sortby=1"
proj_archives = paste0(prefix,forest_name_urls,suffix)
# ##isolate names used on project page urls
prefix = "https://www.fs.usda.gov/projects/"
suffix = "/landmanagement/projects"
proj_current = paste0(prefix,forest_name_urls,suffix)
proj_url = data.frame(project_urls = c(proj_archives,proj_current),status = c(rep('Archived',length(proj_archives)),
rep('Current',length(proj_current))),stringsAsFactors = F)
proj_url$nf = gsub('https://www.fs.usda.gov/projects/','',proj_url$project_urls,fixed=T)
proj_url$nf = gsub('/landmanagement/projects','',proj_url$nf,fixed=T)
proj_url$nf = gsub('https://www.fs.usda.gov/wps/portal/fsinternet/cs/projects/','',proj_url$nf,fixed=T)
proj_url$nf = gsub('?archive=1&sortby=1','',proj_url$nf,fixed=T)
base = 'https://www.fs.fed.us/nepa/nepa_home.php'
port = 4567
pJS <- wdman::phantomjs()
#pJS <- wdman::phantomjs()
Sys.sleep(5) # give the binary a moment
remDr <- remoteDriver(port=port)
remDr$open()
remDr$navigate(base)
nf_options = remDr$findElement(using = 'name','forest')
base
