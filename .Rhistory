<<<<<<< Updated upstream
text_names = paste0(tfloc,'/',doc_record$subd,'/',gsub('PDF$|pdf$','txt',doc_record$File_Name))
flt = list.files('enepa_repository/text_as_datatable/2013',full.names = T)
finfo = file.info(flt)
library(lubridate)
old = month(ymd_hms(finfo$mtime))!=5
pdf_list = list.files('enepa_repository/documents/',recursive = T,full.names = T,pattern = '^2020|^201[3-9]')
#pdf_list = pdf_list[grepl('^201[3-9]',basename(pdf_list))]
pdf_list = gsub('_{2,}','_',pdf_list)
text_list = list.files('enepa_repository/text_as_datatable/',recursive = T,full.names = T)
text_list = text_list[grepl('^2020|^201[3-9]',basename(text_list))]
#docs_unconverted = doc_record[!file.exists(text_names),]
#docs_unconverted[EIS.Number=='201600457',]
docs_unconverted <- pdf_list[!gsub('pdf$|PDF$','txt',basename(pdf_list)) %in% basename(text_list)]
docs_unconverted = docs_unconverted[grepl('2020',basename(docs_unconverted))]
finfo = file.size(pdf_list)
file.remove(pdf_list[finfo==0])
#docs_unconverted = doc_record[subd %in% 2013:2018,]
#mclapply(which(!test_names %in% flt),function(i) {
mclapply(1:length(docs_unconverted),function(i) {
rm(id);rm(temp_text);rm(temp_page)
#i = which(documents$FILE_NAME=='41909_95853_FSPLT3_1658799.pdf')
fname = docs_unconverted[i]
tname = gsub('enepa_repository/documents','enepa_repository/text_as_datatable',fname)
tname = gsub('pdf$|PDF$','txt',tname)
#if(file.exists(tname)){next}#temp = fread(tname);temp[,text:=NULL];all_page_dt = rbind(all_page_dt,temp,use.names=T,fill=T);next}
print(i)
temp_text = tryCatch({pdftools::pdf_text(fname)},error = function(e) NULL)
if(!is.null(temp_text) & length(temp_text)>0 & any(temp_text!='')){
temp_page = unlist(sapply(temp_text,function(x) x))
temp_page = gsub('\\s{1,}',' ',temp_page)
temp = data.table::data.table(Page = seq_along(temp_page),text = temp_page,stringsAsFactors = F)
temp$text[nchar(temp$text)>10000] <- ''
#temp = temp[!grepl('\\.{10,}',temp$text),]
temp$text = textclean::replace_non_ascii(temp$text)
#temp = temp[!grepl('^Figure [0-9]',temp$text),]
temp$text = gsub('\\s{1,}$','',temp$text)
#temp = temp[nchar(temp$text)>500,]
temp = temp[!duplicated(text)&text!='',]
#  if(file.exists(tname)&nrow(temp)==0){file.remove(tname)}
if(!file.exists(tname)|file.exists(tname)){fwrite(x = temp,file = tname,sep = '\t')}
}
},mc.cores = mcores,mc.cleanup = T,mc.preschedule = T)
if(!require(data.table)){install.packages('data.table');require(data.table)}
if(!require(stringr)){install.packages('stringr');require(stringr)}
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(doParallel)){install.packages('doParallel');require(doParallel)}
if(!require(pdftools)){install.packages('pdftools');require(pdftools)}
floc = 'enepa_repository/documents/'
dir.create('enepa_repository/text_as_datatable')
tfloc = 'enepa_repository/text_as_datatable/'
docs = list.files(floc,recursive = T,full.names = T)
text_docs = list.files(tfloc,recursive = T,full.names = T)
doc_record = fread('enepa_repository/meta_data/eis_document_record.csv')
mcores = detectCores() / 2
doc_record = doc_record[PDF==T&!BAD_FILE,]
doc_record$subd = str_extract(doc_record$File_Name,'^[0-9]{4}')
text_names = paste0(tfloc,'/',doc_record$subd,'/',gsub('PDF$|pdf$','txt',doc_record$File_Name))
flt = list.files('enepa_repository/text_as_datatable/2013',full.names = T)
finfo = file.info(flt)
library(lubridate)
old = month(ymd_hms(finfo$mtime))!=5
pdf_list = list.files('enepa_repository/documents/',recursive = T,full.names = T,pattern = '^2020|^201[3-9]')
#pdf_list = pdf_list[grepl('^201[3-9]',basename(pdf_list))]
pdf_list = gsub('_{2,}','_',pdf_list)
text_list = list.files('enepa_repository/text_as_datatable/',recursive = T,full.names = T)
text_list = text_list[grepl('^2020|^201[3-9]',basename(text_list))]
#docs_unconverted = doc_record[!file.exists(text_names),]
#docs_unconverted[EIS.Number=='201600457',]
docs_unconverted <- pdf_list[!gsub('pdf$|PDF$','txt',basename(pdf_list)) %in% basename(text_list)]
docs_unconverted = docs_unconverted[grepl('2020',basename(docs_unconverted))]
finfo = file.size(pdf_list)
file.remove(pdf_list[finfo==0])
#docs_unconverted = doc_record[subd %in% 2013:2018,]
#mclapply(which(!test_names %in% flt),function(i) {
mclapply(1:length(docs_unconverted),function(i) {
rm(id);rm(temp_text);rm(temp_page)
#i = which(documents$FILE_NAME=='41909_95853_FSPLT3_1658799.pdf')
fname = docs_unconverted[i]
tname = gsub('enepa_repository/documents','enepa_repository/text_as_datatable',fname)
tname = gsub('pdf$|PDF$','txt',tname)
#if(file.exists(tname)){next}#temp = fread(tname);temp[,text:=NULL];all_page_dt = rbind(all_page_dt,temp,use.names=T,fill=T);next}
print(i)
temp_text = tryCatch({pdftools::pdf_text(fname)},error = function(e) NULL)
if(!is.null(temp_text) & length(temp_text)>0 & any(temp_text!='')){
temp_page = unlist(sapply(temp_text,function(x) x))
temp_page = gsub('\\s{1,}',' ',temp_page)
temp = data.table::data.table(Page = seq_along(temp_page),text = temp_page,stringsAsFactors = F)
temp$text[nchar(temp$text)>10000] <- ''
#temp = temp[!grepl('\\.{10,}',temp$text),]
temp$text = textclean::replace_non_ascii(temp$text)
#temp = temp[!grepl('^Figure [0-9]',temp$text),]
temp$text = gsub('\\s{1,}$','',temp$text)
#temp = temp[nchar(temp$text)>500,]
temp = temp[!duplicated(text)&text!='',]
#  if(file.exists(tname)&nrow(temp)==0){file.remove(tname)}
if(!file.exists(tname)|file.exists(tname)){fwrite(x = temp,file = tname,sep = '\t')}
}
},mc.cores = mcores,mc.cleanup = T,mc.preschedule = T)
# ###two packages that will do most of what you need
=======
new_record$EIS.Number = as.numeric(new_record$EIS.Number)
new_record[new_record==''] <- NA
new_record = new_record %>% mutate_if(is.logical,as.character)
new_record = data.table(new_record)
all(new_record$EIS.Number %in% record_df$EIS.Number)
if(all(new_record$EIS.Number %in% record_df$EIS.Number)){keep_going <<- F}
else{
if(all(new_record$EIS.Number %in% record_df$EIS.Number)){keep_going <<- F}else{
record_df <<- rbindlist(list(record_df,new_record),fill = T)
#if(any(duplicated(record_df))){
#    record_df <- record_df[!duplicated(record_df),]
#    break}
#if(current_page!=last_page)
search_session = session_follow_link(search_session,i = which(search_session %>% read_html() %>% html_nodes('a') %>% html_text() == 'Next')[1])
p = as.numeric(str_extract(str_extract(search_session$url,'\\bp=[0-9]{1,}\\b'),'[0-9]{1,}'))
#current_page = current_page + 1
Sys.sleep(0.25)
}
p
#while(p < last_page & keep_going){
while(p < last_page & keep_going){
print(p)
new_record = {search_session %>% read_html() %>% html_table(trim=T)}[[1]]
new_record$eis_url = paste0('https://cdxapps.epa.gov',grep('details\\?eisId=[0-9]{1,}$',search_session%>% read_html() %>% html_nodes('a') %>% html_attr('href'),value=T))
new_record <- data.table(new_record)
colnames(new_record) <- gsub('\\s','.',colnames(new_record))
new_record[,Download.Documents:=NULL]
new_record = new_record[!paste(Title,Federal.Register.Date) %in% paste(record_df$Title,record_df$Federal.Register.Date),]
new_record$Title <- enc2utf8(new_record$Title)
new_record$Title <- iconv(new_record$Title, "UTF-8", "UTF-8",sub='')
new_record$Title <- str_remove_all(new_record$Title,'\\\"')
if(nrow(new_record)==0){keep_going <<-FALSE}
if(nrow(new_record)>0){
#if(nrow(new_record)==0){break}
eis_info = lapply(seq_along(new_record$eis_url),function(x) {
new_record$eis_url[x]
page = read_html(new_record$eis_url[x])
i_name =  page %>% html_nodes(css = 'h4') %>% html_text(trim=T)
i_value = page %>% html_nodes(css = '.form-item') %>% html_text(trim=T)
entry = as.data.table(rbind(mapply(function(x,y) gsub(x,'',y) , x = i_name,y = i_value,SIMPLIFY=F)))
entry$eis_url <- new_record$eis_url[x]
Sys.sleep(1.5)
entry = data.frame(do.call(cbind,sapply(entry,function(x) gsub('^ ','',unlist(x)),simplify = F)),stringsAsFactors = F)
entry})
eis_info_df = rbindlist(eis_info,fill = T)
#eis_info_df$EIS.Title <- str_remove_all(eis_info_df$EIS.Title,'[^[:alnum:] ]')
eis_info_df$EIS.Title <- enc2utf8(eis_info_df$EIS.Title)
eis_info_df$EIS.Title <- iconv(eis_info_df$EIS.Title, "UTF-8", "UTF-8",sub='')
eis_info_df$EIS.Title <- str_remove_all(eis_info_df$EIS.Title,'\\\"')
eis_info_df$State.or.Territory <- stringr::str_replace_all(eis_info_df$State.or.Territory, "[\r\t\n]|\\s", "")
eis_info_df$EIS.Comment.Due..Review.Period.Date <- stringr::str_replace_all(eis_info_df$EIS.Comment.Due..Review.Period.Date, "[\r\t\n]|\\s", "")
eis_info_df$Federal.Register.Date <- stringr::str_replace_all(eis_info_df$Federal.Register.Date, "[\r\t\n]|\\s", "")
eis_info_df$Rating..if.Draft.EIS. <- stringr::str_replace_all(eis_info_df$Rating..if.Draft.EIS.,"[\r\t\n]|\\s", "")
eis_info_df$Rating..if.Draft.EIS. <- stringr::str_replace_all(eis_info_df$Rating..if.Draft.EIS.,"Rating\\(ifDraftEIS\\)", "")
eis_info_df$EPA.Comment.Letter.Date <- stringr::str_replace_all(eis_info_df$EPA.Comment.Letter.Date,"[\r\t\n]|\\s", "")
eis_info_df$Amended.Notice.Date <- stringr::str_replace_all(eis_info_df$Amended.Notice.Date,"[\r\t\n]|\\s", "")
eis_info_df[,Federal.Register.Date:=NULL]
eis_info_df[,EPA.Comment.Letter.Date:=NULL]
new_record$EIS_ID <- as.numeric(str_extract(new_record$eis_url,'(?!eisId=)[0-9]{1,}'))
new_record <- merge(new_record,eis_info_df,by = 'eis_url',all = T)
new_record$EIS.Number = as.numeric(new_record$EIS.Number)
new_record[new_record==''] <- NA
new_record = new_record %>% mutate_if(is.logical,as.character)
new_record = data.table(new_record)
if(all(new_record$EIS.Number %in% record_df$EIS.Number)){keep_going <<- F}else{
record_df <<- rbindlist(list(record_df,new_record),fill = T)
#if(any(duplicated(record_df))){
#    record_df <- record_df[!duplicated(record_df),]
#    break}
#if(current_page!=last_page)
search_session = session_follow_link(search_session,i = which(search_session %>% read_html() %>% html_nodes('a') %>% html_text() == 'Next')[1])
p = as.numeric(str_extract(str_extract(search_session$url,'\\bp=[0-9]{1,}\\b'),'[0-9]{1,}'))
#current_page = current_page + 1
Sys.sleep(0.25)
}
}
#
}
last_page
keep_going
all(new_record$EIS.Number %in% record_df$EIS.Number)
keep_going = T
#while(p < last_page & keep_going){
while(p < last_page & keep_going){
print(p)
new_record = {search_session %>% read_html() %>% html_table(trim=T)}[[1]]
new_record$eis_url = paste0('https://cdxapps.epa.gov',grep('details\\?eisId=[0-9]{1,}$',search_session%>% read_html() %>% html_nodes('a') %>% html_attr('href'),value=T))
new_record <- data.table(new_record)
colnames(new_record) <- gsub('\\s','.',colnames(new_record))
new_record[,Download.Documents:=NULL]
new_record = new_record[!paste(Title,Federal.Register.Date) %in% paste(record_df$Title,record_df$Federal.Register.Date),]
new_record$Title <- enc2utf8(new_record$Title)
new_record$Title <- iconv(new_record$Title, "UTF-8", "UTF-8",sub='')
new_record$Title <- str_remove_all(new_record$Title,'\\\"')
if(nrow(new_record)==0){keep_going <<-FALSE}
if(nrow(new_record)>0){
#if(nrow(new_record)==0){break}
eis_info = lapply(seq_along(new_record$eis_url),function(x) {
new_record$eis_url[x]
page = read_html(new_record$eis_url[x])
i_name =  page %>% html_nodes(css = 'h4') %>% html_text(trim=T)
i_value = page %>% html_nodes(css = '.form-item') %>% html_text(trim=T)
entry = as.data.table(rbind(mapply(function(x,y) gsub(x,'',y) , x = i_name,y = i_value,SIMPLIFY=F)))
entry$eis_url <- new_record$eis_url[x]
Sys.sleep(1.5)
entry = data.frame(do.call(cbind,sapply(entry,function(x) gsub('^ ','',unlist(x)),simplify = F)),stringsAsFactors = F)
entry})
eis_info_df = rbindlist(eis_info,fill = T)
#eis_info_df$EIS.Title <- str_remove_all(eis_info_df$EIS.Title,'[^[:alnum:] ]')
eis_info_df$EIS.Title <- enc2utf8(eis_info_df$EIS.Title)
eis_info_df$EIS.Title <- iconv(eis_info_df$EIS.Title, "UTF-8", "UTF-8",sub='')
eis_info_df$EIS.Title <- str_remove_all(eis_info_df$EIS.Title,'\\\"')
eis_info_df$State.or.Territory <- stringr::str_replace_all(eis_info_df$State.or.Territory, "[\r\t\n]|\\s", "")
eis_info_df$EIS.Comment.Due..Review.Period.Date <- stringr::str_replace_all(eis_info_df$EIS.Comment.Due..Review.Period.Date, "[\r\t\n]|\\s", "")
eis_info_df$Federal.Register.Date <- stringr::str_replace_all(eis_info_df$Federal.Register.Date, "[\r\t\n]|\\s", "")
eis_info_df$Rating..if.Draft.EIS. <- stringr::str_replace_all(eis_info_df$Rating..if.Draft.EIS.,"[\r\t\n]|\\s", "")
eis_info_df$Rating..if.Draft.EIS. <- stringr::str_replace_all(eis_info_df$Rating..if.Draft.EIS.,"Rating\\(ifDraftEIS\\)", "")
eis_info_df$EPA.Comment.Letter.Date <- stringr::str_replace_all(eis_info_df$EPA.Comment.Letter.Date,"[\r\t\n]|\\s", "")
eis_info_df$Amended.Notice.Date <- stringr::str_replace_all(eis_info_df$Amended.Notice.Date,"[\r\t\n]|\\s", "")
eis_info_df[,Federal.Register.Date:=NULL]
eis_info_df[,EPA.Comment.Letter.Date:=NULL]
new_record$EIS_ID <- as.numeric(str_extract(new_record$eis_url,'(?!eisId=)[0-9]{1,}'))
new_record <- merge(new_record,eis_info_df,by = 'eis_url',all = T)
new_record$EIS.Number = as.numeric(new_record$EIS.Number)
new_record[new_record==''] <- NA
new_record = new_record %>% mutate_if(is.logical,as.character)
new_record = data.table(new_record)
if(all(new_record$EIS.Number %in% record_df$EIS.Number)){keep_going <<- F}else{
record_df <<- rbindlist(list(record_df,new_record),fill = T)
#if(any(duplicated(record_df))){
#    record_df <- record_df[!duplicated(record_df),]
#    break}
#if(current_page!=last_page)
search_session = session_follow_link(search_session,i = which(search_session %>% read_html() %>% html_nodes('a') %>% html_text() == 'Next')[1])
p = as.numeric(str_extract(str_extract(search_session$url,'\\bp=[0-9]{1,}\\b'),'[0-9]{1,}'))
#current_page = current_page + 1
Sys.sleep(0.25)
}
}
#
}
library(rvest)
library(stringr)
>>>>>>> Stashed changes
library(tidyverse)
library(httr)
library(data.table)
packs = c('rvest','stringr','tidyverse','httr','data.table','RCurl')
sapply(packs[!packs %in% installed.packages()[,'Package']],install.packages)
sapply(packs,require,character.only = T)
file_storage = 'enepa_repository/documents/'
record_df = fread('enepa_repository/metadata/eis_record_detail.csv',stringsAsFactors = F)
record_df = record_df %>% mutate_if(is.logical,as.character)
record_df = data.table(record_df)
record_df = record_df[order(-EIS.Number)]
docs = fread('enepa_repository/metadata/eis_document_record.csv',stringsAsFactors = F)
current_flist = list.files(file_storage,recursive = T)
docs$YEAR = str_extract(docs$EIS.Number,'^[0-9]{4}')
docs = docs[YEAR>=2012,]
doc_df = docs
doc_df
grep('\\(',doc_df$File_Name,value = T)
nms <- grep('\\(',doc_df$File_Name,value = T)
nms <- list.files('enepa_repository/documents/',full.names = T,recursive = T)
nms <- grep('\\(',nms,value = T)
nms
nms <- list.files('enepa_repository/documents/',full.names = T,recursive = T)
nms <- grep('\\(|\\)',nms,value = T)
doc_df$File_Name <- str_remove_all(doc_df$File_Name,'\\(|\\)')
write.csv(x = doc_df,file = paste0('enepa_repository/metadata/eis_document_record','.csv'),row.names = F)
nms <- list.files('enepa_repository/documents/',full.names = T,recursive = T)
nms <- grep('\\(|\\)',nms,value = T)
nms2 <- str_remove_all(nms,'\\(|\\)')
file.rename(from = nms,to = nms2)
library(rvest)
library(stringr)
library(tidyverse)
library(httr)
<<<<<<< Updated upstream
# #progress bar lapply, useful for gauging how long/bad your code is in an apply loop
library(pbapply)
# #page listing forest urls
nfs_site_list = "https://web.archive.org/web/20190701045610/https://www.fs.fed.us/recreation/map/state_list.shtml"
sites = nfs_site_list %>% read_html() %>% html_nodes('a')
site_matches = grep('National Forest$|National Forests$|in Texas$|Management Area|National Gra[s]{1,}land|National Tallgrass Prairie',sites %>% html_text(trim=T),value=F)
text = sites %>% html_text(trim=T)
hrefs = sites %>% html_attr('href')
site_dfs = data.frame(Forest = text[site_matches],url = hrefs[site_matches],stringsAsFactors = F) %>% filter(!duplicated(.))
site_dfs$url = str_extract(site_dfs$url,'[^^]http.+')
#site_dfs$url[!grepl('^http',site_dfs$url)] <- paste0('http://www.fs.fed.us',site_dfs$url[!grepl('^http',site_dfs$url)])
site_dfs = site_dfs[!duplicated(site_dfs),]
site_dfs$url = gsub('^\\/','',site_dfs$url)
site_dfs$url = gsub('http:','https:',site_dfs$url)
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r9/white/" ] <- 'https://www.fs.usda.gov/land/whitemountain'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r5/sequoia" ] <- 'https://www.fs.usda.gov/sequoia'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r3/sfe/"  ] <- 'https://www.fs.usda.gov/santafe'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r1/clearwater/"  ] <- 'https://www.fs.usda.gov/clearwater'
site_dfs$url[site_dfs$url =="https://www.fs.fed.us/r10/tongass/"   ] <- 'https://www.fs.usda.gov/tongass'
site_dfs$url[site_dfs$url ==  "https://www.fs.fed.us/r1/helena/"  ] <- 'https://www.fs.usda.gov/helena'
# # #some urls redirect to a different page, so use session to find redirect
fsites = site_dfs$url
forest_names_url = pblapply(fsites, function(x) html_session(x)$url)
# #some redirects result in duplicates
forest_names_url = unique(unlist(forest_names_url))
# #
# #
forest_names_url = forest_names_url[!duplicated(gsub('\\/$','',forest_names_url))]
# ##isolate names used on project page urls
forest_name_urls = gsub("\\/","",gsub('main\\/|\\/home|land\\/','',gsub("https://www.fs.usda.gov/",'',forest_names_url)))
forest_name_urls = append(forest_name_urls,'ochoco')
prefix = "https://www.fs.usda.gov/wps/portal/fsinternet/cs/projects/"
suffix = "/landmanagement/projects?archive=1&sortby=1"
proj_archives = paste0(prefix,forest_name_urls,suffix)
# ##isolate names used on project page urls
prefix = "https://www.fs.usda.gov/projects/"
suffix = "/landmanagement/projects"
proj_current = paste0(prefix,forest_name_urls,suffix)
proj_url = data.frame(project_urls = c(proj_archives,proj_current),status = c(rep('Archived',length(proj_archives)),
rep('Current',length(proj_current))),stringsAsFactors = F)
proj_url$nf = gsub('https://www.fs.usda.gov/projects/','',proj_url$project_urls,fixed=T)
proj_url$nf = gsub('/landmanagement/projects','',proj_url$nf,fixed=T)
proj_url$nf = gsub('https://www.fs.usda.gov/wps/portal/fsinternet/cs/projects/','',proj_url$nf,fixed=T)
proj_url$nf = gsub('?archive=1&sortby=1','',proj_url$nf,fixed=T)
base = 'https://www.fs.fed.us/nepa/nepa_home.php'
port = 4567
pJS <- wdman::phantomjs()
#pJS <- wdman::phantomjs()
Sys.sleep(5) # give the binary a moment
remDr <- remoteDriver(port=port)
remDr$open()
remDr$navigate(base)
nf_options = remDr$findElement(using = 'name','forest')
base
library(rvest)
library(stringr)
library(tidyverse)
library(data.table)
require(rvest)
fname = 'enepa_repository/meta_data/eis_record_detail.csv'
if(file.exists(fname)){
record_df = fread(fname)}else{record_df = data.table(stringsAsFactors = F)}
library(rvest)
library(stringr)
library(tidyverse)
library(data.table)
require(rvest)
fname = 'enepa_repository/metadata/eis_record_detail.csv'
if(file.exists(fname)){
record_df = fread(fname)}else{record_df = data.table(stringsAsFactors = F)}
#base_page <- 'https://cdxnodengn.epa.gov/cdx-enepa-public/action/eis/search'
base_page <- 'https://cdxapps.epa.gov/cdx-enepa-II/public/action/eis/search'
base_session = base_page %>% session()
search_form = html_form(base_session)[[2]]
search_form$fields$searchCriteria.onlyCommentLetters$value <- 'false'
#search_form <- set_values(search_form,searchCriteria.states = state)
search_session = session_submit(base_session,search_form)
hrefs = search_session %>% read_html() %>% html_nodes('a') %>% html_attr('href')
last_page = max(as.numeric(gsub('p=','',str_extract(hrefs[duplicated(hrefs)],'p=[0-9]{1,}'))))
print(last_page)
p = 1
next_link = which(search_session %>% read_html() %>% html_nodes('a') %>% html_text() == 'Next')[1]
search_session = session_follow_link(search_session,i = next_link)
search_session = rvest::session_jump_to(search_session,'?searchCritera.primaryStates=&d-446779-p=1&reset=Reset&searchCriteria.onlyCommentLetters=false')
p = as.numeric(str_extract(str_extract(search_session$url,'\\bp=[0-9]{1,}\\b'),'[0-9]{1,}'))
keep_going = T
while(p < last_page & keep_going){
print(p)
new_record = {search_session %>% read_html() %>% html_table(trim=T)}[[1]]
new_record$eis_url = paste0('https://cdxapps.epa.gov',grep('details\\?eisId=[0-9]{1,}$',search_session%>% read_html() %>% html_nodes('a') %>% html_attr('href'),value=T))
new_record <- data.table(new_record)
colnames(new_record) <- gsub('\\s','.',colnames(new_record))
new_record[,Download.Documents:=NULL]
new_record = new_record[!paste(Title,Federal.Register.Date) %in% paste(record_df$Title,record_df$Federal.Register.Date),]
if(nrow(new_record)==0){keep_going <<-FALSE}
if(nrow(new_record)>0){
#if(nrow(new_record)==0){break}
eis_info = lapply(seq_along(new_record$eis_url),function(x) {
new_record$eis_url[x]
page = read_html(new_record$eis_url[x])
i_name =  page %>% html_nodes(css = 'h4') %>% html_text(trim=T)
i_value = page %>% html_nodes(css = '.form-item') %>% html_text(trim=T)
entry = as.data.table(rbind(mapply(function(x,y) gsub(x,'',y) , x = i_name,y = i_value,SIMPLIFY=F)))
entry$eis_url <- new_record$eis_url[x]
Sys.sleep(1.5)
entry = data.frame(do.call(cbind,sapply(entry,function(x) gsub('^ ','',unlist(x)),simplify = F)),stringsAsFactors = F)
entry})
eis_info_df = rbindlist(eis_info,fill = T)
eis_info_df$State.or.Territory <- stringr::str_replace_all(eis_info_df$State.or.Territory, "[\r\t\n]|\\s", "")
eis_info_df$EIS.Comment.Due..Review.Period.Date <- stringr::str_replace_all(eis_info_df$EIS.Comment.Due..Review.Period.Date, "[\r\t\n]|\\s", "")
eis_info_df$Federal.Register.Date <- stringr::str_replace_all(eis_info_df$Federal.Register.Date, "[\r\t\n]|\\s", "")
eis_info_df$Rating..if.Draft.EIS. <- stringr::str_replace_all(eis_info_df$Rating..if.Draft.EIS.,"[\r\t\n]|\\s", "")
eis_info_df$Rating..if.Draft.EIS. <- stringr::str_replace_all(eis_info_df$Rating..if.Draft.EIS.,"Rating\\(ifDraftEIS\\)", "")
eis_info_df$EPA.Comment.Letter.Date <- stringr::str_replace_all(eis_info_df$EPA.Comment.Letter.Date,"[\r\t\n]|\\s", "")
eis_info_df$Amended.Notice.Date <- stringr::str_replace_all(eis_info_df$Amended.Notice.Date,"[\r\t\n]|\\s", "")
eis_info_df[,Federal.Register.Date:=NULL]
eis_info_df[,EPA.Comment.Letter.Date:=NULL]
new_record$EIS_ID <- as.numeric(str_extract(new_record$eis_url,'(?!eisId=)[0-9]{1,}'))
new_record <- merge(new_record,eis_info_df,by = 'eis_url',all = T)
new_record$EIS.Number = as.numeric(new_record$EIS.Number)
new_record[new_record==''] <- NA
new_record = new_record %>% mutate_if(is.logical,as.character)
new_record = data.table(new_record)
if(all(new_record$EIS.Number %in% record_df$EIS.Number)){keep_going <<- F}
else{
record_df <<- rbindlist(list(record_df,new_record),fill = T)
#if(any(duplicated(record_df))){
#    record_df <- record_df[!duplicated(record_df),]
#    break}
#if(current_page!=last_page)
search_session = session_follow_link(search_session,i = which(search_session %>% read_html() %>% html_nodes('a') %>% html_text() == 'Next')[1])
p = as.numeric(str_extract(str_extract(search_session$url,'\\bp=[0-9]{1,}\\b'),'[0-9]{1,}'))
#current_page = current_page + 1
Sys.sleep(0.25)
}
}
#
}
fwrite(x = record_df,file = fname,row.names = F)
library(rvest)
library(stringr)
library(tidyverse)
library(httr)
library(data.table)
packs = c('rvest','stringr','tidyverse','httr','data.table','RCurl')
sapply(packs[!packs %in% installed.packages()[,'Package']],install.packages)
sapply(packs,require,character.only = T)
file_storage = 'enepa_repository/documents/'
record_df = fread('enepa_repository/meta_data/eis_record_detail.csv',stringsAsFactors = F)
library(rvest)
library(stringr)
library(tidyverse)
library(httr)
library(data.table)
packs = c('rvest','stringr','tidyverse','httr','data.table','RCurl')
sapply(packs[!packs %in% installed.packages()[,'Package']],install.packages)
sapply(packs,require,character.only = T)
file_storage = 'enepa_repository/documents/'
record_df = fread('enepa_repository/metadata/eis_record_detail.csv',stringsAsFactors = F)
record_df = record_df %>% mutate_if(is.logical,as.character)
record_df = data.table(record_df)
record_df = record_df[order(-EIS.Number)]
rerunALL = FALSE
if(!rerunALL){
docs = fread('enepa_repository/meta_data/eis_document_record.csv',stringsAsFactors = F)
current_flist = list.files(file_storage,recursive = T)
docs$YEAR = str_extract(docs$EIS.Number,'^[0-9]{4}')
docs = docs[YEAR>=2012,]
doc_df = docs}
library(rvest)
library(stringr)
library(tidyverse)
library(httr)
=======
>>>>>>> Stashed changes
library(data.table)
packs = c('rvest','stringr','tidyverse','httr','data.table','RCurl')
sapply(packs[!packs %in% installed.packages()[,'Package']],install.packages)
sapply(packs,require,character.only = T)
file_storage = 'enepa_repository/documents/'
record_df = fread('enepa_repository/metadata/eis_record_detail.csv',stringsAsFactors = F)
record_df = record_df %>% mutate_if(is.logical,as.character)
record_df = data.table(record_df)
record_df = record_df[order(-EIS.Number)]
rerunALL = FALSE
if(!rerunALL){
docs = fread('enepa_repository/metadata/eis_document_record.csv',stringsAsFactors = F)
current_flist = list.files(file_storage,recursive = T)
docs$YEAR = str_extract(docs$EIS.Number,'^[0-9]{4}')
docs = docs[YEAR>=2012,]
<<<<<<< Updated upstream
doc_df = docs}
=======
doc_df = docs
}
>>>>>>> Stashed changes
if(rerunALL){
#needed_docs = docs[!paste(docs$YEAR,docs$File_Name,sep = '/') %in% current_flist,]
doc_df = data.table(EIS.Number = as.numeric(),Original_File_Name = as.character(),
File_Name =as.character(),BAD_FILE =  logical(),PDF = logical(),stringsAsFactors = F)
}
base_page = 'https://cdxapps.epa.gov'
finfo = file.info(paste0(file_storage,current_flist))
not_empty = finfo$size>0
if(any(!not_empty)){
file.remove(paste0(file_storage,current_flist[!not_empty]))
current_flist = current_flist[not_empty]
}
record_df$YEAR = str_extract(record_df$EIS.Number,'^[0-9]{4}')
#record_df = record_df[YEAR %in% 2013:2019,]
fls <- list.files('enepa_repository/documents/',recursive = T)
check_projs <- unique(str_extract(doc_df$File_Name[!doc_df$File_Name %in% basename(fls)],'^[0-9]{8}'))
record_df = record_df[({!EIS.Number %in% doc_df$EIS.Number} | EIS.Number %in% check_projs) & YEAR>2012,]
<<<<<<< Updated upstream
record_df[order(-EIS.Number),]
=======
>>>>>>> Stashed changes
for (i in 1:nrow(record_df)){
Sys.sleep(0.25)
print(record_df$EIS.Number[i])
try = RCurl::getURL(record_df$eis_url[i])
if(!grepl('Server Error',try)){
htmlNodes = record_df$eis_url[i] %>% read_html()  %>% html_nodes('a')
file_url = grep('downloadAttachment',htmlNodes %>% html_attr('href') ,value=T)
file_names = {htmlNodes %>% html_text()}[grepl('downloadAttachment',htmlNodes %>% html_attr('href'))]
file_names = gsub('~|/','-',file_names)
file_names = gsub('\\s{1,}','_',file_names)
file_names = gsub('PDF$','pdf',file_names)
if(length(file_url)==0){next}
for (j in 1:length(file_url)){
subd = str_extract(record_df$EIS.Number[i],'^[0-9]{4}')
if(!dir.exists(paste0(file_storage,subd))){dir.create(paste0(file_storage,subd))}
if(file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
temp_name = paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_'))
temp_name <- gsub('PDF$','pdf',temp_name)
<<<<<<< Updated upstream
download = tryCatch(httr::GET(paste0(base_page,file_url[j]), verbose(),write_disk(temp_name), overwrite=TRUE),error=function(e) NULL)
temp_info = file.info(temp_name)
if(is.null(download)){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
=======
temp_name <- str_remove_all(temp_name,'\\(|\\)')
download = tryCatch(httr::GET(paste0(base_page,file_url[j]), verbose(),write_disk(temp_name), overwrite=TRUE),error=function(e) NULL)
temp_info = file.info(temp_name)
if(is.null(download)){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],str_remove_all(file_names[j],'\\(|\\)'),sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
>>>>>>> Stashed changes
}
if(!is.null(download)){
temp_info$size = file.info(temp_name)
if(file.size(temp_name)==0&all(duplicated(file_names))){
<<<<<<< Updated upstream
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
=======
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],str_remove_all(file_names[j],'\\(|\\)'),sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
>>>>>>> Stashed changes
}
if(file.size(temp_name)==0&file_names[j] %in% file_names[-j]){
file.remove(temp_name)
next
}
if(file.size(temp_name)>0){
<<<<<<< Updated upstream
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
=======
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],str_remove_all(file_names[j],'\\(|\\)'),sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
}
}
doc_df = rbind(doc_df,tdf,use.names = T,fill = T)
}
}
}
library(rvest)
library(stringr)
library(tidyverse)
library(httr)
library(data.table)
packs = c('rvest','stringr','tidyverse','httr','data.table','RCurl')
sapply(packs[!packs %in% installed.packages()[,'Package']],install.packages)
sapply(packs,require,character.only = T)
file_storage = 'enepa_repository/documents/'
record_df = fread('enepa_repository/metadata/eis_record_detail.csv',stringsAsFactors = F)
record_df = record_df %>% mutate_if(is.logical,as.character)
record_df = data.table(record_df)
record_df = record_df[order(-EIS.Number)]
docs = fread('enepa_repository/metadata/eis_document_record.csv',stringsAsFactors = F)
current_flist = list.files(file_storage,recursive = T)
docs$YEAR = str_extract(docs$EIS.Number,'^[0-9]{4}')
docs = docs[YEAR>=2012,]
doc_df = docs
doc_df$File_Name <- str_remove_all(doc_df$File_Name,'\\&')
flist <- list.files('enepa_repository/documents/',recursive = T,full.names = T)
flist <- grep('\\&',flist,value = T)
flist
file.rename(from = flist,to = str_remove_all(flist,'\\&'))
write.csv(x = doc_df,file = paste0('enepa_repository/metadata/eis_document_record','.csv'),row.names = F)
library(rvest)
library(stringr)
library(tidyverse)
library(httr)
library(data.table)
packs = c('rvest','stringr','tidyverse','httr','data.table','RCurl')
sapply(packs[!packs %in% installed.packages()[,'Package']],install.packages)
sapply(packs,require,character.only = T)
file_storage = 'enepa_repository/documents/'
record_df = fread('enepa_repository/metadata/eis_record_detail.csv',stringsAsFactors = F)
record_df = record_df %>% mutate_if(is.logical,as.character)
record_df = data.table(record_df)
record_df = record_df[order(-EIS.Number)]
rerunALL = FALSE
if(!rerunALL){
docs = fread('enepa_repository/metadata/eis_document_record.csv',stringsAsFactors = F)
current_flist = list.files(file_storage,recursive = T)
docs$YEAR = str_extract(docs$EIS.Number,'^[0-9]{4}')
docs = docs[YEAR>=2012,]
doc_df = docs
}
if(rerunALL){
#needed_docs = docs[!paste(docs$YEAR,docs$File_Name,sep = '/') %in% current_flist,]
doc_df = data.table(EIS.Number = as.numeric(),Original_File_Name = as.character(),
File_Name =as.character(),BAD_FILE =  logical(),PDF = logical(),stringsAsFactors = F)
}
base_page = 'https://cdxapps.epa.gov'
finfo = file.info(paste0(file_storage,current_flist))
not_empty = finfo$size>0
if(any(!not_empty)){
file.remove(paste0(file_storage,current_flist[!not_empty]))
current_flist = current_flist[not_empty]
}
record_df$YEAR = str_extract(record_df$EIS.Number,'^[0-9]{4}')
#record_df = record_df[YEAR %in% 2013:2019,]
fls <- list.files('enepa_repository/documents/',recursive = T)
check_projs <- unique(str_extract(doc_df$File_Name[!doc_df$File_Name %in% basename(fls)],'^[0-9]{8}'))
record_df = record_df[({!EIS.Number %in% doc_df$EIS.Number} | EIS.Number %in% check_projs) & YEAR>2012,]
for (i in 1:nrow(record_df)){
Sys.sleep(0.25)
print(record_df$EIS.Number[i])
try = RCurl::getURL(record_df$eis_url[i])
if(!grepl('Server Error',try)){
htmlNodes = record_df$eis_url[i] %>% read_html()  %>% html_nodes('a')
file_url = grep('downloadAttachment',htmlNodes %>% html_attr('href') ,value=T)
file_names = {htmlNodes %>% html_text()}[grepl('downloadAttachment',htmlNodes %>% html_attr('href'))]
file_names = gsub('~|/','-',file_names)
file_names = gsub('\\s{1,}','_',file_names)
file_names = gsub('PDF$','pdf',file_names)
if(length(file_url)==0){next}
for (j in 1:length(file_url)){
subd = str_extract(record_df$EIS.Number[i],'^[0-9]{4}')
if(!dir.exists(paste0(file_storage,subd))){dir.create(paste0(file_storage,subd))}
if(file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
temp_name = paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_'))
temp_name <- gsub('PDF$','pdf',temp_name)
temp_name <- str_remove_all(temp_name,'\\(|\\)\\&')
download = tryCatch(httr::GET(paste0(base_page,file_url[j]), verbose(),write_disk(temp_name), overwrite=TRUE),error=function(e) NULL)
temp_info = file.info(temp_name)
if(is.null(download)){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],str_remove_all(file_names[j],'\\(|\\)\\&'),sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!is.null(download)){
temp_info$size = file.info(temp_name)
if(file.size(temp_name)==0&all(duplicated(file_names))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],str_remove_all(file_names[j],'\\(|\\)\\&'),sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(file.size(temp_name)==0&file_names[j] %in% file_names[-j]){
file.remove(temp_name)
next
}
if(file.size(temp_name)>0){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],str_remove_all(file_names[j],'\\(|\\)\\&'),sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
}
}
doc_df = rbind(doc_df,tdf,use.names = T,fill = T)
}
}
}
library(rvest)
library(stringr)
library(tidyverse)
library(httr)
library(data.table)
packs = c('rvest','stringr','tidyverse','httr','data.table','RCurl')
sapply(packs[!packs %in% installed.packages()[,'Package']],install.packages)
sapply(packs,require,character.only = T)
file_storage = 'enepa_repository/documents/'
record_df = fread('enepa_repository/metadata/eis_record_detail.csv',stringsAsFactors = F)
record_df = record_df %>% mutate_if(is.logical,as.character)
record_df = data.table(record_df)
record_df = record_df[order(-EIS.Number)]
head(record_df)
rerunALL = FALSE
if(!rerunALL){
docs = fread('enepa_repository/metadata/eis_document_record.csv',stringsAsFactors = F)
current_flist = list.files(file_storage,recursive = T)
docs$YEAR = str_extract(docs$EIS.Number,'^[0-9]{4}')
docs = docs[YEAR>=2012,]
doc_df = docs
}
if(rerunALL){
#needed_docs = docs[!paste(docs$YEAR,docs$File_Name,sep = '/') %in% current_flist,]
doc_df = data.table(EIS.Number = as.numeric(),Original_File_Name = as.character(),
File_Name =as.character(),BAD_FILE =  logical(),PDF = logical(),stringsAsFactors = F)
}
base_page = 'https://cdxapps.epa.gov'
finfo = file.info(paste0(file_storage,current_flist))
not_empty = finfo$size>0
if(any(!not_empty)){
file.remove(paste0(file_storage,current_flist[!not_empty]))
current_flist = current_flist[not_empty]
}
record_df$YEAR = str_extract(record_df$EIS.Number,'^[0-9]{4}')
fls <- list.files('enepa_repository/documents/',recursive = T)
check_projs <- unique(str_extract(doc_df$File_Name[!doc_df$File_Name %in% basename(fls)],'^[0-9]{8}'))
record_df = record_df[({!EIS.Number %in% doc_df$EIS.Number} | EIS.Number %in% check_projs) & YEAR>2012,]
record_df
check_projs
check_projs
doc_df[doc_df$EIS.Number%in%check_projs,]
doc_df[doc_df$EIS.Number%in%check_projs,]$File_Name
doc_df[doc_df$EIS.Number%in%check_projs,]$File_Name
grep('\\,',fls,value = T)
fls_temp <- grep('\\,',fls,value = T)
file.rename(fls_temp,str_remove_all(fls_temp,"\\,"))
warnings()
fls_temp <- list.files('enepa_repository/documents/',recursive = T,full.names = T)
fls_temp <- grep('\\,',fls,value = T)
file.rename(fls_temp,str_remove_all(fls_temp,"\\,"))
fls_temp <- list.files('enepa_repository/documents/',recursive = T,full.names = T)
fls_temp <- grep('\\,',fls_temp,value = T)
file.rename(fls_temp,str_remove_all(fls_temp,"\\,"))
docs = fread('enepa_repository/metadata/eis_document_record.csv',stringsAsFactors = F)
current_flist = list.files(file_storage,recursive = T)
docs$YEAR = str_extract(docs$EIS.Number,'^[0-9]{4}')
docs = docs[YEAR>=2012,]
doc_df = docs
doc_df$File_Name
grep('\\,',doc_df$File_Name,value = T)
str_remove_all(doc_df$File_Name,"\\,")
doc_df$File_Name <- str_remove_all(doc_df$File_Name,"\\,")
write.csv(x = doc_df,file = paste0('enepa_repository/metadata/eis_document_record','.csv'),row.names = F)
library(rvest)
library(stringr)
library(tidyverse)
library(httr)
library(data.table)
packs = c('rvest','stringr','tidyverse','httr','data.table','RCurl')
sapply(packs[!packs %in% installed.packages()[,'Package']],install.packages)
sapply(packs,require,character.only = T)
file_storage = 'enepa_repository/documents/'
record_df = fread('enepa_repository/metadata/eis_record_detail.csv',stringsAsFactors = F)
record_df = record_df %>% mutate_if(is.logical,as.character)
record_df = data.table(record_df)
record_df = record_df[order(-EIS.Number)]
rerunALL = FALSE
if(!rerunALL){
docs = fread('enepa_repository/metadata/eis_document_record.csv',stringsAsFactors = F)
current_flist = list.files(file_storage,recursive = T)
docs$YEAR = str_extract(docs$EIS.Number,'^[0-9]{4}')
docs = docs[YEAR>=2012,]
doc_df = docs
}
if(rerunALL){
#needed_docs = docs[!paste(docs$YEAR,docs$File_Name,sep = '/') %in% current_flist,]
doc_df = data.table(EIS.Number = as.numeric(),Original_File_Name = as.character(),
File_Name =as.character(),BAD_FILE =  logical(),PDF = logical(),stringsAsFactors = F)
}
base_page = 'https://cdxapps.epa.gov'
finfo = file.info(paste0(file_storage,current_flist))
not_empty = finfo$size>0
if(any(!not_empty)){
file.remove(paste0(file_storage,current_flist[!not_empty]))
current_flist = current_flist[not_empty]
}
record_df$YEAR = str_extract(record_df$EIS.Number,'^[0-9]{4}')
#record_df = record_df[YEAR %in% 2013:2019,]
fls <- list.files('enepa_repository/documents/',recursive = T)
check_projs <- unique(str_extract(doc_df$File_Name[!doc_df$File_Name %in% basename(fls)],'^[0-9]{8}'))
record_df = record_df[({!EIS.Number %in% doc_df$EIS.Number} | EIS.Number %in% check_projs) & YEAR>2012,]
for (i in 1:nrow(record_df)){
Sys.sleep(0.25)
print(record_df$EIS.Number[i])
try = RCurl::getURL(record_df$eis_url[i])
if(!grepl('Server Error',try)){
htmlNodes = record_df$eis_url[i] %>% read_html()  %>% html_nodes('a')
file_url = grep('downloadAttachment',htmlNodes %>% html_attr('href') ,value=T)
file_names = {htmlNodes %>% html_text()}[grepl('downloadAttachment',htmlNodes %>% html_attr('href'))]
file_names = gsub('~|/','-',file_names)
file_names = gsub('\\s{1,}','_',file_names)
file_names = gsub('PDF$','pdf',file_names)
if(length(file_url)==0){next}
for (j in 1:length(file_url)){
subd = str_extract(record_df$EIS.Number[i],'^[0-9]{4}')
if(!dir.exists(paste0(file_storage,subd))){dir.create(paste0(file_storage,subd))}
if(file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],file_names[j],sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!file.exists(paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_')))){
temp_name = paste0(file_storage,subd,'/',paste(record_df$EIS.Number[i],file_names[j],sep='_'))
temp_name <- gsub('PDF$','pdf',temp_name)
temp_name <- str_remove_all(temp_name,'\\(|\\)|\\&|\\,')
download = tryCatch(httr::GET(paste0(base_page,file_url[j]), verbose(),write_disk(temp_name), overwrite=TRUE),error=function(e) NULL)
temp_info = file.info(temp_name)
if(is.null(download)){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],str_remove_all(file_names[j],'\\(|\\)|\\&|\\,'),sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(!is.null(download)){
temp_info$size = file.info(temp_name)
if(file.size(temp_name)==0&all(duplicated(file_names))){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],str_remove_all(file_names[j],'\\(|\\)|\\&|\\,'),sep='_'),BAD_FILE=T,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
if(file.size(temp_name)==0&file_names[j] %in% file_names[-j]){
file.remove(temp_name)
next
}
if(file.size(temp_name)>0){
tdf = data.frame(EIS.Number = record_df$EIS.Number[i],Original_File_Name = file_names[j],File_Name = paste(record_df$EIS.Number[i],str_remove_all(file_names[j],'\\(|\\)|\\&|\\,'),sep='_'),BAD_FILE=F,PDF = grepl('PDF$',toupper(file_names[j])),stringsAsFactors = F)
}
>>>>>>> Stashed changes
}
}
doc_df = rbind(doc_df,tdf,use.names = T,fill = T)
}
}
}
write.csv(x = doc_df,file = paste0('enepa_repository/metadata/eis_document_record','.csv'),row.names = F)
<<<<<<< Updated upstream
if(!require(data.table)){install.packages('data.table');require(data.table)}
if(!require(stringr)){install.packages('stringr');require(stringr)}
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(doParallel)){install.packages('doParallel');require(doParallel)}
if(!require(pdftools)){install.packages('pdftools');require(pdftools)}
if(!require(textclean)){install.packages('textclean');require(pdftools)}
projects = fread('enepa_repository/meta_data/eis_record_detail.csv',colClasses = 'character')
install.packages('textclean')
install.packages("textclean")
if(!require(data.table)){install.packages('data.table');require(data.table)}
if(!require(stringr)){install.packages('stringr');require(stringr)}
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(doParallel)){install.packages('doParallel');require(doParallel)}
if(!require(pdftools)){install.packages('pdftools');require(pdftools)}
if(!require(textclean)){install.packages('textclean');require(pdftools)}
if(!require(textclean)){install.packages('textclean');require(textclean)}
projects = fread('enepa_repository/meta_data/eis_record_detail.csv',colClasses = 'character')
projects = fread('enepa_repository/metadata/eis_record_detail.csv',colClasses = 'character')
documents = fread('enepa_repository/metadata/eis_document_record.csv',colClasses = 'character')
pdf_files = list.files('enepa_repository/documents/',full.names = T,recursive = T)
txt_files = list.files('enepa_repository/text_as_datatable/',full.names = T,recursive = T)
still_need = documents[!gsub('pdf$','txt',documents$File_Name) %in% basename(txt_files),]
dr <- list.dirs('enepa_repository/documents')
dr2 <- gsub('documents','text_as_datatable',dr)
sapply(dr2[!dir.exists(dr2)],dir.create)
for(i in nrow(still_need):1){
pdf_name = grep(still_need$File_Name[i],pdf_files,value = T)
print(pdf_name)
text_name = gsub('enepa_repository/documents','enepa_repository/text_as_datatable',pdf_name,fixed = T)
text_name <- gsub('pdf$','txt',text_name)
temp_text = tryCatch({pdftools::pdf_text(grep(still_need$File_Name[i],pdf_files,value = T))},error = function(e) NULL)
if(!is.null(temp_text) & length(temp_text)>0 & any(temp_text!='')){
temp_page = unlist(sapply(temp_text,function(x) x))
temp_page = gsub('\\s{1,}',' ',temp_page)
temp = data.table::data.table(Page = seq_along(temp_page),text = temp_page,stringsAsFactors = F)
temp$text[nchar(temp$text)>10000] <- ''
#temp = temp[!grepl('\\.{10,}',temp$text),]
temp$text = textclean::replace_non_ascii(temp$text)
#temp = temp[!grepl('^Figure [0-9]',temp$text),]
temp$text = gsub('\\s{1,}$','',temp$text)
#temp = temp[nchar(temp$text)>500,]
temp = temp[!duplicated(text)&text!='',]
if(nrow(temp)>0){
fwrite(x = temp,file = text_name,sep = '\t')
}
}
}
fname = 'enepa_repository/metadata/eis_record_detail.csv'
temp <- fread(fname)
library(data.table)
temp <- fread(fname)
saveRDS(temp,'enepa_repository/metadata/eis_record_detail.rds')
=======
#
>>>>>>> Stashed changes
