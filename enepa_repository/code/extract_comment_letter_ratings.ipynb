{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract EPA Comment Letter Ratings\n",
    "\n",
    "EPA comment letters prior to October 2018 included a rating system:\n",
    "\n",
    "## Rating System\n",
    "\n",
    "### Environmental Impact (Letters)\n",
    "- **LO** (Lack of Objections): No potential environmental impacts requiring changes\n",
    "- **EC** (Environmental Concerns): Environmental impacts that should be avoided\n",
    "- **EO** (Environmental Objections): Significant environmental impacts that must be avoided\n",
    "- **EU** (Environmentally Unsatisfactory): Unsatisfactory due to potential for significant harm\n",
    "\n",
    "### Adequacy of Information (Numbers)\n",
    "- **1** (Adequate): EIS adequately sets forth environmental impacts\n",
    "- **2** (Insufficient Information): Draft EIS lacks sufficient information\n",
    "- **3** (Inadequate): Draft EIS does not meet NEPA/Section 309 requirements\n",
    "\n",
    "### Combined Rating\n",
    "Example: **EC-2** = Environmental Concerns + Insufficient Information\n",
    "\n",
    "## Output\n",
    "CSV file with: filename, eisId, combined_rating, letter_rating, number_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install pypdf2 pdfplumber pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "# Try different PDF libraries\n",
    "try:\n",
    "    import pdfplumber\n",
    "    PDF_LIBRARY = \"pdfplumber\"\n",
    "except ImportError:\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader\n",
    "        PDF_LIBRARY = \"pypdf2\"\n",
    "    except ImportError:\n",
    "        PDF_LIBRARY = None\n",
    "        print(\"WARNING: No PDF library found. Install pdfplumber or pypdf2:\")\n",
    "        print(\"  pip install pdfplumber\")\n",
    "        print(\"  pip install pypdf2\")\n",
    "\n",
    "print(f\"Using PDF library: {PDF_LIBRARY}\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "METADATA_DIR = REPO_ROOT / \"metadata\"\n",
    "DOCUMENTS_DIR = REPO_ROOT / \"documents\"\n",
    "COMMENT_LETTERS_DIR = DOCUMENTS_DIR / \"comment_letters\"\n",
    "\n",
    "# Input files\n",
    "COMMENT_LETTER_PKL = METADATA_DIR / \"comment_letter_record_api.pkl\"\n",
    "DOC_RECORD_PKL = METADATA_DIR / \"eis_document_record_api.pkl\"\n",
    "EIS_RECORD_PKL = METADATA_DIR / \"eis_record_api.pkl\"\n",
    "\n",
    "# Legacy R data files (fallback)\n",
    "EIS_RECORD_RDS = METADATA_DIR / \"eis_record_detail.rds\"\n",
    "DOC_RECORD_RDS = METADATA_DIR / \"eis_document_record.rds\"\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = METADATA_DIR / \"comment_letter_ratings.csv\"\n",
    "\n",
    "# Cutoff date for ratings (October 2018)\n",
    "RATING_CUTOFF_DATE = datetime(2018, 10, 1)\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Comment letters directory: {COMMENT_LETTERS_DIR}\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\")\n",
    "print(f\"Rating cutoff: {RATING_CUTOFF_DATE.strftime('%B %Y')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid rating components\n",
    "LETTER_RATINGS = ['LO', 'EC', 'EO', 'EU']\n",
    "NUMBER_RATINGS = ['1', '2', '3']\n",
    "\n",
    "# Regex patterns for finding ratings\n",
    "# Pattern 1: Combined rating like \"EC-2\", \"LO-1\", etc.\n",
    "COMBINED_PATTERN = re.compile(\n",
    "    r'\\b(LO|EC|EO|EU)\\s*[-–—]\\s*([123])\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Pattern 2: Rating in context like \"Rating: EC-2\" or \"rated EC-2\"\n",
    "RATING_CONTEXT_PATTERN = re.compile(\n",
    "    r'(?:rating|rated|rate)\\s*[:.]?\\s*(LO|EC|EO|EU)\\s*[-–—]?\\s*([123])?',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Pattern 3: Spelled out ratings\n",
    "SPELLED_OUT_PATTERNS = {\n",
    "    'LO': re.compile(r'lack\\s+of\\s+objections?', re.IGNORECASE),\n",
    "    'EC': re.compile(r'environmental\\s+concerns?', re.IGNORECASE),\n",
    "    'EO': re.compile(r'environmental\\s+objections?', re.IGNORECASE),\n",
    "    'EU': re.compile(r'environmentally\\s+unsatisfactory', re.IGNORECASE),\n",
    "}\n",
    "\n",
    "# Pattern for number ratings in context\n",
    "NUMBER_CONTEXT_PATTERNS = {\n",
    "    '1': re.compile(r'\\b(?:category|adequacy)\\s*[-:]?\\s*1\\b|\\badequate\\b', re.IGNORECASE),\n",
    "    '2': re.compile(r'\\b(?:category|adequacy)\\s*[-:]?\\s*2\\b|\\binsufficient\\s+information\\b', re.IGNORECASE),\n",
    "    '3': re.compile(r'\\b(?:category|adequacy)\\s*[-:]?\\s*3\\b|\\binadequate\\b', re.IGNORECASE),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: Path, max_pages: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        max_pages: Maximum number of pages to read (ratings are usually on first few pages)\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    \n",
    "    try:\n",
    "        if PDF_LIBRARY == \"pdfplumber\":\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for i, page in enumerate(pdf.pages[:max_pages]):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "        \n",
    "        elif PDF_LIBRARY == \"pypdf2\":\n",
    "            with open(pdf_path, 'rb') as f:\n",
    "                reader = PdfReader(f)\n",
    "                for i, page in enumerate(reader.pages[:max_pages]):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "        else:\n",
    "            logger.warning(\"No PDF library available\")\n",
    "            return \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error reading {pdf_path.name}: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_rating(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract EPA rating from text.\n",
    "    \n",
    "    Args:\n",
    "        text: Extracted PDF text\n",
    "    \n",
    "    Returns:\n",
    "        Dict with combined_rating, letter_rating, number_rating\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'combined_rating': None,\n",
    "        'letter_rating': None,\n",
    "        'number_rating': None,\n",
    "        'extraction_method': None\n",
    "    }\n",
    "    \n",
    "    if not text:\n",
    "        return result\n",
    "    \n",
    "    # Method 1: Look for combined rating pattern (most reliable)\n",
    "    combined_match = COMBINED_PATTERN.search(text)\n",
    "    if combined_match:\n",
    "        letter = combined_match.group(1).upper()\n",
    "        number = combined_match.group(2)\n",
    "        result['letter_rating'] = letter\n",
    "        result['number_rating'] = number\n",
    "        result['combined_rating'] = f\"{letter}-{number}\"\n",
    "        result['extraction_method'] = 'combined_pattern'\n",
    "        return result\n",
    "    \n",
    "    # Method 2: Look for rating in context\n",
    "    context_match = RATING_CONTEXT_PATTERN.search(text)\n",
    "    if context_match:\n",
    "        letter = context_match.group(1).upper()\n",
    "        number = context_match.group(2) if context_match.group(2) else None\n",
    "        result['letter_rating'] = letter\n",
    "        result['number_rating'] = number\n",
    "        if number:\n",
    "            result['combined_rating'] = f\"{letter}-{number}\"\n",
    "        else:\n",
    "            result['combined_rating'] = letter\n",
    "        result['extraction_method'] = 'context_pattern'\n",
    "        return result\n",
    "    \n",
    "    # Method 3: Look for spelled-out ratings\n",
    "    found_letter = None\n",
    "    found_number = None\n",
    "    \n",
    "    for letter, pattern in SPELLED_OUT_PATTERNS.items():\n",
    "        if pattern.search(text):\n",
    "            # Verify it's in rating context (near \"rating\" or \"EPA\")\n",
    "            # Find the match position\n",
    "            match = pattern.search(text)\n",
    "            if match:\n",
    "                # Check surrounding context (100 chars before/after)\n",
    "                start = max(0, match.start() - 100)\n",
    "                end = min(len(text), match.end() + 100)\n",
    "                context = text[start:end].lower()\n",
    "                if 'rating' in context or 'epa' in context or 'category' in context:\n",
    "                    found_letter = letter\n",
    "                    break\n",
    "    \n",
    "    for number, pattern in NUMBER_CONTEXT_PATTERNS.items():\n",
    "        if pattern.search(text):\n",
    "            found_number = number\n",
    "            break\n",
    "    \n",
    "    if found_letter:\n",
    "        result['letter_rating'] = found_letter\n",
    "        result['number_rating'] = found_number\n",
    "        if found_number:\n",
    "            result['combined_rating'] = f\"{found_letter}-{found_number}\"\n",
    "        else:\n",
    "            result['combined_rating'] = found_letter\n",
    "        result['extraction_method'] = 'spelled_out'\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Comment Letter Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comment_letter_metadata():\n",
    "    \"\"\"\n",
    "    Load comment letter metadata from available sources.\n",
    "    \"\"\"\n",
    "    # Try API-generated files first\n",
    "    if COMMENT_LETTER_PKL.exists():\n",
    "        df = pd.read_pickle(COMMENT_LETTER_PKL)\n",
    "        logger.info(f\"Loaded {len(df)} records from {COMMENT_LETTER_PKL.name}\")\n",
    "        return df\n",
    "    \n",
    "    if DOC_RECORD_PKL.exists():\n",
    "        df = pd.read_pickle(DOC_RECORD_PKL)\n",
    "        df = df[df['type'] == 'Comment_Letter'].copy()\n",
    "        logger.info(f\"Loaded {len(df)} comment letters from {DOC_RECORD_PKL.name}\")\n",
    "        return df\n",
    "    \n",
    "    # Try to load from EIS records (which may have commentLetterDate)\n",
    "    if EIS_RECORD_PKL.exists():\n",
    "        eis_df = pd.read_pickle(EIS_RECORD_PKL)\n",
    "        logger.info(f\"Loaded EIS records from {EIS_RECORD_PKL.name}\")\n",
    "        return eis_df\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        \"No metadata files found. Run fetch_eis_records_api.ipynb first.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_eis_metadata():\n",
    "    \"\"\"\n",
    "    Load EIS record metadata for dates.\n",
    "    \"\"\"\n",
    "    if EIS_RECORD_PKL.exists():\n",
    "        return pd.read_pickle(EIS_RECORD_PKL)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "comment_df = load_comment_letter_metadata()\n",
    "eis_df = get_eis_metadata()\n",
    "\n",
    "print(f\"Total comment letter records: {len(comment_df)}\")\n",
    "print(f\"\\nColumns: {list(comment_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pre_october_2018(df: pd.DataFrame, eis_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter to comment letters before October 2018.\n",
    "    \n",
    "    Uses commentLetterDate if available, otherwise uses eisId year.\n",
    "    \"\"\"\n",
    "    filtered = df.copy()\n",
    "    \n",
    "    # Try to get date from commentLetterDate column\n",
    "    if 'commentLetterDate' in filtered.columns:\n",
    "        # Parse dates\n",
    "        filtered['_date'] = pd.to_datetime(filtered['commentLetterDate'], errors='coerce')\n",
    "        has_date = filtered['_date'].notna()\n",
    "        filtered.loc[has_date, '_pre_cutoff'] = filtered.loc[has_date, '_date'] < RATING_CUTOFF_DATE\n",
    "    else:\n",
    "        filtered['_pre_cutoff'] = None\n",
    "    \n",
    "    # For records without date, try to merge from EIS records\n",
    "    if eis_df is not None and 'commentLetterDate' in eis_df.columns:\n",
    "        no_date = filtered['_pre_cutoff'].isna()\n",
    "        if no_date.any():\n",
    "            eis_dates = eis_df[['eisId', 'commentLetterDate']].copy()\n",
    "            eis_dates['_eis_date'] = pd.to_datetime(eis_dates['commentLetterDate'], errors='coerce')\n",
    "            filtered = filtered.merge(eis_dates[['eisId', '_eis_date']], on='eisId', how='left')\n",
    "            \n",
    "            # Fill in missing dates\n",
    "            still_no_date = filtered['_pre_cutoff'].isna() & filtered['_eis_date'].notna()\n",
    "            filtered.loc[still_no_date, '_pre_cutoff'] = filtered.loc[still_no_date, '_eis_date'] < RATING_CUTOFF_DATE\n",
    "    \n",
    "    # For remaining records without date, use EIS ID year\n",
    "    still_missing = filtered['_pre_cutoff'].isna()\n",
    "    if still_missing.any():\n",
    "        filtered['_year'] = filtered['eisId'].astype(str).str[:4].astype(int)\n",
    "        # If year < 2018, definitely pre-cutoff; if year > 2018, definitely post-cutoff\n",
    "        # If year == 2018, we can't be sure, so include it to be safe\n",
    "        filtered.loc[still_missing, '_pre_cutoff'] = filtered.loc[still_missing, '_year'] <= 2018\n",
    "    \n",
    "    # Filter\n",
    "    result = filtered[filtered['_pre_cutoff'] == True].copy()\n",
    "    \n",
    "    # Clean up temp columns\n",
    "    cols_to_drop = [c for c in result.columns if c.startswith('_')]\n",
    "    result = result.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to pre-October 2018\n",
    "pre_2018_df = filter_pre_october_2018(comment_df, eis_df)\n",
    "print(f\"Comment letters pre-October 2018: {len(pre_2018_df)}\")\n",
    "\n",
    "# Show year distribution\n",
    "pre_2018_df['year'] = pre_2018_df['eisId'].astype(str).str[:4]\n",
    "print(f\"\\nBy year:\")\n",
    "print(pre_2018_df['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Comment Letter Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"Sanitize filename to match download convention.\"\"\"\n",
    "    clean = re.sub(r'[()&,~\\/]', '', filename)\n",
    "    clean = re.sub(r'[\\s_]+', '_', clean)\n",
    "    clean = re.sub(r'\\.PDF$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = re.sub(r'\\.pdf\\.pdf$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = clean.strip('_')\n",
    "    return clean\n",
    "\n",
    "\n",
    "def find_comment_letter_files(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find actual PDF files for comment letters.\n",
    "    \n",
    "    Checks both:\n",
    "    - documents/comment_letters/ (flat structure)\n",
    "    - documents/{YEAR}/ (by-year structure)\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    result['file_path'] = None\n",
    "    result['file_found'] = False\n",
    "    \n",
    "    # Build expected filenames\n",
    "    def build_expected_filename(row):\n",
    "        name = row.get('name') or row.get('fileNameForDownload') or f\"{row.get('attachmentId', 'unknown')}.pdf\"\n",
    "        return f\"{row['eisId']}_{sanitize_filename(name)}\"\n",
    "    \n",
    "    result['expected_filename'] = result.apply(build_expected_filename, axis=1)\n",
    "    \n",
    "    # Get existing files from comment_letters directory\n",
    "    comment_letter_files = {}\n",
    "    if COMMENT_LETTERS_DIR.exists():\n",
    "        for f in COMMENT_LETTERS_DIR.iterdir():\n",
    "            if f.is_file() and f.suffix.lower() == '.pdf':\n",
    "                comment_letter_files[f.name] = f\n",
    "    \n",
    "    # Get existing files from year directories\n",
    "    year_dir_files = {}\n",
    "    if DOCUMENTS_DIR.exists():\n",
    "        for year_dir in DOCUMENTS_DIR.iterdir():\n",
    "            if year_dir.is_dir() and year_dir.name.isdigit():\n",
    "                for f in year_dir.iterdir():\n",
    "                    if f.is_file() and f.suffix.lower() == '.pdf':\n",
    "                        year_dir_files[f.name] = f\n",
    "    \n",
    "    logger.info(f\"Found {len(comment_letter_files)} files in comment_letters/\")\n",
    "    logger.info(f\"Found {len(year_dir_files)} files in year directories\")\n",
    "    \n",
    "    # Match files\n",
    "    for idx, row in result.iterrows():\n",
    "        expected = row['expected_filename']\n",
    "        eis_id = str(row['eisId'])\n",
    "        year = eis_id[:4]\n",
    "        \n",
    "        # Check comment_letters directory first\n",
    "        if expected in comment_letter_files:\n",
    "            result.at[idx, 'file_path'] = str(comment_letter_files[expected])\n",
    "            result.at[idx, 'file_found'] = True\n",
    "            continue\n",
    "        \n",
    "        # Check year directory\n",
    "        if expected in year_dir_files:\n",
    "            result.at[idx, 'file_path'] = str(year_dir_files[expected])\n",
    "            result.at[idx, 'file_found'] = True\n",
    "            continue\n",
    "        \n",
    "        # Try fuzzy match (same EIS ID prefix)\n",
    "        for fname, fpath in {**comment_letter_files, **year_dir_files}.items():\n",
    "            if fname.startswith(f\"{eis_id}_\") and 'comment' in fname.lower():\n",
    "                result.at[idx, 'file_path'] = str(fpath)\n",
    "                result.at[idx, 'file_found'] = True\n",
    "                break\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files\n",
    "files_df = find_comment_letter_files(pre_2018_df)\n",
    "\n",
    "found_count = files_df['file_found'].sum()\n",
    "total_count = len(files_df)\n",
    "\n",
    "print(f\"\\n=== File Search Results ===\")\n",
    "print(f\"Files found: {found_count} / {total_count} ({100*found_count/total_count:.1f}%)\")\n",
    "print(f\"Files missing: {total_count - found_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comment_letters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all comment letters and extract ratings.\n",
    "    \"\"\"\n",
    "    # Filter to files that were found\n",
    "    to_process = df[df['file_found']].copy()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(to_process.iterrows(), total=len(to_process), desc=\"Extracting ratings\"):\n",
    "        file_path = Path(row['file_path'])\n",
    "        \n",
    "        # Extract text\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        \n",
    "        # Extract rating\n",
    "        rating = extract_rating(text)\n",
    "        \n",
    "        results.append({\n",
    "            'filename': file_path.name,\n",
    "            'eisId': row['eisId'],\n",
    "            'combined_rating': rating['combined_rating'],\n",
    "            'letter_rating': rating['letter_rating'],\n",
    "            'number_rating': rating['number_rating'],\n",
    "            'extraction_method': rating['extraction_method'],\n",
    "            'file_path': str(file_path)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process files\n",
    "if PDF_LIBRARY is None:\n",
    "    print(\"ERROR: No PDF library available. Please install pdfplumber or pypdf2.\")\n",
    "else:\n",
    "    ratings_df = process_comment_letters(files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "if 'ratings_df' in dir() and len(ratings_df) > 0:\n",
    "    print(f\"\\n=== Extraction Results ===\")\n",
    "    print(f\"Total files processed: {len(ratings_df)}\")\n",
    "    \n",
    "    ratings_found = ratings_df['combined_rating'].notna().sum()\n",
    "    print(f\"Ratings found: {ratings_found} ({100*ratings_found/len(ratings_df):.1f}%)\")\n",
    "    print(f\"Ratings not found: {len(ratings_df) - ratings_found}\")\n",
    "    \n",
    "    print(f\"\\n=== Rating Distribution ===\")\n",
    "    print(f\"\\nCombined ratings:\")\n",
    "    print(ratings_df['combined_rating'].value_counts(dropna=False))\n",
    "    \n",
    "    print(f\"\\nLetter ratings:\")\n",
    "    print(ratings_df['letter_rating'].value_counts(dropna=False))\n",
    "    \n",
    "    print(f\"\\nNumber ratings:\")\n",
    "    print(ratings_df['number_rating'].value_counts(dropna=False))\n",
    "    \n",
    "    print(f\"\\nExtraction methods used:\")\n",
    "    print(ratings_df['extraction_method'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview results\n",
    "if 'ratings_df' in dir() and len(ratings_df) > 0:\n",
    "    print(\"\\n=== Sample Results ===\")\n",
    "    display(ratings_df[['filename', 'eisId', 'combined_rating', 'letter_rating', 'number_rating']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "if 'ratings_df' in dir() and len(ratings_df) > 0:\n",
    "    # Select and order columns for output\n",
    "    output_df = ratings_df[['filename', 'eisId', 'combined_rating', 'letter_rating', 'number_rating']].copy()\n",
    "    \n",
    "    # Save\n",
    "    output_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"Saved {len(output_df)} records to {OUTPUT_FILE}\")\n",
    "    \n",
    "    # Also save full results with extraction metadata\n",
    "    full_output = METADATA_DIR / \"comment_letter_ratings_full.csv\"\n",
    "    ratings_df.to_csv(full_output, index=False)\n",
    "    print(f\"Saved full results to {full_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Files Without Ratings\n",
    "\n",
    "These files may need manual review or have different formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show files where rating wasn't found\n",
    "if 'ratings_df' in dir() and len(ratings_df) > 0:\n",
    "    no_rating = ratings_df[ratings_df['combined_rating'].isna()]\n",
    "    \n",
    "    if len(no_rating) > 0:\n",
    "        print(f\"\\n=== Files Without Extracted Ratings ({len(no_rating)}) ===\")\n",
    "        print(\"These may need manual review:\")\n",
    "        display(no_rating[['filename', 'eisId', 'file_path']].head(20))\n",
    "        \n",
    "        # Save list of files needing review\n",
    "        review_file = METADATA_DIR / \"comment_letters_need_review.csv\"\n",
    "        no_rating[['filename', 'eisId', 'file_path']].to_csv(review_file, index=False)\n",
    "        print(f\"\\nSaved list to {review_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: Test Rating Extraction on Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extraction(file_path: str, show_text: bool = True):\n",
    "    \"\"\"\n",
    "    Test rating extraction on a single file.\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    if not path.exists():\n",
    "        print(f\"File not found: {path}\")\n",
    "        return\n",
    "    \n",
    "    text = extract_text_from_pdf(path, max_pages=5)\n",
    "    \n",
    "    if show_text:\n",
    "        print(\"=== Extracted Text (first 3000 chars) ===\")\n",
    "        print(text[:3000])\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    rating = extract_rating(text)\n",
    "    print(f\"\\n=== Extracted Rating ===\")\n",
    "    print(f\"Combined: {rating['combined_rating']}\")\n",
    "    print(f\"Letter: {rating['letter_rating']}\")\n",
    "    print(f\"Number: {rating['number_rating']}\")\n",
    "    print(f\"Method: {rating['extraction_method']}\")\n",
    "\n",
    "# Uncomment to test on a specific file:\n",
    "# test_extraction(\"/path/to/comment_letter.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
