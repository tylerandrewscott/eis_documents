{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Convert EIS Documents to Text using PyMuPDF\n",
    "\n",
    "This notebook converts PDF documents to text using `pymupdf` (fitz), which provides fast extraction suitable for large document corpora.\n",
    "\n",
    "**Key Features:**\n",
    "- Mirrors directory structure from `documents/` to `text_conversions/`\n",
    "- Very fast: processes hundreds of pages per second\n",
    "- Handles both digital and scanned PDFs (with OCR fallback if needed)\n",
    "- Tracks conversion progress to allow resuming\n",
    "- Parallel processing support\n",
    "\n",
    "**Output format:** Plain text files (`.txt`) with page breaks indicated.\n",
    "\n",
    "This replaces the text extraction previously done in `make_filter_text_tables.R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install pymupdf pandas pyarrow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport re\nimport pandas as pd\nimport fitz  # pymupdf\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport logging\nfrom datetime import datetime\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nREPO_ROOT = Path(\"../\").resolve()\nMETADATA_DIR = REPO_ROOT / \"metadata\"\nDOCUMENTS_DIR = REPO_ROOT / \"documents\"  # Source PDFs (may be symlink to Box)\nOUTPUT_DIR = REPO_ROOT / \"text_conversions\"  # Output text files\n\n# Input metadata file\nDOC_RECORD_FILE = METADATA_DIR / \"eis_document_record_api.parquet\"\n\n# Failed files tracking (simple CSV - only tracks failures)\nFAILED_FILES_LOG = METADATA_DIR / \"text_conversion_failures.csv\"\n\nprint(f\"Repository root: {REPO_ROOT}\")\nprint(f\"Documents directory: {DOCUMENTS_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Failed files log: {FAILED_FILES_LOG}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize filename to match existing convention.\n",
    "    - Remove special characters: ( ) & , ~\n",
    "    - Replace spaces with underscores\n",
    "    - Normalize PDF extension\n",
    "    \"\"\"\n",
    "    clean = re.sub(r'[()&,~\\/]', '', filename)\n",
    "    clean = re.sub(r'[\\s_]+', '_', clean)\n",
    "    clean = re.sub(r'\\.PDF$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = re.sub(r'\\.pdf\\.pdf$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = clean.strip('_')\n",
    "    return clean\n",
    "\n",
    "\n",
    "def build_local_filename(ceq_number, original_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the local filename following existing convention.\n",
    "    Format: {CEQ_NUMBER}_{sanitized_filename}\n",
    "    \"\"\"\n",
    "    sanitized = sanitize_filename(original_filename)\n",
    "    return f\"{ceq_number}_{sanitized}\"\n",
    "\n",
    "\n",
    "def get_year_from_ceq(ceq_number) -> str:\n",
    "    \"\"\"Extract year from CEQ Number (first 4 digits).\"\"\"\n",
    "    return str(ceq_number)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "def extract_text_from_pdf(pdf_path: Path, max_pages: int = 5000) -> tuple[str, int, bool]:\n    \"\"\"\n    Extract text from a PDF using pymupdf with defensive error handling.\n    \n    Args:\n        pdf_path: Path to the PDF file\n        max_pages: Maximum pages to process (skip very large docs)\n    \n    Returns:\n        Tuple of (extracted_text, num_pages, has_text)\n    \"\"\"\n    doc = None\n    try:\n        # Check file size first (skip files > 500MB)\n        file_size = pdf_path.stat().st_size\n        if file_size > 500 * 1024 * 1024:\n            return f\"SKIPPED: File too large ({file_size / 1024 / 1024:.0f} MB)\", 0, False\n        \n        doc = fitz.open(pdf_path)\n        num_pages = len(doc)\n        \n        if num_pages == 0:\n            doc.close()\n            return \"\", 0, False\n        \n        if num_pages > max_pages:\n            doc.close()\n            return f\"SKIPPED: Too many pages ({num_pages})\", num_pages, False\n        \n        text_parts = []\n        total_chars = 0\n        \n        for page_num in range(num_pages):\n            try:\n                page = doc[page_num]\n                page_text = page.get_text()\n                total_chars += len(page_text.strip())\n                \n                # Add page marker and text\n                text_parts.append(f\"\\n\\n--- PAGE {page_num + 1} ---\\n\\n\")\n                text_parts.append(page_text)\n            except Exception as page_error:\n                # Skip problematic pages but continue\n                text_parts.append(f\"\\n\\n--- PAGE {page_num + 1} (ERROR: {str(page_error)[:100]}) ---\\n\\n\")\n        \n        doc.close()\n        doc = None\n        \n        full_text = \"\".join(text_parts)\n        has_text = total_chars > (num_pages * 50)  # At least 50 chars per page average\n        \n        return full_text, num_pages, has_text\n        \n    except Exception as e:\n        if doc is not None:\n            try:\n                doc.close()\n            except:\n                pass\n        return f\"ERROR: {str(e)[:200]}\", 0, False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "def convert_single_pdf(args: tuple) -> dict:\n    \"\"\"\n    Convert a single PDF to text with robust error handling.\n    \n    Args:\n        args: Tuple of (pdf_path, output_path, ceq_number, attachment_id)\n    \n    Returns:\n        Dict with conversion status\n    \"\"\"\n    pdf_path, output_path, ceq_number, attachment_id = args\n    pdf_path = Path(pdf_path)\n    output_path = Path(output_path)\n    \n    try:\n        # Check if source exists\n        if not pdf_path.exists():\n            return {\n                \"ceqNumber\": ceq_number,\n                \"attachmentId\": attachment_id,\n                \"source_file\": str(pdf_path),\n                \"output_file\": None,\n                \"converted\": False,\n                \"num_pages\": 0,\n                \"has_text\": False,\n                \"error\": \"Source file not found\",\n                \"timestamp\": datetime.now().isoformat()\n            }\n        \n        # Extract text\n        text, num_pages, has_text = extract_text_from_pdf(pdf_path)\n        \n        # Check for errors or skipped files\n        if text.startswith(\"ERROR:\") or text.startswith(\"SKIPPED:\"):\n            return {\n                \"ceqNumber\": ceq_number,\n                \"attachmentId\": attachment_id,\n                \"source_file\": str(pdf_path),\n                \"output_file\": None,\n                \"converted\": False,\n                \"num_pages\": num_pages,\n                \"has_text\": False,\n                \"error\": text,\n                \"timestamp\": datetime.now().isoformat()\n            }\n        \n        # Ensure output directory exists\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        # Write text file\n        with open(output_path, 'w', encoding='utf-8', errors='replace') as f:\n            f.write(text)\n        \n        return {\n            \"ceqNumber\": ceq_number,\n            \"attachmentId\": attachment_id,\n            \"source_file\": str(pdf_path),\n            \"output_file\": str(output_path),\n            \"converted\": True,\n            \"num_pages\": num_pages,\n            \"has_text\": has_text,\n            \"error\": None,\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n    except Exception as e:\n        return {\n            \"ceqNumber\": ceq_number,\n            \"attachmentId\": attachment_id,\n            \"source_file\": str(pdf_path),\n            \"output_file\": None,\n            \"converted\": False,\n            \"num_pages\": 0,\n            \"has_text\": False,\n            \"error\": f\"CONVERT_ERROR: {str(e)[:400]}\",\n            \"timestamp\": datetime.now().isoformat()\n        }"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Load Document Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "def load_document_records():\n    \"\"\"Load document records from the API metadata.\"\"\"\n    if DOC_RECORD_FILE.exists():\n        return pd.read_parquet(DOC_RECORD_FILE)\n    else:\n        raise FileNotFoundError(\n            f\"Document records not found at {DOC_RECORD_FILE}.\\n\"\n            f\"Run fetch_eis_records_api.ipynb first.\"\n        )\n\n\ndef load_failed_files() -> set:\n    \"\"\"Load set of attachment IDs that previously failed.\"\"\"\n    if FAILED_FILES_LOG.exists():\n        df = pd.read_csv(FAILED_FILES_LOG)\n        return set(df['attachmentId'].astype(str))\n    return set()\n\n\ndef append_failed_file(attachment_id, ceq_number, source_file, error):\n    \"\"\"Append a failed file to the log.\"\"\"\n    file_exists = FAILED_FILES_LOG.exists()\n    with open(FAILED_FILES_LOG, 'a', encoding='utf-8') as f:\n        if not file_exists:\n            f.write(\"attachmentId,ceqNumber,source_file,error,timestamp\\n\")\n        # Escape commas and quotes in error message\n        error_clean = str(error).replace('\"', \"'\").replace('\\n', ' ')[:200]\n        f.write(f'{attachment_id},{ceq_number},\"{source_file}\",\"{error_clean}\",{datetime.now().isoformat()}\\n')"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45704 document records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ceqNumber</th>\n",
       "      <th>year</th>\n",
       "      <th>localFilename</th>\n",
       "      <th>sourcePath</th>\n",
       "      <th>outputPath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_LoMo_FRR_Comprehensive_Study_Draft_Re...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_LoMo_System_Plan_-_Basis_of_Estimate.pdf</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_Appendix_A.1_LoMo_FRM_Past_Performanc...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_Appendix_A.2.1_LoMo_RAS_Calibration_O...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_Appendix_A.2.2_LoMo_RAS_Calibration_K...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ceqNumber  year                                      localFilename  \\\n",
       "0  20250186  2025  20250186_LoMo_FRR_Comprehensive_Study_Draft_Re...   \n",
       "1  20250186  2025  20250186_LoMo_System_Plan_-_Basis_of_Estimate.pdf   \n",
       "2  20250186  2025  20250186_Appendix_A.1_LoMo_FRM_Past_Performanc...   \n",
       "3  20250186  2025  20250186_Appendix_A.2.1_LoMo_RAS_Calibration_O...   \n",
       "4  20250186  2025  20250186_Appendix_A.2.2_LoMo_RAS_Calibration_K...   \n",
       "\n",
       "                                          sourcePath  \\\n",
       "0  /Users/admin-tascott/Documents/GitHub/eis_docu...   \n",
       "1  /Users/admin-tascott/Documents/GitHub/eis_docu...   \n",
       "2  /Users/admin-tascott/Documents/GitHub/eis_docu...   \n",
       "3  /Users/admin-tascott/Documents/GitHub/eis_docu...   \n",
       "4  /Users/admin-tascott/Documents/GitHub/eis_docu...   \n",
       "\n",
       "                                          outputPath  \n",
       "0  /Users/admin-tascott/Documents/GitHub/eis_docu...  \n",
       "1  /Users/admin-tascott/Documents/GitHub/eis_docu...  \n",
       "2  /Users/admin-tascott/Documents/GitHub/eis_docu...  \n",
       "3  /Users/admin-tascott/Documents/GitHub/eis_docu...  \n",
       "4  /Users/admin-tascott/Documents/GitHub/eis_docu...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load document records\n",
    "doc_df = load_document_records()\n",
    "print(f\"Loaded {len(doc_df)} document records\")\n",
    "\n",
    "# Add helper columns\n",
    "doc_df['year'] = doc_df['ceqNumber'].astype(str).str[:4]\n",
    "doc_df['localFilename'] = doc_df.apply(\n",
    "    lambda row: build_local_filename(\n",
    "        row['ceqNumber'], \n",
    "        row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Build source and output paths\n",
    "doc_df['sourcePath'] = doc_df.apply(\n",
    "    lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n",
    "    axis=1\n",
    ")\n",
    "doc_df['outputPath'] = doc_df.apply(\n",
    "    lambda row: OUTPUT_DIR / row['year'] / (row['localFilename'].replace('.pdf', '.txt').replace('.PDF', '.txt')),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "display(doc_df[['ceqNumber', 'year', 'localFilename', 'sourcePath', 'outputPath']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files found: 45559 / 45704\n",
      "Documents available for conversion: 45559\n",
      "\n",
      "By year:\n",
      "year\n",
      "1987       2\n",
      "1988       1\n",
      "1990       2\n",
      "1991       4\n",
      "1992       3\n",
      "1993       3\n",
      "1994      10\n",
      "1995      20\n",
      "1996      59\n",
      "1997      82\n",
      "1998      58\n",
      "1999     202\n",
      "2000     369\n",
      "2001     481\n",
      "2002     596\n",
      "2003     562\n",
      "2004     631\n",
      "2005     755\n",
      "2006     712\n",
      "2007     912\n",
      "2008     911\n",
      "2009     894\n",
      "2010     661\n",
      "2011     406\n",
      "2012     991\n",
      "2013    3007\n",
      "2014    2918\n",
      "2015    3991\n",
      "2016    3651\n",
      "2017    2697\n",
      "2018    3239\n",
      "2019    2865\n",
      "2020    3280\n",
      "2021    2309\n",
      "2022    2206\n",
      "2023    1633\n",
      "2024    2498\n",
      "2025    1938\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check which source files exist\n",
    "doc_df['sourceExists'] = doc_df['sourcePath'].apply(lambda p: p.exists())\n",
    "print(f\"Source files found: {doc_df['sourceExists'].sum()} / {len(doc_df)}\")\n",
    "\n",
    "# Documents with source files available for conversion\n",
    "to_convert = doc_df[doc_df['sourceExists']].copy()\n",
    "print(f\"Documents available for conversion: {len(to_convert)}\")\n",
    "\n",
    "print(f\"\\nBy year:\")\n",
    "print(to_convert['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Conversion Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CONVERSION SETTINGS - MODIFY AS NEEDED\n# ============================================\n\n# Filter by year (set to None to convert all years)\nYEAR_FILTER = None\n# YEAR_FILTER = [2024, 2025]  # Example: only recent years\n\n# Maximum number of files to convert (set to None for all)\nMAX_CONVERSIONS = None\n# MAX_CONVERSIONS = 100  # Example: test with 100 files\n\n# Set to True to retry previously failed files\nRETRY_FAILURES = False\n\nprint(f\"Settings:\")\nprint(f\"  YEAR_FILTER: {YEAR_FILTER}\")\nprint(f\"  MAX_CONVERSIONS: {MAX_CONVERSIONS}\")\nprint(f\"  RETRY_FAILURES: {RETRY_FAILURES}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Build conversion queue\nconversion_queue = to_convert.copy()\n\n# Apply year filter\nif YEAR_FILTER:\n    year_filter_str = [str(y) for y in YEAR_FILTER]\n    conversion_queue = conversion_queue[conversion_queue['year'].isin(year_filter_str)]\n    print(f\"Filtered to years {YEAR_FILTER}: {len(conversion_queue)} documents\")\n\n# Skip files that already have .txt output\nconversion_queue['outputExists'] = conversion_queue['outputPath'].apply(lambda p: p.exists())\nexisting_count = conversion_queue['outputExists'].sum()\nprint(f\"Already converted (txt exists): {existing_count}\")\nconversion_queue = conversion_queue[~conversion_queue['outputExists']]\n\n# Skip previously failed files (unless RETRY_FAILURES is True)\nif not RETRY_FAILURES:\n    failed_ids = load_failed_files()\n    if failed_ids:\n        conversion_queue['previouslyFailed'] = conversion_queue['attachmentId'].astype(str).isin(failed_ids)\n        failed_count = conversion_queue['previouslyFailed'].sum()\n        print(f\"Previously failed (skipping): {failed_count}\")\n        conversion_queue = conversion_queue[~conversion_queue['previouslyFailed']]\n\n# Apply max conversions limit\nif MAX_CONVERSIONS and len(conversion_queue) > MAX_CONVERSIONS:\n    conversion_queue = conversion_queue.head(MAX_CONVERSIONS)\n    print(f\"Limited to {MAX_CONVERSIONS} conversions\")\n\nprint(f\"\\nFinal conversion queue: {len(conversion_queue)} files\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Create Output Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 year directories in documents/\n",
      "Created directory structure in /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/text_conversions\n"
     ]
    }
   ],
   "source": [
    "# Create output directory structure mirroring documents/\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get all year directories from documents\n",
    "year_dirs = [d for d in DOCUMENTS_DIR.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "print(f\"Found {len(year_dirs)} year directories in documents/\")\n",
    "\n",
    "# Create corresponding directories in marker_conversions/\n",
    "for year_dir in year_dirs:\n",
    "    output_year_dir = OUTPUT_DIR / year_dir.name\n",
    "    output_year_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created directory structure in {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Run Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "def run_conversions(queue: pd.DataFrame) -> dict:\n    \"\"\"\n    Run text extraction on all files in the queue.\n    Failures are logged immediately to the failures CSV.\n    \n    Args:\n        queue: DataFrame with sourcePath and outputPath columns\n    \n    Returns:\n        Dict with summary counts\n    \"\"\"\n    success_count = 0\n    fail_count = 0\n    total_pages = 0\n    \n    for _, row in tqdm(queue.iterrows(), total=len(queue), desc=\"Converting\"):\n        args = (str(row['sourcePath']), str(row['outputPath']), row['ceqNumber'], row['attachmentId'])\n        \n        try:\n            result = convert_single_pdf(args)\n            \n            if result['converted']:\n                success_count += 1\n                total_pages += result.get('num_pages', 0)\n            else:\n                fail_count += 1\n                append_failed_file(\n                    row['attachmentId'],\n                    row['ceqNumber'],\n                    str(row['sourcePath']),\n                    result.get('error', 'Unknown error')\n                )\n        except Exception as e:\n            fail_count += 1\n            append_failed_file(\n                row['attachmentId'],\n                row['ceqNumber'],\n                str(row['sourcePath']),\n                f\"EXCEPTION: {str(e)[:200]}\"\n            )\n    \n    return {\n        'success': success_count,\n        'failed': fail_count,\n        'total_pages': total_pages\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Run the conversions\nif len(conversion_queue) > 0:\n    print(f\"Starting conversion of {len(conversion_queue)} files...\")\n    print(f\"Failures logged to: {FAILED_FILES_LOG}\")\n    \n    import time\n    start_time = time.time()\n    \n    results = run_conversions(conversion_queue)\n    \n    elapsed = time.time() - start_time\n    \n    print(f\"\\n=== Conversion Summary ===\")\n    print(f\"Time elapsed: {elapsed/60:.1f} minutes\")\n    print(f\"Successful: {results['success']}\")\n    print(f\"Failed: {results['failed']}\")\n    print(f\"Total pages: {results['total_pages']:,}\")\n    if elapsed > 0:\n        print(f\"Rate: {(results['success'] + results['failed'])/elapsed:.1f} docs/sec\")\n    \n    if results['failed'] > 0:\n        print(f\"\\nSee {FAILED_FILES_LOG} for failure details\")\nelse:\n    print(\"No files to convert.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e30f1-d77f-47d6-acf1-69fa99624454",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_results['source_file'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Verify Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "def verify_conversions():\n    \"\"\"Quick verification of conversion status.\"\"\"\n    doc_df = load_document_records()\n    \n    # Build paths\n    doc_df['year'] = doc_df['ceqNumber'].astype(str).str[:4]\n    doc_df['localFilename'] = doc_df.apply(\n        lambda row: build_local_filename(\n            row['ceqNumber'], \n            row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n        ),\n        axis=1\n    )\n    doc_df['sourcePath'] = doc_df.apply(\n        lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n        axis=1\n    )\n    doc_df['outputPath'] = doc_df.apply(\n        lambda row: OUTPUT_DIR / row['year'] / (row['localFilename'].replace('.pdf', '.txt').replace('.PDF', '.txt')),\n        axis=1\n    )\n    \n    # Check status\n    doc_df['sourceExists'] = doc_df['sourcePath'].apply(lambda p: p.exists())\n    doc_df['outputExists'] = doc_df['outputPath'].apply(lambda p: p.exists())\n    \n    with_source = doc_df[doc_df['sourceExists']]\n    total = len(with_source)\n    converted = with_source['outputExists'].sum()\n    \n    # Count failures\n    failed_ids = load_failed_files()\n    \n    print(f\"=== Conversion Status ===\")\n    print(f\"Source PDFs: {total}\")\n    print(f\"Converted: {converted} ({100*converted/total:.1f}%)\")\n    print(f\"Failed: {len(failed_ids)}\")\n    print(f\"Remaining: {total - converted - len(failed_ids)}\")\n\nverify_conversions()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Retry Failed Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# View failed files\ndef show_failures():\n    if FAILED_FILES_LOG.exists():\n        df = pd.read_csv(FAILED_FILES_LOG)\n        print(f\"Total failures: {len(df)}\")\n        display(df)\n    else:\n        print(\"No failures logged.\")\n\n# Uncomment to view failures:\n# show_failures()\n\n# To retry failures, set RETRY_FAILURES = True in cell 13 and re-run\n# To clear failure log and start fresh: \n# FAILED_FILES_LOG.unlink()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Sample Output Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "# Inspect a sample converted file\ndef show_sample_output(n_chars: int = 2000):\n    \"\"\"Display the beginning of a random converted text file.\"\"\"\n    txt_files = list(OUTPUT_DIR.glob(\"**/*.txt\"))\n    \n    if not txt_files:\n        print(\"No converted files found.\")\n        return\n    \n    import random\n    sample_file = random.choice(txt_files)\n    \n    print(f\"File: {sample_file.name}\")\n    print(f\"Size: {sample_file.stat().st_size:,} bytes\")\n    print(\"=\" * 60)\n    \n    with open(sample_file, 'r', encoding='utf-8') as f:\n        print(f.read(n_chars))\n\n# Uncomment to see a sample:\n# show_sample_output()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}