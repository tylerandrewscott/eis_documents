{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch EPA Comment Letters via API\n",
    "\n",
    "This notebook extracts EPA comment letter records from the E-NEPA API.\n",
    "\n",
    "Comment letters are included in the main EIS record attachments with `type: \"Comment_Letter\"`. This notebook:\n",
    "1. Fetches all EIS records (or uses existing data from `fetch_eis_records_api.ipynb`)\n",
    "2. Filters to only comment letter attachments\n",
    "3. Saves a dedicated comment letter metadata file\n",
    "\n",
    "**API Documentation:** https://cdxapps.epa.gov/cdx-enepa-II/apidocs/index.html\n",
    "\n",
    "## Output\n",
    "- `comment_letter_record_api.parquet` - All comment letter records with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install requests pandas pyarrow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_URL = \"https://cdxapps.epa.gov/cdx-enepa-II/rest\"\n",
    "SEARCH_ENDPOINT = f\"{BASE_URL}/public/v1/eis/search\"\n",
    "\n",
    "# Output paths - relative to repository root\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "METADATA_DIR = REPO_ROOT / \"metadata\"\n",
    "\n",
    "# Ensure metadata directory exists\n",
    "METADATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Input files (from fetch_eis_records_api.ipynb)\n",
    "EIS_RECORD_PKL = METADATA_DIR / \"eis_record_api.pkl\"\n",
    "DOC_RECORD_PKL = METADATA_DIR / \"eis_document_record_api.pkl\"\n",
    "\n",
    "# Output files\n",
    "COMMENT_LETTER_FILE = METADATA_DIR / \"comment_letter_record_api.parquet\"\n",
    "COMMENT_LETTER_PKL = METADATA_DIR / \"comment_letter_record_api.pkl\"\n",
    "\n",
    "# Year range for fetching (if fetching fresh)\n",
    "START_YEAR = 1987\n",
    "END_YEAR = datetime.now().year\n",
    "\n",
    "# Rate limiting\n",
    "REQUEST_DELAY = 0.5\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Metadata directory: {METADATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FETCH SETTINGS - MODIFY AS NEEDED\n",
    "# ============================================\n",
    "\n",
    "# Set to True to fetch fresh data from the API\n",
    "# Set to False to use existing data from fetch_eis_records_api.ipynb (faster)\n",
    "FETCH_FRESH = False\n",
    "\n",
    "# If FETCH_FRESH=True, set to True to re-fetch ALL records\n",
    "# Set to False to only fetch NEW records not already in the database\n",
    "OVERWRITE = False\n",
    "\n",
    "# Year range (only used if FETCH_FRESH=True)\n",
    "FETCH_START_YEAR = 1987\n",
    "FETCH_END_YEAR = datetime.now().year\n",
    "\n",
    "print(f\"=== Fetch Configuration ===\")\n",
    "print(f\"  FETCH_FRESH: {FETCH_FRESH}\")\n",
    "if FETCH_FRESH:\n",
    "    print(f\"  OVERWRITE: {OVERWRITE}\")\n",
    "    print(f\"  Year range: {FETCH_START_YEAR} to {FETCH_END_YEAR}\")\n",
    "else:\n",
    "    print(f\"  Will use existing data from: {DOC_RECORD_PKL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_eis_with_comment_letters(start_date: str, end_date: str, max_retries: int = 3) -> list:\n",
    "    \"\"\"\n",
    "    Search for EIS records that have comment letters within a date range.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date in MM/dd/yyyy format\n",
    "        end_date: End date in MM/dd/yyyy format\n",
    "        max_retries: Number of retry attempts on failure\n",
    "    \n",
    "    Returns:\n",
    "        List of EIS record dictionaries\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"startFRDate\": start_date,\n",
    "        \"endFRDate\": end_date,\n",
    "        \"onlyCommentLetters\": \"true\"  # Only get records with comment letters\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(SEARCH_ENDPOINT, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1}/{max_retries} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                logger.error(f\"Failed to fetch records for {start_date} to {end_date}\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def extract_comment_letters(record: dict) -> list:\n",
    "    \"\"\"\n",
    "    Extract comment letter attachments from an EIS record.\n",
    "    \n",
    "    Args:\n",
    "        record: Raw EIS record from API\n",
    "    \n",
    "    Returns:\n",
    "        List of comment letter dictionaries\n",
    "    \"\"\"\n",
    "    eis_id = record.get(\"eisId\")\n",
    "    title = record.get(\"title\")\n",
    "    lead_agency = record.get(\"leadAgency\")\n",
    "    comment_letter_date = record.get(\"commentLetterDate\")\n",
    "    attachments = record.get(\"attachments\", [])\n",
    "    \n",
    "    # Extract states\n",
    "    states = record.get(\"states\", [])\n",
    "    states_str = \", \".join([s.get(\"name\", \"\") for s in states]) if states else None\n",
    "    \n",
    "    letters = []\n",
    "    for att in attachments:\n",
    "        # Filter to only comment letters\n",
    "        if att.get(\"type\") == \"Comment_Letter\":\n",
    "            letter = {\n",
    "                \"eisId\": eis_id,\n",
    "                \"eisTitle\": title,\n",
    "                \"leadAgency\": lead_agency,\n",
    "                \"states\": states_str,\n",
    "                \"commentLetterDate\": comment_letter_date,\n",
    "                \"attachmentId\": att.get(\"id\"),\n",
    "                \"name\": att.get(\"name\"),\n",
    "                \"title\": att.get(\"title\"),\n",
    "                \"fileNameForDownload\": att.get(\"fileNameForDownload\"),\n",
    "                \"size\": att.get(\"size\"),\n",
    "                \"sizeKb\": att.get(\"sizeKb\"),\n",
    "                \"pages\": att.get(\"pages\"),\n",
    "            }\n",
    "            letters.append(letter)\n",
    "    \n",
    "    return letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Comment Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_comment_letters():\n",
    "    \"\"\"\n",
    "    Load existing comment letter records if available.\n",
    "    \"\"\"\n",
    "    if COMMENT_LETTER_PKL.exists():\n",
    "        try:\n",
    "            df = pd.read_pickle(COMMENT_LETTER_PKL)\n",
    "            logger.info(f\"Loaded {len(df)} existing comment letter records\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load existing records: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_comment_letters(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Save comment letter records.\n",
    "    \"\"\"\n",
    "    df.to_pickle(COMMENT_LETTER_PKL)\n",
    "    df.to_parquet(COMMENT_LETTER_FILE, index=False)\n",
    "    df.to_csv(METADATA_DIR / \"comment_letter_record_api.csv\", index=False)\n",
    "    logger.info(f\"Saved {len(df)} comment letter records\")\n",
    "\n",
    "\n",
    "def fetch_from_existing_data():\n",
    "    \"\"\"\n",
    "    Extract comment letters from existing document records.\n",
    "    \"\"\"\n",
    "    if not DOC_RECORD_PKL.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Document records not found at {DOC_RECORD_PKL}\\n\"\n",
    "            \"Run fetch_eis_records_api.ipynb first, or set FETCH_FRESH=True\"\n",
    "        )\n",
    "    \n",
    "    # Load document records\n",
    "    doc_df = pd.read_pickle(DOC_RECORD_PKL)\n",
    "    logger.info(f\"Loaded {len(doc_df)} document records\")\n",
    "    \n",
    "    # Filter to comment letters\n",
    "    comment_letters = doc_df[doc_df['type'] == 'Comment_Letter'].copy()\n",
    "    logger.info(f\"Found {len(comment_letters)} comment letters\")\n",
    "    \n",
    "    # Load EIS records to get additional metadata\n",
    "    if EIS_RECORD_PKL.exists():\n",
    "        eis_df = pd.read_pickle(EIS_RECORD_PKL)\n",
    "        # Merge to get EIS title, agency, etc.\n",
    "        comment_letters = comment_letters.merge(\n",
    "            eis_df[['eisId', 'title', 'leadAgency', 'states', 'commentLetterDate']],\n",
    "            on='eisId',\n",
    "            how='left',\n",
    "            suffixes=('', '_eis')\n",
    "        )\n",
    "        # Rename columns to match fresh fetch format\n",
    "        comment_letters = comment_letters.rename(columns={'title_eis': 'eisTitle'})\n",
    "        if 'title' in comment_letters.columns and 'eisTitle' not in comment_letters.columns:\n",
    "            # title from doc_df is the attachment title, not EIS title\n",
    "            pass\n",
    "    \n",
    "    return comment_letters\n",
    "\n",
    "\n",
    "def fetch_comment_letters_fresh(start_year: int, end_year: int, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Fetch comment letters directly from the API.\n",
    "    \n",
    "    Uses the onlyCommentLetters=true filter for efficiency.\n",
    "    \"\"\"\n",
    "    existing_df = load_existing_comment_letters()\n",
    "    \n",
    "    if overwrite or existing_df is None:\n",
    "        all_letters = []\n",
    "        existing_ids = set()\n",
    "    else:\n",
    "        all_letters = existing_df.to_dict('records')\n",
    "        existing_ids = set(existing_df['attachmentId'].astype(str))\n",
    "        logger.info(f\"Starting with {len(existing_ids)} existing records\")\n",
    "    \n",
    "    years = list(range(end_year, start_year - 1, -1))\n",
    "    \n",
    "    for year in tqdm(years, desc=\"Fetching years\"):\n",
    "        start_date = f\"01/01/{year}\"\n",
    "        end_date = f\"12/31/{year}\"\n",
    "        \n",
    "        records = search_eis_with_comment_letters(start_date, end_date)\n",
    "        \n",
    "        new_count = 0\n",
    "        for record in records:\n",
    "            letters = extract_comment_letters(record)\n",
    "            for letter in letters:\n",
    "                att_id = str(letter.get(\"attachmentId\"))\n",
    "                if not overwrite and att_id in existing_ids:\n",
    "                    continue\n",
    "                all_letters.append(letter)\n",
    "                existing_ids.add(att_id)\n",
    "                new_count += 1\n",
    "        \n",
    "        logger.info(f\"Year {year}: Found {len(records)} EIS with letters, {new_count} new letters\")\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "    \n",
    "    return pd.DataFrame(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch comment letters\n",
    "if FETCH_FRESH:\n",
    "    print(\"Fetching comment letters directly from API...\")\n",
    "    comment_df = fetch_comment_letters_fresh(\n",
    "        start_year=FETCH_START_YEAR,\n",
    "        end_year=FETCH_END_YEAR,\n",
    "        overwrite=OVERWRITE\n",
    "    )\n",
    "else:\n",
    "    print(\"Extracting comment letters from existing data...\")\n",
    "    comment_df = fetch_from_existing_data()\n",
    "\n",
    "# Save\n",
    "save_comment_letters(comment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "print(f\"\\n=== Comment Letter Summary ===\")\n",
    "print(f\"Total comment letters: {len(comment_df)}\")\n",
    "\n",
    "# By year (from EIS ID)\n",
    "comment_df['year'] = comment_df['eisId'].astype(str).str[:4]\n",
    "print(f\"\\nComment letters by year:\")\n",
    "print(comment_df['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview records\n",
    "print(\"\\n=== Sample Records ===\")\n",
    "display_cols = ['eisId', 'attachmentId', 'name', 'sizeKb', 'pages']\n",
    "if 'eisTitle' in comment_df.columns:\n",
    "    display_cols.insert(1, 'eisTitle')\n",
    "if 'leadAgency' in comment_df.columns:\n",
    "    display_cols.insert(2, 'leadAgency')\n",
    "display(comment_df[display_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "print(f\"\\n=== Statistics ===\")\n",
    "print(f\"Total file size: {comment_df['size'].sum() / (1024**3):.2f} GB\")\n",
    "print(f\"Average pages per letter: {comment_df['pages'].mean():.1f}\")\n",
    "print(f\"Unique EIS projects with letters: {comment_df['eisId'].nunique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
