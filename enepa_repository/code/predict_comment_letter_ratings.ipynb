{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict EPA Comment Letter Ratings Using Transformer Embeddings\n",
    "\n",
    "This notebook uses transformer models to predict EPA ratings for comment letters published after October 2018 (when EPA stopped providing ratings) based on the content of pre-October 2018 letters that have ratings.\n",
    "\n",
    "## Approach\n",
    "1. Load pre-October 2018 letters with known ratings (training data)\n",
    "2. Load post-October 2018 letters (prediction targets)\n",
    "3. Preprocess text to extract letter body (excluding headers, footers, salutations, ratings)\n",
    "4. Generate embeddings using transformer models\n",
    "5. Train classifiers for letter rating (LO/EC/EO/EU) and number rating (1/2/3)\n",
    "6. Predict ratings for post-2018 letters\n",
    "\n",
    "## Model Options\n",
    "- **sentence-transformers/all-mpnet-base-v2** (default): High-quality general embeddings, fast\n",
    "- **nlpaueb/legal-bert-base-uncased**: Domain-specific legal text model\n",
    "- **joelniklaus/legal-english-longformer-base**: Better for longer documents\n",
    "\n",
    "## References\n",
    "- [Legal-BERT](https://huggingface.co/nlpaueb/legal-bert-base-uncased)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Legal English Longformer](https://huggingface.co/joelniklaus/legal-english-longformer-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install sentence-transformers transformers torch scikit-learn pandas numpy tqdm pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check for GPU\n",
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "METADATA_DIR = REPO_ROOT / \"metadata\"\n",
    "DOCUMENTS_DIR = REPO_ROOT / \"documents\"\n",
    "COMMENT_LETTERS_DIR = DOCUMENTS_DIR / \"comment_letters\"\n",
    "\n",
    "# Input files\n",
    "RATINGS_FILE = METADATA_DIR / \"comment_letter_ratings.csv\"\n",
    "COMMENT_LETTER_PKL = METADATA_DIR / \"comment_letter_record_api.pkl\"\n",
    "DOC_RECORD_PKL = METADATA_DIR / \"eis_document_record_api.pkl\"\n",
    "EIS_RECORD_PKL = METADATA_DIR / \"eis_record_api.pkl\"\n",
    "\n",
    "# Output files\n",
    "EMBEDDINGS_DIR = METADATA_DIR / \"embeddings\"\n",
    "PREDICTED_RATINGS_FILE = METADATA_DIR / \"comment_letter_ratings_predicted.csv\"\n",
    "MODEL_DIR = METADATA_DIR / \"models\"\n",
    "\n",
    "# Create directories\n",
    "EMBEDDINGS_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Cutoff date\n",
    "RATING_CUTOFF_DATE = datetime(2018, 10, 1)\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Ratings file: {RATINGS_FILE}\")\n",
    "print(f\"Output: {PREDICTED_RATINGS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODEL SETTINGS - MODIFY AS NEEDED\n",
    "# ============================================\n",
    "\n",
    "# Embedding model options:\n",
    "# - \"sentence-transformers/all-mpnet-base-v2\" (recommended, good quality, fast)\n",
    "# - \"sentence-transformers/all-MiniLM-L6-v2\" (faster, slightly lower quality)\n",
    "# - \"nlpaueb/legal-bert-base-uncased\" (legal domain-specific)\n",
    "# - \"joelniklaus/legal-english-longformer-base\" (better for long documents)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Use cached embeddings if available (set to False to regenerate)\n",
    "USE_CACHED_EMBEDDINGS = True\n",
    "\n",
    "# Maximum text length (in characters) - longer texts will be truncated\n",
    "# Models have token limits: BERT ~512 tokens, Longformer ~4096 tokens\n",
    "MAX_TEXT_LENGTH = 10000  # ~2000 tokens\n",
    "\n",
    "# Classifier to use: \"logistic\", \"svm\", \"rf\", \"mlp\", \"ensemble\"\n",
    "CLASSIFIER_TYPE = \"ensemble\"\n",
    "\n",
    "# Cross-validation folds\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# Test split ratio (for final evaluation)\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "print(f\"=== Configuration ===\")\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Use cached embeddings: {USE_CACHED_EMBEDDINGS}\")\n",
    "print(f\"Max text length: {MAX_TEXT_LENGTH}\")\n",
    "print(f\"Classifier: {CLASSIFIER_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Extract the main body of the letter, excluding:\n",
    "- Headers (EPA letterhead, dates, addresses)\n",
    "- Footers (page numbers, signatures)\n",
    "- Salutations (\"Dear...\", \"Sincerely...\")\n",
    "- Rating statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF text extraction\n",
    "try:\n",
    "    import pdfplumber\n",
    "    PDF_LIBRARY = \"pdfplumber\"\n",
    "except ImportError:\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader\n",
    "        PDF_LIBRARY = \"pypdf2\"\n",
    "    except ImportError:\n",
    "        PDF_LIBRARY = None\n",
    "        print(\"WARNING: Install pdfplumber or pypdf2 for PDF extraction\")\n",
    "\n",
    "print(f\"PDF library: {PDF_LIBRARY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: Path, max_pages: int = None) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        if PDF_LIBRARY == \"pdfplumber\":\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                pages = pdf.pages[:max_pages] if max_pages else pdf.pages\n",
    "                for page in pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "        elif PDF_LIBRARY == \"pypdf2\":\n",
    "            with open(pdf_path, 'rb') as f:\n",
    "                reader = PdfReader(f)\n",
    "                pages = reader.pages[:max_pages] if max_pages else reader.pages\n",
    "                for page in pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error reading {pdf_path.name}: {e}\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_letter_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess letter text to extract main body content.\n",
    "    \n",
    "    Removes:\n",
    "    - EPA letterhead and headers\n",
    "    - Addresses and date lines\n",
    "    - Salutations and closings\n",
    "    - Rating statements\n",
    "    - Page numbers and footers\n",
    "    - Excessive whitespace\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Split into lines for processing\n",
    "    lines = text.split('\\n')\n",
    "    processed_lines = []\n",
    "    \n",
    "    # Patterns to remove\n",
    "    header_patterns = [\n",
    "        r'^\\s*UNITED STATES ENVIRONMENTAL PROTECTION AGENCY',\n",
    "        r'^\\s*U\\.?S\\.?\\s*EPA',\n",
    "        r'^\\s*Environmental Protection Agency',\n",
    "        r'^\\s*Region\\s+\\d+',\n",
    "        r'^\\s*\\d+\\s+[A-Z][a-z]+\\s+Street',  # Address lines\n",
    "        r'^\\s*[A-Z][a-z]+,\\s*[A-Z]{2}\\s+\\d{5}',  # City, State ZIP\n",
    "        r'^\\s*\\d{1,2}/\\d{1,2}/\\d{2,4}',  # Date formats\n",
    "        r'^\\s*[A-Z][a-z]+\\s+\\d{1,2},\\s*\\d{4}',  # Month Day, Year\n",
    "        r'^\\s*Page\\s+\\d+',\n",
    "        r'^\\s*-\\s*\\d+\\s*-',\n",
    "    ]\n",
    "    \n",
    "    salutation_patterns = [\n",
    "        r'^\\s*Dear\\s+',\n",
    "        r'^\\s*To\\s+Whom\\s+It\\s+May\\s+Concern',\n",
    "        r'^\\s*RE:\\s*',\n",
    "        r'^\\s*Re:\\s*',\n",
    "        r'^\\s*Subject:\\s*',\n",
    "    ]\n",
    "    \n",
    "    closing_patterns = [\n",
    "        r'^\\s*Sincerely',\n",
    "        r'^\\s*Respectfully',\n",
    "        r'^\\s*Best\\s+regards',\n",
    "        r'^\\s*Thank\\s+you',\n",
    "        r'^\\s*cc:\\s*',\n",
    "        r'^\\s*Enclosure',\n",
    "        r'^\\s*Attachment',\n",
    "    ]\n",
    "    \n",
    "    rating_patterns = [\n",
    "        r'\\b(LO|EC|EO|EU)\\s*[-–—]\\s*[123]\\b',\n",
    "        r'Rating\\s*:\\s*(LO|EC|EO|EU)',\n",
    "        r'EPA\\s+Rating',\n",
    "        r'Lack\\s+of\\s+Objections',\n",
    "        r'Environmental\\s+Concerns',\n",
    "        r'Environmental\\s+Objections',\n",
    "        r'Environmentally\\s+Unsatisfactory',\n",
    "    ]\n",
    "    \n",
    "    in_body = False\n",
    "    past_closing = False\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Skip empty lines at the start\n",
    "        if not stripped and not in_body:\n",
    "            continue\n",
    "        \n",
    "        # Skip headers\n",
    "        if any(re.match(p, stripped, re.IGNORECASE) for p in header_patterns):\n",
    "            continue\n",
    "        \n",
    "        # Detect salutations (marks start of body)\n",
    "        if any(re.match(p, stripped, re.IGNORECASE) for p in salutation_patterns):\n",
    "            in_body = True\n",
    "            continue\n",
    "        \n",
    "        # Detect closings (marks end of body)\n",
    "        if any(re.match(p, stripped, re.IGNORECASE) for p in closing_patterns):\n",
    "            past_closing = True\n",
    "            continue\n",
    "        \n",
    "        # Skip lines after closing\n",
    "        if past_closing:\n",
    "            continue\n",
    "        \n",
    "        # Remove rating statements from line\n",
    "        for pattern in rating_patterns:\n",
    "            stripped = re.sub(pattern, '', stripped, flags=re.IGNORECASE)\n",
    "        \n",
    "        # If we have substantive content, we're in the body\n",
    "        if len(stripped) > 20:\n",
    "            in_body = True\n",
    "        \n",
    "        if in_body and stripped:\n",
    "            processed_lines.append(stripped)\n",
    "    \n",
    "    # Join and clean up\n",
    "    result = ' '.join(processed_lines)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings_data():\n",
    "    \"\"\"\n",
    "    Load the extracted ratings for pre-October 2018 letters.\n",
    "    \"\"\"\n",
    "    if not RATINGS_FILE.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Ratings file not found: {RATINGS_FILE}\\n\"\n",
    "            \"Run extract_comment_letter_ratings.ipynb first.\"\n",
    "        )\n",
    "    \n",
    "    df = pd.read_csv(RATINGS_FILE)\n",
    "    \n",
    "    # Filter to records with valid ratings\n",
    "    df = df[df['combined_rating'].notna()].copy()\n",
    "    \n",
    "    # Standardize ratings\n",
    "    df['letter_rating'] = df['letter_rating'].str.upper()\n",
    "    df['number_rating'] = df['number_rating'].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_comment_letter_metadata():\n",
    "    \"\"\"\n",
    "    Load all comment letter metadata.\n",
    "    \"\"\"\n",
    "    if COMMENT_LETTER_PKL.exists():\n",
    "        return pd.read_pickle(COMMENT_LETTER_PKL)\n",
    "    elif DOC_RECORD_PKL.exists():\n",
    "        df = pd.read_pickle(DOC_RECORD_PKL)\n",
    "        return df[df['type'] == 'Comment_Letter'].copy()\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Comment letter metadata not found.\")\n",
    "\n",
    "\n",
    "def get_post_2018_letters(comment_df: pd.DataFrame, eis_df: pd.DataFrame = None):\n",
    "    \"\"\"\n",
    "    Get comment letters from October 2018 onwards.\n",
    "    \"\"\"\n",
    "    df = comment_df.copy()\n",
    "    \n",
    "    # Try to use commentLetterDate\n",
    "    if 'commentLetterDate' in df.columns:\n",
    "        df['_date'] = pd.to_datetime(df['commentLetterDate'], errors='coerce')\n",
    "    else:\n",
    "        df['_date'] = None\n",
    "    \n",
    "    # Merge dates from EIS records if needed\n",
    "    if eis_df is not None and 'commentLetterDate' in eis_df.columns:\n",
    "        eis_dates = eis_df[['eisId', 'commentLetterDate']].copy()\n",
    "        eis_dates['_eis_date'] = pd.to_datetime(eis_dates['commentLetterDate'], errors='coerce')\n",
    "        df = df.merge(eis_dates[['eisId', '_eis_date']], on='eisId', how='left')\n",
    "        df['_date'] = df['_date'].fillna(df['_eis_date'])\n",
    "    \n",
    "    # Use EIS ID year for records without date\n",
    "    df['_year'] = df['eisId'].astype(str).str[:4].astype(int)\n",
    "    \n",
    "    # Filter to post-2018 (October 2018 onwards)\n",
    "    has_date = df['_date'].notna()\n",
    "    post_cutoff_date = has_date & (df['_date'] >= RATING_CUTOFF_DATE)\n",
    "    post_cutoff_year = ~has_date & (df['_year'] >= 2019)  # Conservative: 2019 and later\n",
    "    maybe_post = ~has_date & (df['_year'] == 2018)  # 2018 might be before or after October\n",
    "    \n",
    "    # Include definite post-cutoff and 2018 unknowns\n",
    "    result = df[post_cutoff_date | post_cutoff_year | maybe_post].copy()\n",
    "    \n",
    "    # Clean up\n",
    "    cols_to_drop = [c for c in result.columns if c.startswith('_')]\n",
    "    result = result.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (pre-2018 with ratings)\n",
    "ratings_df = load_ratings_data()\n",
    "print(f\"Training data: {len(ratings_df)} letters with ratings\")\n",
    "\n",
    "print(f\"\\nRating distribution:\")\n",
    "print(ratings_df['combined_rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post-2018 letters for prediction\n",
    "try:\n",
    "    comment_df = load_comment_letter_metadata()\n",
    "    eis_df = pd.read_pickle(EIS_RECORD_PKL) if EIS_RECORD_PKL.exists() else None\n",
    "    post_2018_df = get_post_2018_letters(comment_df, eis_df)\n",
    "    print(f\"\\nPost-October 2018 letters to predict: {len(post_2018_df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load post-2018 letters: {e}\")\n",
    "    post_2018_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model_name: str):\n",
    "    \"\"\"\n",
    "    Load the embedding model.\n",
    "    \"\"\"\n",
    "    if model_name.startswith(\"sentence-transformers/\"):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer(model_name.replace(\"sentence-transformers/\", \"\"))\n",
    "        return model, \"sentence-transformers\"\n",
    "    \n",
    "    elif \"legal-bert\" in model_name.lower():\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        model.to(DEVICE)\n",
    "        return (tokenizer, model), \"transformers\"\n",
    "    \n",
    "    elif \"longformer\" in model_name.lower():\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        model.to(DEVICE)\n",
    "        return (tokenizer, model), \"transformers\"\n",
    "    \n",
    "    else:\n",
    "        # Default to sentence-transformers\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer(model_name)\n",
    "        return model, \"sentence-transformers\"\n",
    "\n",
    "\n",
    "def get_embeddings(texts: list, model, model_type: str, batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts.\n",
    "    \"\"\"\n",
    "    if model_type == \"sentence-transformers\":\n",
    "        embeddings = model.encode(\n",
    "            texts, \n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        return embeddings\n",
    "    \n",
    "    elif model_type == \"transformers\":\n",
    "        tokenizer, transformer = model\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = transformer(**inputs)\n",
    "                # Mean pooling over token embeddings\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                token_embeddings = outputs.last_hidden_state\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "                batch_embeddings = (sum_embeddings / sum_mask).cpu().numpy()\n",
    "            \n",
    "            embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(embeddings)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "print(f\"Loading embedding model: {EMBEDDING_MODEL}\")\n",
    "embedding_model, model_type = load_embedding_model(EMBEDDING_MODEL)\n",
    "print(f\"Model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Embed Training Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_file(filename: str, eis_id: str) -> Path:\n",
    "    \"\"\"\n",
    "    Find the PDF file in various possible locations.\n",
    "    \"\"\"\n",
    "    year = str(eis_id)[:4]\n",
    "    \n",
    "    # Possible locations\n",
    "    locations = [\n",
    "        COMMENT_LETTERS_DIR / filename,\n",
    "        DOCUMENTS_DIR / year / filename,\n",
    "        DOCUMENTS_DIR / \"comment_letters\" / filename,\n",
    "    ]\n",
    "    \n",
    "    for loc in locations:\n",
    "        if loc.exists():\n",
    "            return loc\n",
    "    \n",
    "    # Try fuzzy match\n",
    "    for dir_path in [COMMENT_LETTERS_DIR, DOCUMENTS_DIR / year]:\n",
    "        if dir_path.exists():\n",
    "            for f in dir_path.iterdir():\n",
    "                if f.name.startswith(f\"{eis_id}_\") and 'comment' in f.name.lower():\n",
    "                    return f\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_texts_for_dataframe(df: pd.DataFrame, desc: str = \"Extracting\") -> tuple:\n",
    "    \"\"\"\n",
    "    Extract and preprocess texts for all records in dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (texts, valid_indices)\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
    "        filename = row.get('filename')\n",
    "        eis_id = str(row['eisId'])\n",
    "        \n",
    "        # Find PDF file\n",
    "        if filename:\n",
    "            pdf_path = find_pdf_file(filename, eis_id)\n",
    "        else:\n",
    "            pdf_path = None\n",
    "        \n",
    "        if pdf_path is None:\n",
    "            continue\n",
    "        \n",
    "        # Extract and preprocess text\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "        processed_text = preprocess_letter_text(raw_text)\n",
    "        \n",
    "        # Skip if text is too short\n",
    "        if len(processed_text) < 100:\n",
    "            continue\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(processed_text) > MAX_TEXT_LENGTH:\n",
    "            processed_text = processed_text[:MAX_TEXT_LENGTH]\n",
    "        \n",
    "        texts.append(processed_text)\n",
    "        valid_indices.append(idx)\n",
    "    \n",
    "    return texts, valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache file for embeddings\n",
    "model_name_safe = EMBEDDING_MODEL.replace(\"/\", \"_\")\n",
    "train_embeddings_file = EMBEDDINGS_DIR / f\"train_embeddings_{model_name_safe}.pkl\"\n",
    "\n",
    "if USE_CACHED_EMBEDDINGS and train_embeddings_file.exists():\n",
    "    print(f\"Loading cached training embeddings from {train_embeddings_file}\")\n",
    "    with open(train_embeddings_file, 'rb') as f:\n",
    "        cache = pickle.load(f)\n",
    "    train_embeddings = cache['embeddings']\n",
    "    train_df = cache['dataframe']\n",
    "    print(f\"Loaded {len(train_embeddings)} embeddings\")\n",
    "else:\n",
    "    print(\"Extracting texts from training PDFs...\")\n",
    "    train_texts, train_indices = extract_texts_for_dataframe(ratings_df, desc=\"Extracting training texts\")\n",
    "    \n",
    "    # Create filtered dataframe\n",
    "    train_df = ratings_df.loc[train_indices].copy()\n",
    "    print(f\"Successfully extracted {len(train_texts)} texts\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"\\nGenerating embeddings...\")\n",
    "    train_embeddings = get_embeddings(train_texts, embedding_model, model_type)\n",
    "    print(f\"Embedding shape: {train_embeddings.shape}\")\n",
    "    \n",
    "    # Cache embeddings\n",
    "    with open(train_embeddings_file, 'wb') as f:\n",
    "        pickle.dump({'embeddings': train_embeddings, 'dataframe': train_df}, f)\n",
    "    print(f\"Cached embeddings to {train_embeddings_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(classifier_type: str):\n",
    "    \"\"\"\n",
    "    Get classifier based on type.\n",
    "    \"\"\"\n",
    "    if classifier_type == \"logistic\":\n",
    "        return LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "    elif classifier_type == \"svm\":\n",
    "        return SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced')\n",
    "    elif classifier_type == \"rf\":\n",
    "        return RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    elif classifier_type == \"mlp\":\n",
    "        return MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=500, random_state=42)\n",
    "    elif classifier_type == \"ensemble\":\n",
    "        # Use voting ensemble of multiple classifiers\n",
    "        from sklearn.ensemble import VotingClassifier\n",
    "        estimators = [\n",
    "            ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')),\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')),\n",
    "            ('svm', SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced')),\n",
    "        ]\n",
    "        return VotingClassifier(estimators=estimators, voting='soft')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown classifier type: {classifier_type}\")\n",
    "\n",
    "\n",
    "def train_and_evaluate(X: np.ndarray, y: np.ndarray, label_name: str, classifier_type: str):\n",
    "    \"\"\"\n",
    "    Train classifier and evaluate using cross-validation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training classifier for: {label_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    \n",
    "    print(f\"Classes: {le.classes_}\")\n",
    "    print(f\"Class distribution: {np.bincount(y_encoded)}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=TEST_SIZE, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Train classifier\n",
    "    clf = get_classifier(classifier_type)\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    cv_scores = cross_val_score(clf, X_train, y_train, cv=CV_FOLDS, scoring='accuracy')\n",
    "    print(f\"\\nCross-validation accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
    "    \n",
    "    # Train on full training set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nTest set accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Retrain on all data for final model\n",
    "    final_clf = get_classifier(classifier_type)\n",
    "    final_clf.fit(X, y_encoded)\n",
    "    \n",
    "    return final_clf, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels\n",
    "y_letter = train_df['letter_rating'].values\n",
    "y_number = train_df['number_rating'].astype(str).values\n",
    "\n",
    "print(f\"Training samples: {len(train_embeddings)}\")\n",
    "print(f\"\\nLetter rating distribution:\")\n",
    "print(pd.Series(y_letter).value_counts())\n",
    "print(f\"\\nNumber rating distribution:\")\n",
    "print(pd.Series(y_number).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train letter rating classifier\n",
    "letter_clf, letter_encoder = train_and_evaluate(\n",
    "    train_embeddings, y_letter, \"Letter Rating (LO/EC/EO/EU)\", CLASSIFIER_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train number rating classifier\n",
    "number_clf, number_encoder = train_and_evaluate(\n",
    "    train_embeddings, y_number, \"Number Rating (1/2/3)\", CLASSIFIER_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "model_file = MODEL_DIR / f\"rating_classifiers_{model_name_safe}.pkl\"\n",
    "with open(model_file, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'letter_clf': letter_clf,\n",
    "        'letter_encoder': letter_encoder,\n",
    "        'number_clf': number_clf,\n",
    "        'number_encoder': number_encoder,\n",
    "        'embedding_model': EMBEDDING_MODEL,\n",
    "        'classifier_type': CLASSIFIER_TYPE,\n",
    "    }, f)\n",
    "print(f\"Saved models to {model_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Ratings for Post-2018 Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"Sanitize filename.\"\"\"\n",
    "    clean = re.sub(r'[()&,~\\/]', '', filename)\n",
    "    clean = re.sub(r'[\\s_]+', '_', clean)\n",
    "    clean = re.sub(r'\\.PDF$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = clean.strip('_')\n",
    "    return clean\n",
    "\n",
    "\n",
    "def build_filename_for_row(row):\n",
    "    \"\"\"Build expected filename for a record.\"\"\"\n",
    "    name = row.get('name') or row.get('fileNameForDownload') or f\"{row.get('attachmentId', 'unknown')}.pdf\"\n",
    "    return f\"{row['eisId']}_{sanitize_filename(name)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare post-2018 data for prediction\n",
    "if len(post_2018_df) > 0:\n",
    "    # Add filename column if not present\n",
    "    if 'filename' not in post_2018_df.columns:\n",
    "        post_2018_df['filename'] = post_2018_df.apply(build_filename_for_row, axis=1)\n",
    "    \n",
    "    # Check for cached embeddings\n",
    "    pred_embeddings_file = EMBEDDINGS_DIR / f\"pred_embeddings_{model_name_safe}.pkl\"\n",
    "    \n",
    "    if USE_CACHED_EMBEDDINGS and pred_embeddings_file.exists():\n",
    "        print(f\"Loading cached prediction embeddings from {pred_embeddings_file}\")\n",
    "        with open(pred_embeddings_file, 'rb') as f:\n",
    "            cache = pickle.load(f)\n",
    "        pred_embeddings = cache['embeddings']\n",
    "        pred_df = cache['dataframe']\n",
    "        print(f\"Loaded {len(pred_embeddings)} embeddings\")\n",
    "    else:\n",
    "        print(\"Extracting texts from post-2018 PDFs...\")\n",
    "        pred_texts, pred_indices = extract_texts_for_dataframe(post_2018_df, desc=\"Extracting prediction texts\")\n",
    "        \n",
    "        pred_df = post_2018_df.loc[pred_indices].copy()\n",
    "        print(f\"Successfully extracted {len(pred_texts)} texts\")\n",
    "        \n",
    "        if len(pred_texts) > 0:\n",
    "            print(\"\\nGenerating embeddings...\")\n",
    "            pred_embeddings = get_embeddings(pred_texts, embedding_model, model_type)\n",
    "            print(f\"Embedding shape: {pred_embeddings.shape}\")\n",
    "            \n",
    "            # Cache\n",
    "            with open(pred_embeddings_file, 'wb') as f:\n",
    "                pickle.dump({'embeddings': pred_embeddings, 'dataframe': pred_df}, f)\n",
    "        else:\n",
    "            pred_embeddings = np.array([])\n",
    "            print(\"No texts extracted for prediction.\")\n",
    "else:\n",
    "    print(\"No post-2018 letters to predict.\")\n",
    "    pred_embeddings = np.array([])\n",
    "    pred_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "if len(pred_embeddings) > 0:\n",
    "    print(f\"Making predictions for {len(pred_embeddings)} letters...\")\n",
    "    \n",
    "    # Predict letter ratings\n",
    "    letter_pred_encoded = letter_clf.predict(pred_embeddings)\n",
    "    letter_pred = letter_encoder.inverse_transform(letter_pred_encoded)\n",
    "    \n",
    "    # Predict number ratings\n",
    "    number_pred_encoded = number_clf.predict(pred_embeddings)\n",
    "    number_pred = number_encoder.inverse_transform(number_pred_encoded)\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    if hasattr(letter_clf, 'predict_proba'):\n",
    "        letter_proba = letter_clf.predict_proba(pred_embeddings)\n",
    "        letter_confidence = letter_proba.max(axis=1)\n",
    "    else:\n",
    "        letter_confidence = np.ones(len(letter_pred))\n",
    "    \n",
    "    if hasattr(number_clf, 'predict_proba'):\n",
    "        number_proba = number_clf.predict_proba(pred_embeddings)\n",
    "        number_confidence = number_proba.max(axis=1)\n",
    "    else:\n",
    "        number_confidence = np.ones(len(number_pred))\n",
    "    \n",
    "    # Build combined rating\n",
    "    combined_pred = [f\"{l}-{n}\" for l, n in zip(letter_pred, number_pred)]\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pred_df[['eisId', 'filename']].copy()\n",
    "    results_df['predicted_combined_rating'] = combined_pred\n",
    "    results_df['predicted_letter_rating'] = letter_pred\n",
    "    results_df['predicted_number_rating'] = number_pred\n",
    "    results_df['letter_confidence'] = letter_confidence\n",
    "    results_df['number_confidence'] = number_confidence\n",
    "    results_df['avg_confidence'] = (letter_confidence + number_confidence) / 2\n",
    "    \n",
    "    print(f\"\\n=== Prediction Summary ===\")\n",
    "    print(f\"Total predictions: {len(results_df)}\")\n",
    "    print(f\"\\nPredicted letter rating distribution:\")\n",
    "    print(results_df['predicted_letter_rating'].value_counts())\n",
    "    print(f\"\\nPredicted number rating distribution:\")\n",
    "    print(results_df['predicted_number_rating'].value_counts())\n",
    "    print(f\"\\nPredicted combined rating distribution:\")\n",
    "    print(results_df['predicted_combined_rating'].value_counts())\n",
    "else:\n",
    "    results_df = pd.DataFrame()\n",
    "    print(\"No predictions to make.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview predictions\n",
    "if len(results_df) > 0:\n",
    "    print(\"\\n=== Sample Predictions ===\")\n",
    "    display(results_df[[\n",
    "        'eisId', 'filename', 'predicted_combined_rating', \n",
    "        'letter_confidence', 'number_confidence'\n",
    "    ]].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "if len(results_df) > 0:\n",
    "    # Select columns for output\n",
    "    output_df = results_df[[\n",
    "        'filename', 'eisId', \n",
    "        'predicted_combined_rating', 'predicted_letter_rating', 'predicted_number_rating',\n",
    "        'letter_confidence', 'number_confidence', 'avg_confidence'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Sort by EIS ID\n",
    "    output_df = output_df.sort_values('eisId')\n",
    "    \n",
    "    # Save\n",
    "    output_df.to_csv(PREDICTED_RATINGS_FILE, index=False)\n",
    "    print(f\"Saved {len(output_df)} predictions to {PREDICTED_RATINGS_FILE}\")\n",
    "    \n",
    "    # Also save with full metadata\n",
    "    full_output = METADATA_DIR / \"comment_letter_ratings_predicted_full.csv\"\n",
    "    results_df.to_csv(full_output, index=False)\n",
    "    print(f\"Saved full results to {full_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Compare Training vs Predicted Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Letter rating comparison\n",
    "    ax1 = axes[0]\n",
    "    train_letter_counts = pd.Series(y_letter).value_counts(normalize=True).sort_index()\n",
    "    pred_letter_counts = results_df['predicted_letter_rating'].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    x = np.arange(len(train_letter_counts))\n",
    "    width = 0.35\n",
    "    ax1.bar(x - width/2, train_letter_counts.values, width, label='Training (pre-2018)', alpha=0.7)\n",
    "    ax1.bar(x + width/2, [pred_letter_counts.get(k, 0) for k in train_letter_counts.index], width, label='Predicted (post-2018)', alpha=0.7)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(train_letter_counts.index)\n",
    "    ax1.set_title('Letter Rating Distribution')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylabel('Proportion')\n",
    "    \n",
    "    # Number rating comparison\n",
    "    ax2 = axes[1]\n",
    "    train_number_counts = pd.Series(y_number).value_counts(normalize=True).sort_index()\n",
    "    pred_number_counts = results_df['predicted_number_rating'].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    x = np.arange(len(train_number_counts))\n",
    "    ax2.bar(x - width/2, train_number_counts.values, width, label='Training (pre-2018)', alpha=0.7)\n",
    "    ax2.bar(x + width/2, [pred_number_counts.get(k, 0) for k in train_number_counts.index], width, label='Predicted (post-2018)', alpha=0.7)\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(train_number_counts.index)\n",
    "    ax2.set_title('Number Rating Distribution')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylabel('Proportion')\n",
    "    \n",
    "    # Confidence distribution\n",
    "    ax3 = axes[2]\n",
    "    ax3.hist(results_df['avg_confidence'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax3.set_title('Prediction Confidence Distribution')\n",
    "    ax3.set_xlabel('Average Confidence')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.axvline(results_df['avg_confidence'].median(), color='red', linestyle='--', label=f'Median: {results_df[\"avg_confidence\"].median():.2f}')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(METADATA_DIR / \"rating_prediction_analysis.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Saved analysis plot to {METADATA_DIR / 'rating_prediction_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Confidence Predictions\n",
    "\n",
    "These predictions may need manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    # Identify low confidence predictions\n",
    "    low_confidence = results_df[results_df['avg_confidence'] < 0.5].copy()\n",
    "    \n",
    "    print(f\"Low confidence predictions (< 50%): {len(low_confidence)} ({100*len(low_confidence)/len(results_df):.1f}%)\")\n",
    "    \n",
    "    if len(low_confidence) > 0:\n",
    "        display(low_confidence[[\n",
    "            'eisId', 'filename', 'predicted_combined_rating', 'avg_confidence'\n",
    "        ]].head(20))\n",
    "        \n",
    "        # Save for review\n",
    "        review_file = METADATA_DIR / \"predictions_need_review.csv\"\n",
    "        low_confidence.to_csv(review_file, index=False)\n",
    "        print(f\"\\nSaved low confidence predictions to {review_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
