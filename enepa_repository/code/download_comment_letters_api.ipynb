{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download EPA Comment Letters via API\n",
    "\n",
    "This notebook downloads EPA comment letters using the E-NEPA API.\n",
    "\n",
    "**Key Features:**\n",
    "- Downloads individual comment letter files using `attachmentId`\n",
    "- `OVERWRITE` toggle to skip existing files or re-download\n",
    "- Stores all files in flat directory: `documents/comment_letters/`\n",
    "- Maintains naming convention: `{CEQ_NUMBER}_{filename}`\n",
    "\n",
    "**API Documentation:** https://cdxapps.epa.gov/cdx-enepa-II/apidocs/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install requests pandas tqdm pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository\n",
      "Comment letters directory: /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/documents/comment_letters\n",
      "Documents directory is symlink: True\n",
      "Symlink target: /Users/admin-tascott/Library/CloudStorage/Box-Box/eis_documents/enepa_repository/documents\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_URL = \"https://cdxapps.epa.gov/cdx-enepa-II/rest\"\n",
    "DOWNLOAD_ENDPOINT = f\"{BASE_URL}/public/v1/eis/document/download\"\n",
    "\n",
    "# Paths - relative to repository root\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "METADATA_DIR = REPO_ROOT / \"metadata\"\n",
    "DOCUMENTS_DIR = REPO_ROOT / \"documents\"\n",
    "COMMENT_LETTERS_DIR = DOCUMENTS_DIR / \"comment_letters\"  # Flat directory for all comment letters\n",
    "\n",
    "# Input file (from fetch_eis_records_api.ipynb)\n",
    "COMMENT_LETTER_PKL = METADATA_DIR / \"comment_letter_record_api.pkl\"\n",
    "COMMENT_LETTER_PARQUET = METADATA_DIR / \"comment_letter_record_api.parquet\"\n",
    "\n",
    "# Alternative: use main document records\n",
    "DOC_RECORD_PKL = METADATA_DIR / \"eis_document_record_api.pkl\"\n",
    "\n",
    "# Download tracking file\n",
    "DOWNLOAD_STATUS_FILE = METADATA_DIR / \"comment_letter_download_status.pkl\"\n",
    "\n",
    "# Rate limiting\n",
    "REQUEST_DELAY = 0.25\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Comment letters directory: {COMMENT_LETTERS_DIR}\")\n",
    "print(f\"Documents directory is symlink: {DOCUMENTS_DIR.is_symlink()}\")\n",
    "if DOCUMENTS_DIR.is_symlink():\n",
    "    print(f\"Symlink target: {DOCUMENTS_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Download Configuration ===\n",
      "  OVERWRITE: False\n",
      "  USE_COPY_OPTIMIZATION: False\n",
      "  YEAR_FILTER: None\n",
      "  MAX_DOWNLOADS: None\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DOWNLOAD SETTINGS - MODIFY AS NEEDED\n",
    "# ============================================\n",
    "\n",
    "# Set to True to re-download all files, False to skip existing files\n",
    "OVERWRITE = False\n",
    "\n",
    "# Copy optimization: If True, check documents/{YEAR}/ for existing files and copy\n",
    "# instead of re-downloading. Set to False if files are on cloud storage (Box) \n",
    "# and copying is slow due to on-demand download.\n",
    "USE_COPY_OPTIMIZATION = False\n",
    "\n",
    "# Filter by year (set to None to download all years)\n",
    "# Example: YEAR_FILTER = [2023, 2024]\n",
    "YEAR_FILTER = None\n",
    "\n",
    "# Maximum number of files to download (set to None for all)\n",
    "# Useful for testing\n",
    "MAX_DOWNLOADS = None  # e.g., 100\n",
    "\n",
    "print(f\"=== Download Configuration ===\")\n",
    "print(f\"  OVERWRITE: {OVERWRITE}\")\n",
    "print(f\"  USE_COPY_OPTIMIZATION: {USE_COPY_OPTIMIZATION}\")\n",
    "print(f\"  YEAR_FILTER: {YEAR_FILTER}\")\n",
    "print(f\"  MAX_DOWNLOADS: {MAX_DOWNLOADS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize filename to match existing convention.\n",
    "    \"\"\"\n",
    "    # Remove problematic characters\n",
    "    clean = re.sub(r'[()&,~\\/]', '', filename)\n",
    "    # Replace multiple spaces/underscores with single underscore\n",
    "    clean = re.sub(r'[\\s_]+', '_', clean)\n",
    "    # Normalize PDF extension\n",
    "    clean = re.sub(r'\\.PDF$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = re.sub(r'\\.pdf\\.pdf$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    # Remove leading/trailing underscores\n",
    "    clean = clean.strip('_')\n",
    "    return clean\n",
    "\n",
    "\n",
    "def build_local_filename(ceq_number, original_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the local filename: {CEQ_NUMBER}_{sanitized_filename}\n",
    "    \"\"\"\n",
    "    sanitized = sanitize_filename(original_filename)\n",
    "    return f\"{ceq_number}_{sanitized}\"\n",
    "\n",
    "\n",
    "def get_existing_files() -> set:\n",
    "    \"\"\"\n",
    "    Get set of existing files in comment_letters directory.\n",
    "    \"\"\"\n",
    "    existing = set()\n",
    "    if COMMENT_LETTERS_DIR.exists():\n",
    "        for file in COMMENT_LETTERS_DIR.iterdir():\n",
    "            if file.is_file():\n",
    "                existing.add(file.name)\n",
    "    return existing\n",
    "\n",
    "\n",
    "def find_in_documents_dir(ceq_number, filename: str) -> Path:\n",
    "    \"\"\"\n",
    "    Check if a file exists in the main documents directory (documents/{YEAR}/).\n",
    "    \n",
    "    Args:\n",
    "        ceq_number: CEQ number (used to determine year and filename)\n",
    "        filename: The expected filename\n",
    "    \n",
    "    Returns:\n",
    "        Path to existing file if found, None otherwise\n",
    "    \"\"\"\n",
    "    year = str(ceq_number)[:4]\n",
    "    expected_path = DOCUMENTS_DIR / year / filename\n",
    "    \n",
    "    if expected_path.exists():\n",
    "        return expected_path\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_attachment(attachment_id: int, dest_path: Path, max_retries: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Download a single attachment by ID.\n",
    "    \"\"\"\n",
    "    params = {\"attachmentId\": attachment_id}\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                DOWNLOAD_ENDPOINT, \n",
    "                params=params, \n",
    "                timeout=120,\n",
    "                stream=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Ensure parent directory exists\n",
    "            dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Write file\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            file_size = dest_path.stat().st_size\n",
    "            \n",
    "            if file_size == 0:\n",
    "                dest_path.unlink()\n",
    "                return {\"success\": False, \"size\": 0, \"error\": \"Empty file received\"}\n",
    "            \n",
    "            return {\"success\": True, \"size\": file_size, \"error\": None}\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1}/{max_retries} failed for attachment {attachment_id}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return {\"success\": False, \"size\": 0, \"error\": str(e)}\n",
    "    \n",
    "    return {\"success\": False, \"size\": 0, \"error\": \"Max retries exceeded\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Comment Letter Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comment_letter_records():\n",
    "    \"\"\"\n",
    "    Load comment letter records.\n",
    "    First tries dedicated comment letter file, then falls back to main doc records.\n",
    "    \"\"\"\n",
    "    # Try dedicated comment letter file first\n",
    "    if COMMENT_LETTER_PKL.exists():\n",
    "        return pd.read_pickle(COMMENT_LETTER_PKL)\n",
    "    elif COMMENT_LETTER_PARQUET.exists():\n",
    "        return pd.read_parquet(COMMENT_LETTER_PARQUET)\n",
    "    \n",
    "    # Fall back to main document records\n",
    "    if DOC_RECORD_PKL.exists():\n",
    "        doc_df = pd.read_pickle(DOC_RECORD_PKL)\n",
    "        # Filter to comment letters\n",
    "        return doc_df[doc_df['type'] == 'Comment_Letter'].copy()\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"Comment letter records not found.\\n\"\n",
    "        f\"Run fetch_comment_letters_api.ipynb or fetch_eis_records_api.ipynb first.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_download_status():\n",
    "    \"\"\"\n",
    "    Load existing download status.\n",
    "    \"\"\"\n",
    "    if DOWNLOAD_STATUS_FILE.exists():\n",
    "        return pd.read_pickle(DOWNLOAD_STATUS_FILE)\n",
    "    return pd.DataFrame(columns=['attachmentId', 'eisId', 'filename', 'downloaded', 'size', 'error', 'timestamp'])\n",
    "\n",
    "\n",
    "def save_download_status(status_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Save download status.\n",
    "    \"\"\"\n",
    "    status_df.to_pickle(DOWNLOAD_STATUS_FILE)\n",
    "    status_df.to_csv(METADATA_DIR / \"comment_letter_download_status.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11700 comment letter records\n",
      "\n",
      "Comment letters by year:\n",
      "year\n",
      "1987      2\n",
      "1988      1\n",
      "1990      2\n",
      "1991      4\n",
      "1992      3\n",
      "1993      3\n",
      "1994     10\n",
      "1995     20\n",
      "1996     59\n",
      "1997     82\n",
      "1998     58\n",
      "1999    202\n",
      "2000    369\n",
      "2001    478\n",
      "2002    596\n",
      "2003    553\n",
      "2004    631\n",
      "2005    751\n",
      "2006    712\n",
      "2007    912\n",
      "2008    911\n",
      "2009    894\n",
      "2010    661\n",
      "2011    406\n",
      "2012    352\n",
      "2013    332\n",
      "2014    331\n",
      "2015    318\n",
      "2016    293\n",
      "2017    197\n",
      "2018    284\n",
      "2019    252\n",
      "2020    221\n",
      "2021    165\n",
      "2022    163\n",
      "2023    139\n",
      "2024    198\n",
      "2025    135\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load comment letter records\n",
    "comment_df = load_comment_letter_records()\n",
    "print(f\"Loaded {len(comment_df)} comment letter records\")\n",
    "\n",
    "# Add year column - use ceqNumber (format: YYYYNNNN) for year extraction\n",
    "comment_df['year'] = comment_df['ceqNumber'].astype(str).str[:4]\n",
    "\n",
    "print(f\"\\nComment letters by year:\")\n",
    "print(comment_df['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Download Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_download_queue(df: pd.DataFrame, overwrite: bool = False, \n",
    "                           year_filter: list = None,\n",
    "                           use_copy_optimization: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the download queue.\n",
    "    \n",
    "    Checks for files in three places:\n",
    "    1. Already in comment_letters/ directory\n",
    "    2. Available in documents/{YEAR}/ directory (can be copied) - if use_copy_optimization=True\n",
    "    3. Need to download from API\n",
    "    \"\"\"\n",
    "    queue = df.copy()\n",
    "    \n",
    "    # Ensure year column exists - use ceqNumber for year extraction\n",
    "    if 'year' not in queue.columns:\n",
    "        queue['year'] = queue['ceqNumber'].astype(str).str[:4]\n",
    "    \n",
    "    # Apply year filter\n",
    "    if year_filter:\n",
    "        year_filter_str = [str(y) for y in year_filter]\n",
    "        queue = queue[queue['year'].isin(year_filter_str)]\n",
    "        logger.info(f\"Filtered to years {year_filter}: {len(queue)} letters\")\n",
    "    \n",
    "    # Build local filenames using ceqNumber as prefix (matches existing convention)\n",
    "    queue['localFilename'] = queue.apply(\n",
    "        lambda row: build_local_filename(\n",
    "            row['ceqNumber'], \n",
    "            row.get('name') or row.get('fileNameForDownload') or f\"{row['attachmentId']}.pdf\"\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Build full paths (flat directory for comment letters)\n",
    "    queue['localPath'] = queue['localFilename'].apply(lambda f: COMMENT_LETTERS_DIR / f)\n",
    "    \n",
    "    # Check for source in main documents directory (can be copied instead of downloaded)\n",
    "    if use_copy_optimization:\n",
    "        logger.info(\"Checking documents/ directory for existing files (this may take a moment)...\")\n",
    "        queue['sourcePath'] = queue.apply(\n",
    "            lambda row: find_in_documents_dir(row['ceqNumber'], row['localFilename']),\n",
    "            axis=1\n",
    "        )\n",
    "        queue['canCopy'] = queue['sourcePath'].notna()\n",
    "    else:\n",
    "        queue['sourcePath'] = None\n",
    "        queue['canCopy'] = False\n",
    "    \n",
    "    # Check for existing files in comment_letters/\n",
    "    if not overwrite:\n",
    "        queue['exists'] = queue['localPath'].apply(lambda p: p.exists())\n",
    "        existing_count = queue['exists'].sum()\n",
    "        logger.info(f\"Found {existing_count} existing files in comment_letters/\")\n",
    "        queue = queue[~queue['exists']]\n",
    "        logger.info(f\"Queue after removing existing: {len(queue)} letters\")\n",
    "    \n",
    "    # Remove rows with missing attachment IDs\n",
    "    queue = queue[queue['attachmentId'].notna()]\n",
    "    \n",
    "    # Log copy vs download stats\n",
    "    if use_copy_optimization:\n",
    "        copy_count = queue['canCopy'].sum()\n",
    "        download_count = len(queue) - copy_count\n",
    "        logger.info(f\"Can copy from documents/: {copy_count}\")\n",
    "        logger.info(f\"Need to download: {download_count}\")\n",
    "    \n",
    "    return queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 14:26:19,303 - INFO - Found 11626 existing files in comment_letters/\n",
      "2026-01-30 14:26:19,305 - INFO - Queue after removing existing: 74 letters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Download Queue ===\n",
      "Total files to process: 74\n",
      "\n",
      "By year:\n",
      "year\n",
      "2010     4\n",
      "2011    66\n",
      "2015     4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare download queue\n",
    "download_queue = prepare_download_queue(\n",
    "    comment_df,\n",
    "    overwrite=OVERWRITE,\n",
    "    year_filter=YEAR_FILTER,\n",
    "    use_copy_optimization=USE_COPY_OPTIMIZATION\n",
    ")\n",
    "\n",
    "# Apply max downloads limit\n",
    "if MAX_DOWNLOADS and len(download_queue) > MAX_DOWNLOADS:\n",
    "    download_queue = download_queue.head(MAX_DOWNLOADS)\n",
    "    logger.info(f\"Limited to {MAX_DOWNLOADS} downloads\")\n",
    "\n",
    "print(f\"\\n=== Download Queue ===\")\n",
    "print(f\"Total files to process: {len(download_queue)}\")\n",
    "\n",
    "if len(download_queue) > 0:\n",
    "    print(f\"\\nBy year:\")\n",
    "    print(download_queue['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file(source_path: Path, dest_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Copy a file from source to destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(source_path, dest_path)\n",
    "        file_size = dest_path.stat().st_size\n",
    "        return {\"success\": True, \"size\": file_size, \"error\": None, \"method\": \"copy\"}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"size\": 0, \"error\": str(e), \"method\": \"copy\"}\n",
    "\n",
    "\n",
    "def download_files(queue: pd.DataFrame, use_copy: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download comment letter files.\n",
    "    \n",
    "    Args:\n",
    "        queue: DataFrame with files to process\n",
    "        use_copy: If True and files exist in documents/{YEAR}/, copy instead of download\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    COMMENT_LETTERS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if use_copy and 'canCopy' in queue.columns:\n",
    "        copy_queue = queue[queue['canCopy'] == True].copy()\n",
    "        download_queue = queue[queue['canCopy'] == False].copy()\n",
    "    else:\n",
    "        copy_queue = pd.DataFrame()\n",
    "        download_queue = queue\n",
    "    \n",
    "    # Copy files from documents directory\n",
    "    if len(copy_queue) > 0:\n",
    "        logger.info(f\"Copying {len(copy_queue)} files from documents/...\")\n",
    "        failed_copies = []\n",
    "        \n",
    "        for idx, row in tqdm(copy_queue.iterrows(), total=len(copy_queue), desc=\"Copying\"):\n",
    "            result = copy_file(row['sourcePath'], row['localPath'])\n",
    "            \n",
    "            if result['success']:\n",
    "                results.append({\n",
    "                    'attachmentId': row['attachmentId'],\n",
    "                    'ceqNumber': row['ceqNumber'],\n",
    "                    'filename': row['localFilename'],\n",
    "                    'downloaded': True,\n",
    "                    'size': result['size'],\n",
    "                    'error': None,\n",
    "                    'method': 'copy',\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "            else:\n",
    "                failed_copies.append(row)\n",
    "        \n",
    "        # Add failed copies to download queue\n",
    "        if failed_copies:\n",
    "            logger.info(f\"{len(failed_copies)} copies failed, will download instead\")\n",
    "            failed_df = pd.DataFrame(failed_copies)\n",
    "            download_queue = pd.concat([download_queue, failed_df], ignore_index=True)\n",
    "    \n",
    "    # Download files from API\n",
    "    if len(download_queue) > 0:\n",
    "        logger.info(f\"Downloading {len(download_queue)} files from API...\")\n",
    "        for idx, row in tqdm(download_queue.iterrows(), total=len(download_queue), desc=\"Downloading\"):\n",
    "            result = download_attachment(\n",
    "                int(row['attachmentId']),\n",
    "                row['localPath']\n",
    "            )\n",
    "            results.append({\n",
    "                'attachmentId': row['attachmentId'],\n",
    "                'ceqNumber': row['ceqNumber'],\n",
    "                'filename': row['localFilename'],\n",
    "                'downloaded': result['success'],\n",
    "                'size': result['size'],\n",
    "                'error': result['error'],\n",
    "                'method': 'download',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 14:26:19,330 - INFO - Downloading 74 files from API...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 74 comment letters...\n",
      "Destination: /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/documents/comment_letters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834d9b20dc1147038769b8c0dbcff275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "Successful: 71\n",
      "Failed: 3\n",
      "Total size: 62.76 MB\n",
      "\n",
      "Failed downloads:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ceqNumber</th>\n",
       "      <th>filename</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20150083</td>\n",
       "      <td>20150083_20150083.pdf</td>\n",
       "      <td>Empty file received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150058</td>\n",
       "      <td>20150058_20150058.pdf</td>\n",
       "      <td>Empty file received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150060</td>\n",
       "      <td>20150060_20150060.pdf</td>\n",
       "      <td>Empty file received</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ceqNumber               filename                error\n",
       "0  20150083  20150083_20150083.pdf  Empty file received\n",
       "1  20150058  20150058_20150058.pdf  Empty file received\n",
       "3  20150060  20150060_20150060.pdf  Empty file received"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run downloads\n",
    "if len(download_queue) > 0:\n",
    "    print(f\"Processing {len(download_queue)} comment letters...\")\n",
    "    print(f\"Destination: {COMMENT_LETTERS_DIR}\")\n",
    "    \n",
    "    if USE_COPY_OPTIMIZATION:\n",
    "        copy_count = download_queue['canCopy'].sum()\n",
    "        print(f\"  - Can copy from documents/: {copy_count}\")\n",
    "        print(f\"  - Need to download: {len(download_queue) - copy_count}\")\n",
    "    \n",
    "    download_results = download_files(download_queue, use_copy=USE_COPY_OPTIMIZATION)\n",
    "    \n",
    "    # Merge with existing status\n",
    "    existing_status = load_download_status()\n",
    "    combined_status = pd.concat([existing_status, download_results], ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates (keep latest)\n",
    "    if 'filename' in combined_status.columns:\n",
    "        combined_status = combined_status.drop_duplicates(subset=['filename'], keep='last')\n",
    "    \n",
    "    # Save status\n",
    "    save_download_status(combined_status)\n",
    "    \n",
    "    # Summary\n",
    "    success_count = download_results['downloaded'].sum()\n",
    "    fail_count = len(download_results) - success_count\n",
    "    total_size = download_results['size'].sum()\n",
    "    \n",
    "    print(f\"\\n=== Summary ===\")\n",
    "    print(f\"Successful: {success_count}\")\n",
    "    print(f\"Failed: {fail_count}\")\n",
    "    print(f\"Total size: {total_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    if fail_count > 0:\n",
    "        print(f\"\\nFailed downloads:\")\n",
    "        display(download_results[~download_results['downloaded']][['ceqNumber', 'filename', 'error']])\n",
    "else:\n",
    "    print(\"No files to download. All comment letters already exist or queue is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comment Letter Verification ===\n",
      "Total expected: 11700\n",
      "Existing files: 11696 (100.0%)\n",
      "Missing files: 4 (0.0%)\n",
      "\n",
      "By year:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>existing</th>\n",
       "      <th>missing</th>\n",
       "      <th>pct_complete</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>478</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>596</td>\n",
       "      <td>596</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>553</td>\n",
       "      <td>553</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>631</td>\n",
       "      <td>631</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>712</td>\n",
       "      <td>712</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>912</td>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>911</td>\n",
       "      <td>911</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>894</td>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>661</td>\n",
       "      <td>661</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>406</td>\n",
       "      <td>406</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>352</td>\n",
       "      <td>352</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>331</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>318</td>\n",
       "      <td>314</td>\n",
       "      <td>4</td>\n",
       "      <td>98.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>293</td>\n",
       "      <td>293</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>284</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>165</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>139</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      total  existing  missing  pct_complete\n",
       "year                                        \n",
       "1987      2         2        0         100.0\n",
       "1988      1         1        0         100.0\n",
       "1990      2         2        0         100.0\n",
       "1991      4         4        0         100.0\n",
       "1992      3         3        0         100.0\n",
       "1993      3         3        0         100.0\n",
       "1994     10        10        0         100.0\n",
       "1995     20        20        0         100.0\n",
       "1996     59        59        0         100.0\n",
       "1997     82        82        0         100.0\n",
       "1998     58        58        0         100.0\n",
       "1999    202       202        0         100.0\n",
       "2000    369       369        0         100.0\n",
       "2001    478       478        0         100.0\n",
       "2002    596       596        0         100.0\n",
       "2003    553       553        0         100.0\n",
       "2004    631       631        0         100.0\n",
       "2005    751       751        0         100.0\n",
       "2006    712       712        0         100.0\n",
       "2007    912       912        0         100.0\n",
       "2008    911       911        0         100.0\n",
       "2009    894       894        0         100.0\n",
       "2010    661       661        0         100.0\n",
       "2011    406       406        0         100.0\n",
       "2012    352       352        0         100.0\n",
       "2013    332       332        0         100.0\n",
       "2014    331       331        0         100.0\n",
       "2015    318       314        4          98.7\n",
       "2016    293       293        0         100.0\n",
       "2017    197       197        0         100.0\n",
       "2018    284       284        0         100.0\n",
       "2019    252       252        0         100.0\n",
       "2020    221       221        0         100.0\n",
       "2021    165       165        0         100.0\n",
       "2022    163       163        0         100.0\n",
       "2023    139       139        0         100.0\n",
       "2024    198       198        0         100.0\n",
       "2025    135       135        0         100.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def verify_downloads():\n",
    "    \"\"\"\n",
    "    Verify downloaded comment letters against expected records.\n",
    "    \"\"\"\n",
    "    comment_df_full = load_comment_letter_records()\n",
    "    \n",
    "    # Build expected filenames using ceqNumber as prefix\n",
    "    comment_df_full['expectedFilename'] = comment_df_full.apply(\n",
    "        lambda row: build_local_filename(\n",
    "            row['ceqNumber'],\n",
    "            row.get('name') or row.get('fileNameForDownload') or f\"{row['attachmentId']}.pdf\"\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    comment_df_full['expectedPath'] = comment_df_full['expectedFilename'].apply(\n",
    "        lambda f: COMMENT_LETTERS_DIR / f\n",
    "    )\n",
    "    \n",
    "    # Check existence\n",
    "    comment_df_full['exists'] = comment_df_full['expectedPath'].apply(lambda p: p.exists())\n",
    "    \n",
    "    total = len(comment_df_full)\n",
    "    existing = comment_df_full['exists'].sum()\n",
    "    missing = total - existing\n",
    "    \n",
    "    print(f\"=== Comment Letter Verification ===\")\n",
    "    print(f\"Total expected: {total}\")\n",
    "    print(f\"Existing files: {existing} ({100*existing/total:.1f}%)\")\n",
    "    print(f\"Missing files: {missing} ({100*missing/total:.1f}%)\")\n",
    "    \n",
    "    # By year - use ceqNumber for year extraction\n",
    "    comment_df_full['year'] = comment_df_full['ceqNumber'].astype(str).str[:4]\n",
    "    print(f\"\\nBy year:\")\n",
    "    summary = comment_df_full.groupby('year').agg(\n",
    "        total=('exists', 'count'),\n",
    "        existing=('exists', 'sum')\n",
    "    )\n",
    "    summary['missing'] = summary['total'] - summary['existing']\n",
    "    summary['pct_complete'] = (100 * summary['existing'] / summary['total']).round(1)\n",
    "    display(summary)\n",
    "    \n",
    "    return comment_df_full\n",
    "\n",
    "verification_df = verify_downloads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry Failed Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_failed_downloads():\n",
    "    \"\"\"\n",
    "    Retry any previously failed downloads.\n",
    "    \"\"\"\n",
    "    status_df = load_download_status()\n",
    "    failed = status_df[~status_df['downloaded']]\n",
    "    \n",
    "    if len(failed) == 0:\n",
    "        print(\"No failed downloads to retry.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Retrying {len(failed)} failed downloads...\")\n",
    "    \n",
    "    # Rebuild queue\n",
    "    comment_df_full = load_comment_letter_records()\n",
    "    \n",
    "    if 'attachmentId' in failed.columns and failed['attachmentId'].notna().any():\n",
    "        retry_queue = comment_df_full[comment_df_full['attachmentId'].isin(failed['attachmentId'])].copy()\n",
    "    else:\n",
    "        retry_queue = comment_df_full[comment_df_full['eisId'].isin(failed['eisId'])].copy()\n",
    "    \n",
    "    # Build filenames\n",
    "    retry_queue['localFilename'] = retry_queue.apply(\n",
    "        lambda row: build_local_filename(\n",
    "            row['ceqNumber'],\n",
    "            row.get('name') or row.get('fileNameForDownload') or f\"{row['attachmentId']}.pdf\"\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    retry_queue['localPath'] = retry_queue['localFilename'].apply(lambda f: COMMENT_LETTERS_DIR / f)\n",
    "    \n",
    "    # Download (no copy optimization for retries)\n",
    "    retry_results = download_files(retry_queue, use_copy=False)\n",
    "    \n",
    "    # Update status\n",
    "    status_df = status_df[~status_df['filename'].isin(retry_results['filename'])]\n",
    "    combined_status = pd.concat([status_df, retry_results], ignore_index=True)\n",
    "    save_download_status(combined_status)\n",
    "    \n",
    "    success_count = retry_results['downloaded'].sum()\n",
    "    print(f\"Retry complete: {success_count}/{len(retry_results)} successful\")\n",
    "\n",
    "# Uncomment to retry failed downloads:\n",
    "# retry_failed_downloads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Downloaded Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/documents/comment_letters:\n",
      "  Total files: 11641\n",
      "  PDF files: 6428\n",
      "  Total size: 3963.26 MB\n",
      "\n",
      "Sample files:\n",
      "  19870010_19870010.pdf (569.6 KB)\n",
      "  19870393_19870393.pdf (2081.2 KB)\n",
      "  19880342_19880342.pdf (800.7 KB)\n",
      "  19900002_19900002.pdf (235.4 KB)\n",
      "  19900271_19900271.pdf (1777.3 KB)\n",
      "  19910005_19910005.pdf (290.1 KB)\n",
      "  19910009_19910009.pdf (977.3 KB)\n",
      "  19910014_19910014.pdf (309.5 KB)\n",
      "  19910406_PETRIFIED_FOREST_NP_GMP.pdf (279.0 KB)\n",
      "  19920401_19920401.pdf (38.8 KB)\n"
     ]
    }
   ],
   "source": [
    "# List files in comment_letters directory\n",
    "if COMMENT_LETTERS_DIR.exists():\n",
    "    files = list(COMMENT_LETTERS_DIR.iterdir())\n",
    "    pdf_files = [f for f in files if f.suffix.lower() == '.pdf']\n",
    "    \n",
    "    print(f\"Files in {COMMENT_LETTERS_DIR}:\")\n",
    "    print(f\"  Total files: {len(files)}\")\n",
    "    print(f\"  PDF files: {len(pdf_files)}\")\n",
    "    \n",
    "    if pdf_files:\n",
    "        total_size = sum(f.stat().st_size for f in pdf_files)\n",
    "        print(f\"  Total size: {total_size / (1024**2):.2f} MB\")\n",
    "        \n",
    "        print(f\"\\nSample files:\")\n",
    "        for f in sorted(pdf_files)[:10]:\n",
    "            print(f\"  {f.name} ({f.stat().st_size / 1024:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"Directory does not exist yet: {COMMENT_LETTERS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
