{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Convert EIS Documents to Markdown using Marker\n",
    "\n",
    "This notebook converts PDF documents to Markdown using `marker`, which provides high-quality extraction with better structure preservation than plain text extraction.\n",
    "\n",
    "**Key Features:**\n",
    "- Mirrors directory structure from `documents/` to `marker_conversions/`\n",
    "- Split/apply/combine strategy for large PDFs\n",
    "- Excludes images/maps/figures from output\n",
    "- Parallel processing with timeout handling\n",
    "- Tracks conversion progress to allow resuming\n",
    "\n",
    "**Output format:** Markdown files (`.md`) with preserved document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install marker-pdf pandas pyarrow tqdm pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import fitz  # pymupdf - for splitting PDFs\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "import subprocess\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Force fork context for multiprocessing (required for Jupyter on macOS)\n",
    "_mp_ctx = mp.get_context('fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository\n",
      "Documents directory: /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/documents\n",
      "Output directory: /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/marker_conversions\n",
      "Failed files log: /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/metadata/marker_conversion_failures.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "METADATA_DIR = REPO_ROOT / \"metadata\"\n",
    "DOCUMENTS_DIR = REPO_ROOT / \"documents\"  # Source PDFs\n",
    "OUTPUT_DIR = REPO_ROOT / \"marker_conversions\"  # Output markdown files\n",
    "\n",
    "# Input metadata file\n",
    "DOC_RECORD_FILE = METADATA_DIR / \"eis_document_record_api.parquet\"\n",
    "\n",
    "# Failed files tracking\n",
    "FAILED_FILES_LOG = METADATA_DIR / \"marker_conversion_failures.csv\"\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Documents directory: {DOCUMENTS_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Failed files log: {FAILED_FILES_LOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize filename to match existing convention.\n",
    "    - Remove special characters: ( ) & , ~\n",
    "    - Replace spaces with underscores\n",
    "    - Normalize PDF extension\n",
    "    \"\"\"\n",
    "    clean = re.sub(r'[()&,~\\/]', '', filename)\n",
    "    clean = re.sub(r'[\\s_]+', '_', clean)\n",
    "    clean = re.sub(r'\\.PDF$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = re.sub(r'\\.pdf\\.pdf$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = clean.strip('_')\n",
    "    return clean\n",
    "\n",
    "\n",
    "def build_local_filename(ceq_number, original_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the local filename following existing convention.\n",
    "    Format: {CEQ_NUMBER}_{sanitized_filename}\n",
    "    \"\"\"\n",
    "    sanitized = sanitize_filename(original_filename)\n",
    "    return f\"{ceq_number}_{sanitized}\"\n",
    "\n",
    "\n",
    "def get_year_from_ceq(ceq_number) -> str:\n",
    "    \"\"\"Extract year from CEQ Number (first 4 digits).\"\"\"\n",
    "    return str(ceq_number)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf(pdf_path: Path, output_dir: Path, pages_per_chunk: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Split a PDF into smaller chunks for processing.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to source PDF\n",
    "        output_dir: Directory to write chunk files\n",
    "        pages_per_chunk: Number of pages per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of paths to chunk PDFs\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    num_pages = len(doc)\n",
    "    chunk_paths = []\n",
    "    \n",
    "    for start_page in range(0, num_pages, pages_per_chunk):\n",
    "        end_page = min(start_page + pages_per_chunk, num_pages)\n",
    "        chunk_doc = fitz.open()  # New empty PDF\n",
    "        chunk_doc.insert_pdf(doc, from_page=start_page, to_page=end_page - 1)\n",
    "        \n",
    "        chunk_path = output_dir / f\"chunk_{start_page:05d}_{end_page:05d}.pdf\"\n",
    "        chunk_doc.save(str(chunk_path))\n",
    "        chunk_doc.close()\n",
    "        chunk_paths.append(chunk_path)\n",
    "    \n",
    "    doc.close()\n",
    "    return chunk_paths\n",
    "\n",
    "\n",
    "def get_pdf_page_count(pdf_path: Path) -> int:\n",
    "    \"\"\"Get the number of pages in a PDF.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        count = len(doc)\n",
    "        doc.close()\n",
    "        return count\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_marker_on_pdf(pdf_path: Path, output_dir: Path, timeout_seconds: int = 300) -> tuple:\n",
    "    \"\"\"\n",
    "    Run marker on a single PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        output_dir: Directory where marker will write output\n",
    "        timeout_seconds: Maximum time to wait for marker\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (success: bool, markdown_content: str, error: str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run marker via command line\n",
    "        # --disable_image_extraction: skip images (faster, smaller output)\n",
    "        # --output_format markdown: output as markdown\n",
    "        result = subprocess.run(\n",
    "            [\n",
    "                \"marker_single\",\n",
    "                str(pdf_path),\n",
    "                \"--output_dir\", str(output_dir),\n",
    "                \"--output_format\", \"markdown\",\n",
    "                \"--disable_image_extraction\",\n",
    "            ],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout_seconds\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            return False, \"\", f\"Marker failed: {result.stderr[:500]}\"\n",
    "        \n",
    "        # Find the output markdown file\n",
    "        # Marker creates: output_dir/pdf_stem/pdf_stem.md\n",
    "        pdf_stem = pdf_path.stem\n",
    "        md_path = output_dir / pdf_stem / f\"{pdf_stem}.md\"\n",
    "        \n",
    "        if not md_path.exists():\n",
    "            # Try to find any .md file in the output\n",
    "            md_files = list(output_dir.rglob(\"*.md\"))\n",
    "            if md_files:\n",
    "                md_path = md_files[0]\n",
    "            else:\n",
    "                return False, \"\", \"No markdown output found\"\n",
    "        \n",
    "        with open(md_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        return True, content, None\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, \"\", f\"Timeout after {timeout_seconds}s\"\n",
    "    except Exception as e:\n",
    "        return False, \"\", str(e)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_images_from_markdown(markdown_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove image references and figure captions from markdown.\n",
    "    \n",
    "    Removes:\n",
    "    - ![alt](path) image syntax\n",
    "    - <img> HTML tags\n",
    "    - Figure/Map/Image caption lines\n",
    "    \"\"\"\n",
    "    # Remove markdown image syntax: ![alt text](path)\n",
    "    text = re.sub(r'!\\[[^\\]]*\\]\\([^)]+\\)', '', markdown_text)\n",
    "    \n",
    "    # Remove HTML img tags\n",
    "    text = re.sub(r'<img[^>]*>', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove figure/map/image caption lines (lines that are just \"Figure X.X\" etc.)\n",
    "    text = re.sub(r'^\\s*(Figure|Map|Image|Photo|Photograph|Exhibit)\\s+[\\d.\\-]+[^\\n]*$', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    \n",
    "    # Remove multiple consecutive blank lines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_with_marker(pdf_path: Path, output_path: Path, \n",
    "                            pages_per_chunk: int = 50,\n",
    "                            chunk_timeout: int = 300) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a PDF to markdown using marker with split/apply/combine for large files.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Source PDF path\n",
    "        output_path: Output markdown path\n",
    "        pages_per_chunk: Pages per chunk for splitting\n",
    "        chunk_timeout: Timeout per chunk in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Dict with conversion status\n",
    "    \"\"\"\n",
    "    if not pdf_path.exists():\n",
    "        return {\"converted\": False, \"num_pages\": 0, \"error\": \"Source file not found\"}\n",
    "    \n",
    "    try:\n",
    "        num_pages = get_pdf_page_count(pdf_path)\n",
    "        if num_pages == 0:\n",
    "            return {\"converted\": False, \"num_pages\": 0, \"error\": \"Empty or unreadable PDF\"}\n",
    "        \n",
    "        # Create temp directory for processing\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_path = Path(temp_dir)\n",
    "            \n",
    "            # Decide whether to split\n",
    "            if num_pages <= pages_per_chunk:\n",
    "                # Small file - process directly\n",
    "                chunks = [pdf_path]\n",
    "                chunk_dirs = [temp_path / \"output\"]\n",
    "                chunk_dirs[0].mkdir()\n",
    "            else:\n",
    "                # Large file - split into chunks\n",
    "                split_dir = temp_path / \"splits\"\n",
    "                split_dir.mkdir()\n",
    "                chunks = split_pdf(pdf_path, split_dir, pages_per_chunk)\n",
    "                chunk_dirs = [temp_path / f\"output_{i}\" for i in range(len(chunks))]\n",
    "                for d in chunk_dirs:\n",
    "                    d.mkdir()\n",
    "            \n",
    "            # Process each chunk\n",
    "            markdown_parts = []\n",
    "            for i, (chunk_path, chunk_output_dir) in enumerate(zip(chunks, chunk_dirs)):\n",
    "                success, content, error = run_marker_on_pdf(chunk_path, chunk_output_dir, chunk_timeout)\n",
    "                \n",
    "                if not success:\n",
    "                    return {\n",
    "                        \"converted\": False, \n",
    "                        \"num_pages\": num_pages, \n",
    "                        \"error\": f\"Chunk {i+1}/{len(chunks)} failed: {error}\"\n",
    "                    }\n",
    "                \n",
    "                markdown_parts.append(content)\n",
    "            \n",
    "            # Combine chunks\n",
    "            if len(markdown_parts) > 1:\n",
    "                combined_markdown = \"\\n\\n---\\n\\n\".join(markdown_parts)\n",
    "            else:\n",
    "                combined_markdown = markdown_parts[0]\n",
    "            \n",
    "            # Remove images/figures\n",
    "            clean_markdown = remove_images_from_markdown(combined_markdown)\n",
    "            \n",
    "            # Write output\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(clean_markdown)\n",
    "            \n",
    "            return {\"converted\": True, \"num_pages\": num_pages, \"error\": None}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"converted\": False, \"num_pages\": 0, \"error\": str(e)[:500]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Load Document Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_records():\n",
    "    \"\"\"Load document records from the API metadata.\"\"\"\n",
    "    if DOC_RECORD_FILE.exists():\n",
    "        return pd.read_parquet(DOC_RECORD_FILE)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Document records not found at {DOC_RECORD_FILE}.\\n\"\n",
    "            f\"Run fetch_eis_records_api.ipynb first.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_failed_files() -> set:\n",
    "    \"\"\"Load set of attachment IDs that previously failed.\"\"\"\n",
    "    if FAILED_FILES_LOG.exists():\n",
    "        df = pd.read_csv(FAILED_FILES_LOG)\n",
    "        return set(df['attachmentId'].astype(str))\n",
    "    return set()\n",
    "\n",
    "\n",
    "def append_failed_file(attachment_id, ceq_number, source_file, error):\n",
    "    \"\"\"Append a failed file to the log.\"\"\"\n",
    "    file_exists = FAILED_FILES_LOG.exists()\n",
    "    with open(FAILED_FILES_LOG, 'a', encoding='utf-8') as f:\n",
    "        if not file_exists:\n",
    "            f.write(\"attachmentId,ceqNumber,source_file,error,timestamp\\n\")\n",
    "        error_clean = str(error).replace('\"', \"'\").replace('\\n', ' ')[:200]\n",
    "        f.write(f'{attachment_id},{ceq_number},\"{source_file}\",\"{error_clean}\",{datetime.now().isoformat()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45704 document records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ceqNumber</th>\n",
       "      <th>year</th>\n",
       "      <th>localFilename</th>\n",
       "      <th>sourcePath</th>\n",
       "      <th>outputPath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_LoMo_FRR_Comprehensive_Study_Draft_Re...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_LoMo_System_Plan_-_Basis_of_Estimate.pdf</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_Appendix_A.1_LoMo_FRM_Past_Performanc...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_Appendix_A.2.1_LoMo_RAS_Calibration_O...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_Appendix_A.2.2_LoMo_RAS_Calibration_K...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "      <td>/Users/admin-tascott/Documents/GitHub/eis_docu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ceqNumber  year                                      localFilename  \\\n",
       "0  20250186  2025  20250186_LoMo_FRR_Comprehensive_Study_Draft_Re...   \n",
       "1  20250186  2025  20250186_LoMo_System_Plan_-_Basis_of_Estimate.pdf   \n",
       "2  20250186  2025  20250186_Appendix_A.1_LoMo_FRM_Past_Performanc...   \n",
       "3  20250186  2025  20250186_Appendix_A.2.1_LoMo_RAS_Calibration_O...   \n",
       "4  20250186  2025  20250186_Appendix_A.2.2_LoMo_RAS_Calibration_K...   \n",
       "\n",
       "                                          sourcePath  \\\n",
       "0  /Users/admin-tascott/Documents/GitHub/eis_docu...   \n",
       "1  /Users/admin-tascott/Documents/GitHub/eis_docu...   \n",
       "2  /Users/admin-tascott/Documents/GitHub/eis_docu...   \n",
       "3  /Users/admin-tascott/Documents/GitHub/eis_docu...   \n",
       "4  /Users/admin-tascott/Documents/GitHub/eis_docu...   \n",
       "\n",
       "                                          outputPath  \n",
       "0  /Users/admin-tascott/Documents/GitHub/eis_docu...  \n",
       "1  /Users/admin-tascott/Documents/GitHub/eis_docu...  \n",
       "2  /Users/admin-tascott/Documents/GitHub/eis_docu...  \n",
       "3  /Users/admin-tascott/Documents/GitHub/eis_docu...  \n",
       "4  /Users/admin-tascott/Documents/GitHub/eis_docu...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load document records\n",
    "doc_df = load_document_records()\n",
    "print(f\"Loaded {len(doc_df)} document records\")\n",
    "\n",
    "# Add helper columns\n",
    "doc_df['year'] = doc_df['ceqNumber'].astype(str).str[:4]\n",
    "doc_df['localFilename'] = doc_df.apply(\n",
    "    lambda row: build_local_filename(\n",
    "        row['ceqNumber'], \n",
    "        row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Build source and output paths\n",
    "doc_df['sourcePath'] = doc_df.apply(\n",
    "    lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n",
    "    axis=1\n",
    ")\n",
    "doc_df['outputPath'] = doc_df.apply(\n",
    "    lambda row: OUTPUT_DIR / row['year'] / (row['localFilename'].replace('.pdf', '.md').replace('.PDF', '.md')),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "display(doc_df[['ceqNumber', 'year', 'localFilename', 'sourcePath', 'outputPath']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files found: 45559 / 45704\n",
      "Documents available for conversion: 45559\n",
      "\n",
      "By year:\n",
      "year\n",
      "1987       2\n",
      "1988       1\n",
      "1990       2\n",
      "1991       4\n",
      "1992       3\n",
      "1993       3\n",
      "1994      10\n",
      "1995      20\n",
      "1996      59\n",
      "1997      82\n",
      "1998      58\n",
      "1999     202\n",
      "2000     369\n",
      "2001     481\n",
      "2002     596\n",
      "2003     562\n",
      "2004     631\n",
      "2005     755\n",
      "2006     712\n",
      "2007     912\n",
      "2008     911\n",
      "2009     894\n",
      "2010     661\n",
      "2011     406\n",
      "2012     991\n",
      "2013    3007\n",
      "2014    2918\n",
      "2015    3991\n",
      "2016    3651\n",
      "2017    2697\n",
      "2018    3239\n",
      "2019    2865\n",
      "2020    3280\n",
      "2021    2309\n",
      "2022    2206\n",
      "2023    1633\n",
      "2024    2498\n",
      "2025    1938\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check which source files exist\n",
    "doc_df['sourceExists'] = doc_df['sourcePath'].apply(lambda p: p.exists())\n",
    "print(f\"Source files found: {doc_df['sourceExists'].sum()} / {len(doc_df)}\")\n",
    "\n",
    "# Documents with source files available for conversion\n",
    "to_convert = doc_df[doc_df['sourceExists']].copy()\n",
    "print(f\"Documents available for conversion: {len(to_convert)}\")\n",
    "\n",
    "print(f\"\\nBy year:\")\n",
    "print(to_convert['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Conversion Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "  YEAR_FILTER: [2019]\n",
      "  MAX_CONVERSIONS: 10\n",
      "  RETRY_FAILURES: False\n",
      "  PAGES_PER_CHUNK: 10\n",
      "  CHUNK_TIMEOUT_SECONDS: 300\n",
      "  PDF_TIMEOUT_SECONDS: 1800\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONVERSION SETTINGS - MODIFY AS NEEDED\n",
    "# ============================================\n",
    "\n",
    "# Filter by year (set to None to convert all years)\n",
    "YEAR_FILTER = None\n",
    "YEAR_FILTER = [2019]  # Example: specific year\n",
    "\n",
    "# Maximum number of files to convert (set to None for all)\n",
    "MAX_CONVERSIONS = None\n",
    "MAX_CONVERSIONS = 10  # Start with a small test batch\n",
    "\n",
    "# Set to True to retry previously failed files\n",
    "RETRY_FAILURES = False\n",
    "\n",
    "# Pages per chunk for splitting large PDFs\n",
    "PAGES_PER_CHUNK = 10\n",
    "\n",
    "# Timeout per chunk in seconds (marker is slower than pymupdf)\n",
    "CHUNK_TIMEOUT_SECONDS = 300  # 5 minutes per 50 pages\n",
    "\n",
    "# Overall timeout per PDF (for the multiprocessing wrapper)\n",
    "PDF_TIMEOUT_SECONDS = 1800  # 30 minutes max per PDF\n",
    "\n",
    "print(f\"Settings:\")\n",
    "print(f\"  YEAR_FILTER: {YEAR_FILTER}\")\n",
    "print(f\"  MAX_CONVERSIONS: {MAX_CONVERSIONS}\")\n",
    "print(f\"  RETRY_FAILURES: {RETRY_FAILURES}\")\n",
    "print(f\"  PAGES_PER_CHUNK: {PAGES_PER_CHUNK}\")\n",
    "print(f\"  CHUNK_TIMEOUT_SECONDS: {CHUNK_TIMEOUT_SECONDS}\")\n",
    "print(f\"  PDF_TIMEOUT_SECONDS: {PDF_TIMEOUT_SECONDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to years [2019]: 2865 documents\n",
      "Already converted (md exists): 0\n",
      "Previously failed (skipping): 1\n",
      "Limited to 10 conversions\n",
      "\n",
      "Final conversion queue: 10 files\n"
     ]
    }
   ],
   "source": [
    "# Build conversion queue\n",
    "conversion_queue = to_convert.copy()\n",
    "\n",
    "# Apply year filter\n",
    "if YEAR_FILTER:\n",
    "    year_filter_str = [str(y) for y in YEAR_FILTER]\n",
    "    conversion_queue = conversion_queue[conversion_queue['year'].isin(year_filter_str)]\n",
    "    print(f\"Filtered to years {YEAR_FILTER}: {len(conversion_queue)} documents\")\n",
    "\n",
    "# Skip files that already have .md output\n",
    "conversion_queue['outputExists'] = conversion_queue['outputPath'].apply(lambda p: p.exists())\n",
    "existing_count = conversion_queue['outputExists'].sum()\n",
    "print(f\"Already converted (md exists): {existing_count}\")\n",
    "conversion_queue = conversion_queue[~conversion_queue['outputExists']]\n",
    "\n",
    "# Skip previously failed files (unless RETRY_FAILURES is True)\n",
    "if not RETRY_FAILURES:\n",
    "    failed_ids = load_failed_files()\n",
    "    if failed_ids:\n",
    "        conversion_queue['previouslyFailed'] = conversion_queue['attachmentId'].astype(str).isin(failed_ids)\n",
    "        failed_count = conversion_queue['previouslyFailed'].sum()\n",
    "        print(f\"Previously failed (skipping): {failed_count}\")\n",
    "        conversion_queue = conversion_queue[~conversion_queue['previouslyFailed']]\n",
    "\n",
    "# Apply max conversions limit\n",
    "if MAX_CONVERSIONS and len(conversion_queue) > MAX_CONVERSIONS:\n",
    "    conversion_queue = conversion_queue.head(MAX_CONVERSIONS)\n",
    "    print(f\"Limited to {MAX_CONVERSIONS} conversions\")\n",
    "\n",
    "print(f\"\\nFinal conversion queue: {len(conversion_queue)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Create Output Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 year directories in documents/\n",
      "Created directory structure in /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/marker_conversions\n"
     ]
    }
   ],
   "source": [
    "# Create output directory structure mirroring documents/\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get all year directories from documents\n",
    "year_dirs = [d for d in DOCUMENTS_DIR.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "print(f\"Found {len(year_dirs)} year directories in documents/\")\n",
    "\n",
    "# Create corresponding directories in marker_conversions/\n",
    "for year_dir in year_dirs:\n",
    "    output_year_dir = OUTPUT_DIR / year_dir.name\n",
    "    output_year_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created directory structure in {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Run Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_in_process(args: tuple, result_queue, pages_per_chunk: int, chunk_timeout: int):\n",
    "    \"\"\"Worker function that runs marker conversion in a separate process.\"\"\"\n",
    "    pdf_path, output_path, ceq_number, attachment_id = args\n",
    "    pdf_path = Path(pdf_path)\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    try:\n",
    "        result = convert_pdf_with_marker(\n",
    "            pdf_path, output_path,\n",
    "            pages_per_chunk=pages_per_chunk,\n",
    "            chunk_timeout=chunk_timeout\n",
    "        )\n",
    "        result_queue.put(result)\n",
    "    except Exception as e:\n",
    "        result_queue.put({\"converted\": False, \"num_pages\": 0, \"error\": str(e)[:500]})\n",
    "\n",
    "\n",
    "def convert_with_timeout(args: tuple, timeout_seconds: int, \n",
    "                         pages_per_chunk: int, chunk_timeout: int) -> dict:\n",
    "    \"\"\"Run conversion in a subprocess that can be killed on timeout.\"\"\"\n",
    "    result_queue = _mp_ctx.Queue()\n",
    "    proc = _mp_ctx.Process(\n",
    "        target=_convert_in_process, \n",
    "        args=(args, result_queue, pages_per_chunk, chunk_timeout)\n",
    "    )\n",
    "    proc.start()\n",
    "    proc.join(timeout=timeout_seconds)\n",
    "    \n",
    "    if proc.is_alive():\n",
    "        proc.terminate()\n",
    "        proc.join(timeout=5)\n",
    "        if proc.is_alive():\n",
    "            proc.kill()\n",
    "            proc.join()\n",
    "        return {\"converted\": False, \"num_pages\": 0, \"error\": f\"TIMEOUT after {timeout_seconds}s\"}\n",
    "    \n",
    "    try:\n",
    "        return result_queue.get_nowait()\n",
    "    except:\n",
    "        return {\"converted\": False, \"num_pages\": 0, \"error\": \"Process ended without result\"}\n",
    "\n",
    "\n",
    "def run_conversions(queue: pd.DataFrame, timeout_seconds: int = 1800,\n",
    "                    pages_per_chunk: int = 50, chunk_timeout: int = 300) -> dict:\n",
    "    \"\"\"Run marker extraction with hard timeouts that kill hung processes.\"\"\"\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    total_pages = 0\n",
    "    \n",
    "    pbar = tqdm(queue.iterrows(), total=len(queue), desc=\"Converting\")\n",
    "    \n",
    "    for idx, row in pbar:\n",
    "        filename = Path(row['sourcePath']).name\n",
    "        short_name = filename[:40] + \"...\" if len(filename) > 40 else filename\n",
    "        pbar.set_postfix_str(short_name)\n",
    "        \n",
    "        args = (str(row['sourcePath']), str(row['outputPath']), row['ceqNumber'], row['attachmentId'])\n",
    "        \n",
    "        try:\n",
    "            result = convert_with_timeout(args, timeout_seconds, pages_per_chunk, chunk_timeout)\n",
    "            \n",
    "            if result.get('converted'):\n",
    "                success_count += 1\n",
    "                total_pages += result.get('num_pages', 0)\n",
    "            else:\n",
    "                fail_count += 1\n",
    "                append_failed_file(\n",
    "                    row['attachmentId'],\n",
    "                    row['ceqNumber'],\n",
    "                    str(row['sourcePath']),\n",
    "                    result.get('error', 'Unknown error')\n",
    "                )\n",
    "                logger.warning(f\"Failed: {short_name} - {result.get('error', '')[:60]}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            fail_count += 1\n",
    "            append_failed_file(\n",
    "                row['attachmentId'],\n",
    "                row['ceqNumber'],\n",
    "                str(row['sourcePath']),\n",
    "                f\"EXCEPTION: {str(e)[:200]}\"\n",
    "            )\n",
    "            logger.error(f\"Exception: {short_name} - {str(e)[:60]}\")\n",
    "    \n",
    "    return {\n",
    "        'success': success_count,\n",
    "        'failed': fail_count,\n",
    "        'total_pages': total_pages\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting marker conversion of 10 files...\n",
      "Timeout per file: 1800s, per chunk: 300s\n",
      "Pages per chunk: 10\n",
      "Failures logged to: /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/metadata/marker_conversion_failures.csv\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6494aac61204fe28dbb7a4087da85fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 22:51:22,413 - WARNING - Failed: 20190296_Butterfield_Sentinel_Draft_EIR-... - Chunk 1/13 failed: Timeout after 300s\n",
      "2026-02-05 22:56:23,448 - WARNING - Failed: 20190296_Butterfield_Sentinel_Draft_EIR-... - Chunk 1/139 failed: Timeout after 300s\n",
      "2026-02-05 23:01:44,113 - WARNING - Failed: 20190296_Butterfield_Sentinel_Draft_EIR-... - Chunk 1/43 failed: Timeout after 300s\n",
      "Process ForkProcess-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/_0/grm8p2890sj6kq7p40wjd3pw0000gn/T/ipykernel_22826/2461315604.py\", line 8, in _convert_in_process\n",
      "    result = convert_pdf_with_marker(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/_0/grm8p2890sj6kq7p40wjd3pw0000gn/T/ipykernel_22826/673105759.py\", line 46, in convert_pdf_with_marker\n",
      "    success, content, error = run_marker_on_pdf(chunk_path, chunk_output_dir, chunk_timeout)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 12\u001b[0m results \u001b[38;5;241m=\u001b[39m run_conversions(\n\u001b[1;32m     13\u001b[0m     conversion_queue, \n\u001b[1;32m     14\u001b[0m     timeout_seconds\u001b[38;5;241m=\u001b[39mPDF_TIMEOUT_SECONDS,\n\u001b[1;32m     15\u001b[0m     pages_per_chunk\u001b[38;5;241m=\u001b[39mPAGES_PER_CHUNK,\n\u001b[1;32m     16\u001b[0m     chunk_timeout\u001b[38;5;241m=\u001b[39mCHUNK_TIMEOUT_SECONDS\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Conversion Summary ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 60\u001b[0m, in \u001b[0;36mrun_conversions\u001b[0;34m(queue, timeout_seconds, pages_per_chunk, chunk_timeout)\u001b[0m\n\u001b[1;32m     57\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msourcePath\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputPath\u001b[39m\u001b[38;5;124m'\u001b[39m]), row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mceqNumber\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattachmentId\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     result \u001b[38;5;241m=\u001b[39m convert_with_timeout(args, timeout_seconds, pages_per_chunk, chunk_timeout)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconverted\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     63\u001b[0m         success_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m, in \u001b[0;36mconvert_with_timeout\u001b[0;34m(args, timeout_seconds, pages_per_chunk, chunk_timeout)\u001b[0m\n\u001b[1;32m     22\u001b[0m proc \u001b[38;5;241m=\u001b[39m _mp_ctx\u001b[38;5;241m.\u001b[39mProcess(\n\u001b[1;32m     23\u001b[0m     target\u001b[38;5;241m=\u001b[39m_convert_in_process, \n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39m(args, result_queue, pages_per_chunk, chunk_timeout)\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m proc\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 27\u001b[0m proc\u001b[38;5;241m.\u001b[39mjoin(timeout\u001b[38;5;241m=\u001b[39mtimeout_seconds)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m     30\u001b[0m     proc\u001b[38;5;241m.\u001b[39mterminate()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel], timeout):\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/connection.py:947\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    944\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 947\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/var/folders/_0/grm8p2890sj6kq7p40wjd3pw0000gn/T/ipykernel_22826/1794098468.py\", line 17, in run_marker_on_pdf\n",
      "    result = subprocess.run(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/subprocess.py\", line 550, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/subprocess.py\", line 1209, in communicate\n",
      "    stdout, stderr = self._communicate(input, endtime, timeout)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/subprocess.py\", line 2133, in _communicate\n",
      "    data = os.read(key.fd, 32768)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Run the conversions\n",
    "if len(conversion_queue) > 0:\n",
    "    print(f\"Starting marker conversion of {len(conversion_queue)} files...\")\n",
    "    print(f\"Timeout per file: {PDF_TIMEOUT_SECONDS}s, per chunk: {CHUNK_TIMEOUT_SECONDS}s\")\n",
    "    print(f\"Pages per chunk: {PAGES_PER_CHUNK}\")\n",
    "    print(f\"Failures logged to: {FAILED_FILES_LOG}\")\n",
    "    print()\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = run_conversions(\n",
    "        conversion_queue, \n",
    "        timeout_seconds=PDF_TIMEOUT_SECONDS,\n",
    "        pages_per_chunk=PAGES_PER_CHUNK,\n",
    "        chunk_timeout=CHUNK_TIMEOUT_SECONDS\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n=== Conversion Summary ===\")\n",
    "    print(f\"Time elapsed: {elapsed/60:.1f} minutes\")\n",
    "    print(f\"Successful: {results['success']}\")\n",
    "    print(f\"Failed: {results['failed']}\")\n",
    "    print(f\"Total pages: {results['total_pages']:,}\")\n",
    "    if elapsed > 0:\n",
    "        print(f\"Rate: {(results['success'] + results['failed'])/elapsed:.2f} docs/sec\")\n",
    "        print(f\"Rate: {results['total_pages']/elapsed:.1f} pages/sec\")\n",
    "    \n",
    "    if results['failed'] > 0:\n",
    "        print(f\"\\nSee {FAILED_FILES_LOG} for failure details\")\n",
    "else:\n",
    "    print(\"No files to convert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Verify Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_conversions():\n",
    "    \"\"\"Quick verification of conversion status.\"\"\"\n",
    "    doc_df = load_document_records()\n",
    "    \n",
    "    # Build paths\n",
    "    doc_df['year'] = doc_df['ceqNumber'].astype(str).str[:4]\n",
    "    doc_df['localFilename'] = doc_df.apply(\n",
    "        lambda row: build_local_filename(\n",
    "            row['ceqNumber'], \n",
    "            row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    doc_df['sourcePath'] = doc_df.apply(\n",
    "        lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n",
    "        axis=1\n",
    "    )\n",
    "    doc_df['outputPath'] = doc_df.apply(\n",
    "        lambda row: OUTPUT_DIR / row['year'] / (row['localFilename'].replace('.pdf', '.md').replace('.PDF', '.md')),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Check status\n",
    "    doc_df['sourceExists'] = doc_df['sourcePath'].apply(lambda p: p.exists())\n",
    "    doc_df['outputExists'] = doc_df['outputPath'].apply(lambda p: p.exists())\n",
    "    \n",
    "    with_source = doc_df[doc_df['sourceExists']]\n",
    "    total = len(with_source)\n",
    "    converted = with_source['outputExists'].sum()\n",
    "    \n",
    "    # Count failures\n",
    "    failed_ids = load_failed_files()\n",
    "    \n",
    "    print(f\"=== Marker Conversion Status ===\")\n",
    "    print(f\"Source PDFs: {total}\")\n",
    "    print(f\"Converted: {converted} ({100*converted/total:.1f}%)\")\n",
    "    print(f\"Failed: {len(failed_ids)}\")\n",
    "    print(f\"Remaining: {total - converted - len(failed_ids)}\")\n",
    "\n",
    "verify_conversions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Sample Output Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample converted file\n",
    "def show_sample_output(n_chars: int = 3000):\n",
    "    \"\"\"Display the beginning of a random converted markdown file.\"\"\"\n",
    "    md_files = list(OUTPUT_DIR.glob(\"**/*.md\"))\n",
    "    \n",
    "    if not md_files:\n",
    "        print(\"No converted files found.\")\n",
    "        return\n",
    "    \n",
    "    import random\n",
    "    sample_file = random.choice(md_files)\n",
    "    \n",
    "    print(f\"File: {sample_file.name}\")\n",
    "    print(f\"Size: {sample_file.stat().st_size:,} bytes\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "        print(f.read(n_chars))\n",
    "\n",
    "# Uncomment to see a sample:\n",
    "# show_sample_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
