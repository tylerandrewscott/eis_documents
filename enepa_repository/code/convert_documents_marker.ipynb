{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Convert EIS Documents to Markdown using Marker\n",
    "\n",
    "This notebook converts PDF documents to Markdown using `marker`, which provides high-quality extraction with better structure preservation than plain text extraction.\n",
    "\n",
    "**Key Features:**\n",
    "- Mirrors directory structure from `documents/` to `marker_conversions/`\n",
    "- Split/apply/combine strategy for large PDFs\n",
    "- Excludes images/maps/figures from output\n",
    "- Parallel processing with timeout handling\n",
    "- Tracks conversion progress to allow resuming\n",
    "\n",
    "**Output format:** Markdown files (`.md`) with preserved document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "#!pip install marker-pdf pandas pyarrow tqdm pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import fitz  # pymupdf - for splitting PDFs\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "import subprocess\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Force fork context for multiprocessing (required for Jupyter on macOS)\n",
    "_mp_ctx = mp.get_context('fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/tscott1/Documents/GitHub/eis_documents/enepa_repository\n",
      "Documents directory: /Users/tscott1/Documents/GitHub/eis_documents/enepa_repository/documents\n",
      "Output directory: /Users/tscott1/Documents/GitHub/eis_documents/enepa_repository/marker_conversions\n",
      "Failed files log: /Users/tscott1/Documents/GitHub/eis_documents/enepa_repository/metadata/marker_conversion_failures.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "METADATA_DIR = REPO_ROOT / \"metadata\"\n",
    "DOCUMENTS_DIR = REPO_ROOT / \"documents\"  # Source PDFs\n",
    "OUTPUT_DIR = REPO_ROOT / \"marker_conversions\"  # Output markdown files\n",
    "\n",
    "# Input metadata file\n",
    "DOC_RECORD_FILE = METADATA_DIR / \"eis_document_record_api.parquet\"\n",
    "\n",
    "# Failed files tracking\n",
    "FAILED_FILES_LOG = METADATA_DIR / \"marker_conversion_failures.csv\"\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Documents directory: {DOCUMENTS_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Failed files log: {FAILED_FILES_LOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize filename to match existing convention.\n",
    "    - Remove special characters: ( ) & , ~\n",
    "    - Replace spaces with underscores\n",
    "    - Normalize PDF extension\n",
    "    \"\"\"\n",
    "    clean = re.sub(r'[()&,~\\/]', '', filename)\n",
    "    clean = re.sub(r'[\\s_]+', '_', clean)\n",
    "    clean = re.sub(r'\\.PDF$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = re.sub(r'\\.pdf\\.pdf$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = clean.strip('_')\n",
    "    return clean\n",
    "\n",
    "\n",
    "def build_local_filename(ceq_number, original_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the local filename following existing convention.\n",
    "    Format: {CEQ_NUMBER}_{sanitized_filename}\n",
    "    \"\"\"\n",
    "    sanitized = sanitize_filename(original_filename)\n",
    "    return f\"{ceq_number}_{sanitized}\"\n",
    "\n",
    "\n",
    "def get_year_from_ceq(ceq_number) -> str:\n",
    "    \"\"\"Extract year from CEQ Number (first 4 digits).\"\"\"\n",
    "    return str(ceq_number)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf(pdf_path: Path, output_dir: Path, pages_per_chunk: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Split a PDF into smaller chunks for processing.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to source PDF\n",
    "        output_dir: Directory to write chunk files\n",
    "        pages_per_chunk: Number of pages per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of paths to chunk PDFs\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    num_pages = len(doc)\n",
    "    chunk_paths = []\n",
    "    \n",
    "    for start_page in range(0, num_pages, pages_per_chunk):\n",
    "        end_page = min(start_page + pages_per_chunk, num_pages)\n",
    "        chunk_doc = fitz.open()  # New empty PDF\n",
    "        chunk_doc.insert_pdf(doc, from_page=start_page, to_page=end_page - 1)\n",
    "        \n",
    "        chunk_path = output_dir / f\"chunk_{start_page:05d}_{end_page:05d}.pdf\"\n",
    "        chunk_doc.save(str(chunk_path))\n",
    "        chunk_doc.close()\n",
    "        chunk_paths.append(chunk_path)\n",
    "    \n",
    "    doc.close()\n",
    "    return chunk_paths\n",
    "\n",
    "\n",
    "def get_pdf_page_count(pdf_path: Path) -> int:\n",
    "    \"\"\"Get the number of pages in a PDF.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        count = len(doc)\n",
    "        doc.close()\n",
    "        return count\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_marker_on_pdf(pdf_path: Path, output_dir: Path, timeout_seconds: int = 300) -> tuple:\n",
    "    \"\"\"\n",
    "    Run marker on a single PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        output_dir: Directory where marker will write output\n",
    "        timeout_seconds: Maximum time to wait for marker\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (success: bool, markdown_content: str, error: str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run marker via command line\n",
    "        # --disable_image_extraction: skip images (faster, smaller output)\n",
    "        # --output_format markdown: output as markdown\n",
    "        result = subprocess.run(\n",
    "            [\n",
    "                \"marker_single\",\n",
    "                str(pdf_path),\n",
    "                \"--output_dir\", str(output_dir),\n",
    "                \"--output_format\", \"markdown\",\n",
    "                \"--disable_image_extraction\",\n",
    "            ],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout_seconds\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            return False, \"\", f\"Marker failed: {result.stderr[:500]}\"\n",
    "        \n",
    "        # Find the output markdown file\n",
    "        # Marker creates: output_dir/pdf_stem/pdf_stem.md\n",
    "        pdf_stem = pdf_path.stem\n",
    "        md_path = output_dir / pdf_stem / f\"{pdf_stem}.md\"\n",
    "        \n",
    "        if not md_path.exists():\n",
    "            # Try to find any .md file in the output\n",
    "            md_files = list(output_dir.rglob(\"*.md\"))\n",
    "            if md_files:\n",
    "                md_path = md_files[0]\n",
    "            else:\n",
    "                return False, \"\", \"No markdown output found\"\n",
    "        \n",
    "        with open(md_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        return True, content, None\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, \"\", f\"Timeout after {timeout_seconds}s\"\n",
    "    except Exception as e:\n",
    "        return False, \"\", str(e)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_images_from_markdown(markdown_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove image references and figure captions from markdown.\n",
    "    \n",
    "    Removes:\n",
    "    - ![alt](path) image syntax\n",
    "    - <img> HTML tags\n",
    "    - Figure/Map/Image caption lines\n",
    "    \"\"\"\n",
    "    # Remove markdown image syntax: ![alt text](path)\n",
    "    text = re.sub(r'!\\[[^\\]]*\\]\\([^)]+\\)', '', markdown_text)\n",
    "    \n",
    "    # Remove HTML img tags\n",
    "    text = re.sub(r'<img[^>]*>', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove figure/map/image caption lines (lines that are just \"Figure X.X\" etc.)\n",
    "    text = re.sub(r'^\\s*(Figure|Map|Image|Photo|Photograph|Exhibit)\\s+[\\d.\\-]+[^\\n]*$', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    \n",
    "    # Remove multiple consecutive blank lines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_with_marker(pdf_path: Path, output_path: Path, \n",
    "                            pages_per_chunk: int = 50,\n",
    "                            chunk_timeout: int = 300) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a PDF to markdown using marker with split/apply/combine for large files.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Source PDF path\n",
    "        output_path: Output markdown path\n",
    "        pages_per_chunk: Pages per chunk for splitting\n",
    "        chunk_timeout: Timeout per chunk in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Dict with conversion status\n",
    "    \"\"\"\n",
    "    if not pdf_path.exists():\n",
    "        return {\"converted\": False, \"num_pages\": 0, \"error\": \"Source file not found\"}\n",
    "    \n",
    "    try:\n",
    "        num_pages = get_pdf_page_count(pdf_path)\n",
    "        if num_pages == 0:\n",
    "            return {\"converted\": False, \"num_pages\": 0, \"error\": \"Empty or unreadable PDF\"}\n",
    "        \n",
    "        # Create temp directory for processing\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_path = Path(temp_dir)\n",
    "            \n",
    "            # Decide whether to split\n",
    "            if num_pages <= pages_per_chunk:\n",
    "                # Small file - process directly\n",
    "                chunks = [pdf_path]\n",
    "                chunk_dirs = [temp_path / \"output\"]\n",
    "                chunk_dirs[0].mkdir()\n",
    "            else:\n",
    "                # Large file - split into chunks\n",
    "                split_dir = temp_path / \"splits\"\n",
    "                split_dir.mkdir()\n",
    "                chunks = split_pdf(pdf_path, split_dir, pages_per_chunk)\n",
    "                chunk_dirs = [temp_path / f\"output_{i}\" for i in range(len(chunks))]\n",
    "                for d in chunk_dirs:\n",
    "                    d.mkdir()\n",
    "            \n",
    "            # Process each chunk\n",
    "            markdown_parts = []\n",
    "            for i, (chunk_path, chunk_output_dir) in enumerate(zip(chunks, chunk_dirs)):\n",
    "                success, content, error = run_marker_on_pdf(chunk_path, chunk_output_dir, chunk_timeout)\n",
    "                \n",
    "                if not success:\n",
    "                    return {\n",
    "                        \"converted\": False, \n",
    "                        \"num_pages\": num_pages, \n",
    "                        \"error\": f\"Chunk {i+1}/{len(chunks)} failed: {error}\"\n",
    "                    }\n",
    "                \n",
    "                markdown_parts.append(content)\n",
    "            \n",
    "            # Combine chunks\n",
    "            if len(markdown_parts) > 1:\n",
    "                combined_markdown = \"\\n\\n---\\n\\n\".join(markdown_parts)\n",
    "            else:\n",
    "                combined_markdown = markdown_parts[0]\n",
    "            \n",
    "            # Remove images/figures\n",
    "            clean_markdown = remove_images_from_markdown(combined_markdown)\n",
    "            \n",
    "            # Write output\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(clean_markdown)\n",
    "            \n",
    "            return {\"converted\": True, \"num_pages\": num_pages, \"error\": None}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"converted\": False, \"num_pages\": 0, \"error\": str(e)[:500]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Load Document Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_records():\n",
    "    \"\"\"Load document records from the API metadata.\"\"\"\n",
    "    if DOC_RECORD_FILE.exists():\n",
    "        return pd.read_parquet(DOC_RECORD_FILE)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Document records not found at {DOC_RECORD_FILE}.\\n\"\n",
    "            f\"Run fetch_eis_records_api.ipynb first.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_failed_files() -> set:\n",
    "    \"\"\"Load set of attachment IDs that previously failed.\"\"\"\n",
    "    if FAILED_FILES_LOG.exists():\n",
    "        df = pd.read_csv(FAILED_FILES_LOG)\n",
    "        return set(df['attachmentId'].astype(str))\n",
    "    return set()\n",
    "\n",
    "\n",
    "def append_failed_file(attachment_id, ceq_number, source_file, error):\n",
    "    \"\"\"Append a failed file to the log.\"\"\"\n",
    "    file_exists = FAILED_FILES_LOG.exists()\n",
    "    with open(FAILED_FILES_LOG, 'a', encoding='utf-8') as f:\n",
    "        if not file_exists:\n",
    "            f.write(\"attachmentId,ceqNumber,source_file,error,timestamp\\n\")\n",
    "        error_clean = str(error).replace('\"', \"'\").replace('\\n', ' ')[:200]\n",
    "        f.write(f'{attachment_id},{ceq_number},\"{source_file}\",\"{error_clean}\",{datetime.now().isoformat()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45704 document records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ceqNumber</th>\n",
       "      <th>year</th>\n",
       "      <th>localFilename</th>\n",
       "      <th>sourcePath</th>\n",
       "      <th>outputPath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_LoMo_FRR_Comprehensive_Study_Draft_Re...</td>\n",
       "      <td>/Users/tscott1/Documents/GitHub/eis_documents/...</td>\n",
       "      <td>/Users/tscott1/Documents/GitHub/eis_documents/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_LoMo_System_Plan_-_Basis_of_Estimate.pdf</td>\n",
       "      <td>/Users/tscott1/Documents/GitHub/eis_documents/...</td>\n",
       "      <td>/Users/tscott1/Documents/GitHub/eis_documents/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_Appendix_A.1_LoMo_FRM_Past_Performanc...</td>\n",
       "      <td>/Users/tscott1/Documents/GitHub/eis_documents/...</td>\n",
       "      <td>/Users/tscott1/Documents/GitHub/eis_documents/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_Appendix_A.2.1_LoMo_RAS_Calibration_O...</td>\n",
       "      <td>/Users/tscott1/Documents/GitHub/eis_documents/...</td>\n",
       "      <td>/Users/tscott1/Documents/GitHub/eis_documents/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20250186</td>\n",
       "      <td>2025</td>\n",
       "      <td>20250186_Appendix_A.2.2_LoMo_RAS_Calibration_K...</td>\n",
       "      <td>/Users/tscott1/Documents/GitHub/eis_documents/...</td>\n",
       "      <td>/Users/tscott1/Documents/GitHub/eis_documents/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ceqNumber  year                                      localFilename  \\\n",
       "0  20250186  2025  20250186_LoMo_FRR_Comprehensive_Study_Draft_Re...   \n",
       "1  20250186  2025  20250186_LoMo_System_Plan_-_Basis_of_Estimate.pdf   \n",
       "2  20250186  2025  20250186_Appendix_A.1_LoMo_FRM_Past_Performanc...   \n",
       "3  20250186  2025  20250186_Appendix_A.2.1_LoMo_RAS_Calibration_O...   \n",
       "4  20250186  2025  20250186_Appendix_A.2.2_LoMo_RAS_Calibration_K...   \n",
       "\n",
       "                                          sourcePath  \\\n",
       "0  /Users/tscott1/Documents/GitHub/eis_documents/...   \n",
       "1  /Users/tscott1/Documents/GitHub/eis_documents/...   \n",
       "2  /Users/tscott1/Documents/GitHub/eis_documents/...   \n",
       "3  /Users/tscott1/Documents/GitHub/eis_documents/...   \n",
       "4  /Users/tscott1/Documents/GitHub/eis_documents/...   \n",
       "\n",
       "                                          outputPath  \n",
       "0  /Users/tscott1/Documents/GitHub/eis_documents/...  \n",
       "1  /Users/tscott1/Documents/GitHub/eis_documents/...  \n",
       "2  /Users/tscott1/Documents/GitHub/eis_documents/...  \n",
       "3  /Users/tscott1/Documents/GitHub/eis_documents/...  \n",
       "4  /Users/tscott1/Documents/GitHub/eis_documents/...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load document records\n",
    "doc_df = load_document_records()\n",
    "print(f\"Loaded {len(doc_df)} document records\")\n",
    "\n",
    "# Add helper columns\n",
    "doc_df['year'] = doc_df['ceqNumber'].astype(str).str[:4]\n",
    "doc_df['localFilename'] = doc_df.apply(\n",
    "    lambda row: build_local_filename(\n",
    "        row['ceqNumber'], \n",
    "        row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Build source and output paths\n",
    "doc_df['sourcePath'] = doc_df.apply(\n",
    "    lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n",
    "    axis=1\n",
    ")\n",
    "doc_df['outputPath'] = doc_df.apply(\n",
    "    lambda row: OUTPUT_DIR / row['year'] / (row['localFilename'].replace('.pdf', '.md').replace('.PDF', '.md')),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "display(doc_df[['ceqNumber', 'year', 'localFilename', 'sourcePath', 'outputPath']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c4210-e4ac-44c8-b193-14965bce8e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files found: 0 / 45704\n",
      "Documents available for conversion: 0\n",
      "\n",
      "By year:\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check which source files exist\n",
    "doc_df['sourceExists'] = doc_df['sourcePath'].apply(lambda p: p.exists())\n",
    "print(f\"Source files found: {doc_df['sourceExists'].sum()} / {len(doc_df)}\")\n",
    "\n",
    "# Documents with source files available for conversion\n",
    "to_convert = doc_df[doc_df['sourceExists']].copy()\n",
    "print(f\"Documents available for conversion: {len(to_convert)}\")\n",
    "\n",
    "print(f\"\\nBy year:\")\n",
    "print(to_convert['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Conversion Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "  YEAR_FILTER: [2019]\n",
      "  MAX_CONVERSIONS: None\n",
      "  RETRY_FAILURES: True\n",
      "  PAGES_PER_CHUNK: 10\n",
      "  CHUNK_TIMEOUT_SECONDS: 300\n",
      "  PDF_TIMEOUT_SECONDS: 1800\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONVERSION SETTINGS - MODIFY AS NEEDED\n",
    "# ============================================\n",
    "\n",
    "# Filter by year (set to None to convert all years)\n",
    "YEAR_FILTER = None\n",
    "YEAR_FILTER = [2019]  # Example: specific year\n",
    "\n",
    "# Maximum number of files to convert (set to None for all)\n",
    "MAX_CONVERSIONS = None\n",
    "#MAX_CONVERSIONS = 100  # Start with a small test batch\n",
    "\n",
    "# Set to True to retry previously failed files\n",
    "RETRY_FAILURES = True\n",
    "\n",
    "# Pages per chunk for splitting large PDFs\n",
    "PAGES_PER_CHUNK = 10\n",
    "\n",
    "# Timeout per chunk in seconds (marker is slower than pymupdf)\n",
    "CHUNK_TIMEOUT_SECONDS = 300  # 5 minutes per 50 pages\n",
    "\n",
    "# Overall timeout per PDF (for the multiprocessing wrapper)\n",
    "PDF_TIMEOUT_SECONDS = 1800  # 30 minutes max per PDF\n",
    "\n",
    "print(f\"Settings:\")\n",
    "print(f\"  YEAR_FILTER: {YEAR_FILTER}\")\n",
    "print(f\"  MAX_CONVERSIONS: {MAX_CONVERSIONS}\")\n",
    "print(f\"  RETRY_FAILURES: {RETRY_FAILURES}\")\n",
    "print(f\"  PAGES_PER_CHUNK: {PAGES_PER_CHUNK}\")\n",
    "print(f\"  CHUNK_TIMEOUT_SECONDS: {CHUNK_TIMEOUT_SECONDS}\")\n",
    "print(f\"  PDF_TIMEOUT_SECONDS: {PDF_TIMEOUT_SECONDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to years [2019]: 0 documents\n",
      "Already converted (md exists): 0\n",
      "\n",
      "Final conversion queue: 0 files\n"
     ]
    }
   ],
   "source": [
    "# Build conversion queue\n",
    "conversion_queue = to_convert.copy()\n",
    "\n",
    "# Apply year filter\n",
    "if YEAR_FILTER:\n",
    "    year_filter_str = [str(y) for y in YEAR_FILTER]\n",
    "    conversion_queue = conversion_queue[conversion_queue['year'].isin(year_filter_str)]\n",
    "    print(f\"Filtered to years {YEAR_FILTER}: {len(conversion_queue)} documents\")\n",
    "\n",
    "# Skip files that already have .md output\n",
    "conversion_queue['outputExists'] = conversion_queue['outputPath'].apply(lambda p: p.exists())\n",
    "existing_count = conversion_queue['outputExists'].sum()\n",
    "print(f\"Already converted (md exists): {existing_count}\")\n",
    "conversion_queue = conversion_queue[~conversion_queue['outputExists']]\n",
    "\n",
    "# Skip previously failed files (unless RETRY_FAILURES is True)\n",
    "if not RETRY_FAILURES:\n",
    "    failed_ids = load_failed_files()\n",
    "    if failed_ids:\n",
    "        conversion_queue['previouslyFailed'] = conversion_queue['attachmentId'].astype(str).isin(failed_ids)\n",
    "        failed_count = conversion_queue['previouslyFailed'].sum()\n",
    "        print(f\"Previously failed (skipping): {failed_count}\")\n",
    "        conversion_queue = conversion_queue[~conversion_queue['previouslyFailed']]\n",
    "\n",
    "# Apply max conversions limit\n",
    "if MAX_CONVERSIONS and len(conversion_queue) > MAX_CONVERSIONS:\n",
    "    conversion_queue = conversion_queue.head(MAX_CONVERSIONS)\n",
    "    print(f\"Limited to {MAX_CONVERSIONS} conversions\")\n",
    "\n",
    "print(f\"\\nFinal conversion queue: {len(conversion_queue)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Create Output Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/tscott1/Documents/GitHub/eis_documents/enepa_repository/documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m OUTPUT_DIR.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Get all year directories from documents\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m year_dirs = [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mDOCUMENTS_DIR\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m d.is_dir() \u001b[38;5;129;01mand\u001b[39;00m d.name.isdigit()]\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(year_dirs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m year directories in documents/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Create corresponding directories in marker_conversions/\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/pathlib/_local.py:575\u001b[39m, in \u001b[36mPath.iterdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Yield path objects of the directory contents.\u001b[39;00m\n\u001b[32m    570\u001b[39m \n\u001b[32m    571\u001b[39m \u001b[33;03mThe children are yielded in arbitrary order, and the\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[33;03mspecial entries '.' and '..' are not included.\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    574\u001b[39m root_dir = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m scandir_it:\n\u001b[32m    576\u001b[39m     paths = [entry.path \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m scandir_it]\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m root_dir == \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/tscott1/Documents/GitHub/eis_documents/enepa_repository/documents'"
     ]
    }
   ],
   "source": [
    "# Create output directory structure mirroring documents/\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get all year directories from documents\n",
    "year_dirs = [d for d in DOCUMENTS_DIR.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "print(f\"Found {len(year_dirs)} year directories in documents/\")\n",
    "\n",
    "# Create corresponding directories in marker_conversions/\n",
    "for year_dir in year_dirs:\n",
    "    output_year_dir = OUTPUT_DIR / year_dir.name\n",
    "    output_year_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created directory structure in {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Run Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_in_process(args: tuple, result_queue, pages_per_chunk: int, chunk_timeout: int):\n",
    "    \"\"\"Worker function that runs marker conversion in a separate process.\"\"\"\n",
    "    pdf_path, output_path, ceq_number, attachment_id = args\n",
    "    pdf_path = Path(pdf_path)\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    try:\n",
    "        result = convert_pdf_with_marker(\n",
    "            pdf_path, output_path,\n",
    "            pages_per_chunk=pages_per_chunk,\n",
    "            chunk_timeout=chunk_timeout\n",
    "        )\n",
    "        result_queue.put(result)\n",
    "    except Exception as e:\n",
    "        result_queue.put({\"converted\": False, \"num_pages\": 0, \"error\": str(e)[:500]})\n",
    "\n",
    "\n",
    "def convert_with_timeout(args: tuple, timeout_seconds: int, \n",
    "                         pages_per_chunk: int, chunk_timeout: int) -> dict:\n",
    "    \"\"\"Run conversion in a subprocess that can be killed on timeout.\"\"\"\n",
    "    result_queue = _mp_ctx.Queue()\n",
    "    proc = _mp_ctx.Process(\n",
    "        target=_convert_in_process, \n",
    "        args=(args, result_queue, pages_per_chunk, chunk_timeout)\n",
    "    )\n",
    "    proc.start()\n",
    "    proc.join(timeout=timeout_seconds)\n",
    "    \n",
    "    if proc.is_alive():\n",
    "        proc.terminate()\n",
    "        proc.join(timeout=5)\n",
    "        if proc.is_alive():\n",
    "            proc.kill()\n",
    "            proc.join()\n",
    "        return {\"converted\": False, \"num_pages\": 0, \"error\": f\"TIMEOUT after {timeout_seconds}s\"}\n",
    "    \n",
    "    try:\n",
    "        return result_queue.get_nowait()\n",
    "    except:\n",
    "        return {\"converted\": False, \"num_pages\": 0, \"error\": \"Process ended without result\"}\n",
    "\n",
    "\n",
    "def run_conversions(queue: pd.DataFrame, timeout_seconds: int = 1800,\n",
    "                    pages_per_chunk: int = 50, chunk_timeout: int = 300) -> dict:\n",
    "    \"\"\"Run marker extraction with hard timeouts that kill hung processes.\"\"\"\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    total_pages = 0\n",
    "    \n",
    "    pbar = tqdm(queue.iterrows(), total=len(queue), desc=\"Converting\")\n",
    "    \n",
    "    for idx, row in pbar:\n",
    "        filename = Path(row['sourcePath']).name\n",
    "        short_name = filename[:40] + \"...\" if len(filename) > 40 else filename\n",
    "        pbar.set_postfix_str(short_name)\n",
    "        \n",
    "        args = (str(row['sourcePath']), str(row['outputPath']), row['ceqNumber'], row['attachmentId'])\n",
    "        \n",
    "        try:\n",
    "            result = convert_with_timeout(args, timeout_seconds, pages_per_chunk, chunk_timeout)\n",
    "            \n",
    "            if result.get('converted'):\n",
    "                success_count += 1\n",
    "                total_pages += result.get('num_pages', 0)\n",
    "            else:\n",
    "                fail_count += 1\n",
    "                append_failed_file(\n",
    "                    row['attachmentId'],\n",
    "                    row['ceqNumber'],\n",
    "                    str(row['sourcePath']),\n",
    "                    result.get('error', 'Unknown error')\n",
    "                )\n",
    "                logger.warning(f\"Failed: {short_name} - {result.get('error', '')[:60]}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            fail_count += 1\n",
    "            append_failed_file(\n",
    "                row['attachmentId'],\n",
    "                row['ceqNumber'],\n",
    "                str(row['sourcePath']),\n",
    "                f\"EXCEPTION: {str(e)[:200]}\"\n",
    "            )\n",
    "            logger.error(f\"Exception: {short_name} - {str(e)[:60]}\")\n",
    "    \n",
    "    return {\n",
    "        'success': success_count,\n",
    "        'failed': fail_count,\n",
    "        'total_pages': total_pages\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversions\n",
    "if len(conversion_queue) > 0:\n",
    "    print(f\"Starting marker conversion of {len(conversion_queue)} files...\")\n",
    "    print(f\"Timeout per file: {PDF_TIMEOUT_SECONDS}s, per chunk: {CHUNK_TIMEOUT_SECONDS}s\")\n",
    "    print(f\"Pages per chunk: {PAGES_PER_CHUNK}\")\n",
    "    print(f\"Failures logged to: {FAILED_FILES_LOG}\")\n",
    "    print()\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = run_conversions(\n",
    "        conversion_queue, \n",
    "        timeout_seconds=PDF_TIMEOUT_SECONDS,\n",
    "        pages_per_chunk=PAGES_PER_CHUNK,\n",
    "        chunk_timeout=CHUNK_TIMEOUT_SECONDS\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n=== Conversion Summary ===\")\n",
    "    print(f\"Time elapsed: {elapsed/60:.1f} minutes\")\n",
    "    print(f\"Successful: {results['success']}\")\n",
    "    print(f\"Failed: {results['failed']}\")\n",
    "    print(f\"Total pages: {results['total_pages']:,}\")\n",
    "    if elapsed > 0:\n",
    "        print(f\"Rate: {(results['success'] + results['failed'])/elapsed:.2f} docs/sec\")\n",
    "        print(f\"Rate: {results['total_pages']/elapsed:.1f} pages/sec\")\n",
    "    \n",
    "    if results['failed'] > 0:\n",
    "        print(f\"\\nSee {FAILED_FILES_LOG} for failure details\")\n",
    "else:\n",
    "    print(\"No files to convert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Verify Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_conversions():\n",
    "    \"\"\"Quick verification of conversion status.\"\"\"\n",
    "    doc_df = load_document_records()\n",
    "    \n",
    "    # Build paths\n",
    "    doc_df['year'] = doc_df['ceqNumber'].astype(str).str[:4]\n",
    "    doc_df['localFilename'] = doc_df.apply(\n",
    "        lambda row: build_local_filename(\n",
    "            row['ceqNumber'], \n",
    "            row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    doc_df['sourcePath'] = doc_df.apply(\n",
    "        lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n",
    "        axis=1\n",
    "    )\n",
    "    doc_df['outputPath'] = doc_df.apply(\n",
    "        lambda row: OUTPUT_DIR / row['year'] / (row['localFilename'].replace('.pdf', '.md').replace('.PDF', '.md')),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Check status\n",
    "    doc_df['sourceExists'] = doc_df['sourcePath'].apply(lambda p: p.exists())\n",
    "    doc_df['outputExists'] = doc_df['outputPath'].apply(lambda p: p.exists())\n",
    "    \n",
    "    with_source = doc_df[doc_df['sourceExists']]\n",
    "    total = len(with_source)\n",
    "    converted = with_source['outputExists'].sum()\n",
    "    \n",
    "    # Count failures\n",
    "    failed_ids = load_failed_files()\n",
    "    \n",
    "    print(f\"=== Marker Conversion Status ===\")\n",
    "    print(f\"Source PDFs: {total}\")\n",
    "    print(f\"Converted: {converted} ({100*converted/total:.1f}%)\")\n",
    "    print(f\"Failed: {len(failed_ids)}\")\n",
    "    print(f\"Remaining: {total - converted - len(failed_ids)}\")\n",
    "\n",
    "verify_conversions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Sample Output Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample converted file\n",
    "def show_sample_output(n_chars: int = 3000):\n",
    "    \"\"\"Display the beginning of a random converted markdown file.\"\"\"\n",
    "    md_files = list(OUTPUT_DIR.glob(\"**/*.md\"))\n",
    "    \n",
    "    if not md_files:\n",
    "        print(\"No converted files found.\")\n",
    "        return\n",
    "    \n",
    "    import random\n",
    "    sample_file = random.choice(md_files)\n",
    "    \n",
    "    print(f\"File: {sample_file.name}\")\n",
    "    print(f\"Size: {sample_file.stat().st_size:,} bytes\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "        print(f.read(n_chars))\n",
    "\n",
    "# Uncomment to see a sample:\n",
    "# show_sample_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28afb375-9613-4374-85cc-e693896760ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
