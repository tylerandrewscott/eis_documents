{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download EIS Documents via EPA E-NEPA API\n",
    "\n",
    "This notebook downloads EIS documents using the official EPA E-NEPA API, replacing the web scraping approach in `download_eis_files_2024.R`.\n",
    "\n",
    "**Key Features:**\n",
    "- Downloads individual files using `attachmentId` (no need for ZIP bundles)\n",
    "- `overwrite` toggle to skip existing files or re-download everything\n",
    "- Interfaces with existing file structure (`documents/{YEAR}/`)\n",
    "- Maintains naming convention: `{EIS_ID}_{filename}`\n",
    "- Handles symbolic links to Box storage transparently\n",
    "\n",
    "**API Documentation:** https://cdxapps.epa.gov/cdx-enepa-II/apidocs/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install requests pandas tqdm pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository\n",
      "Documents directory: /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/documents\n",
      "Is symlink: True\n",
      "Symlink target: /Users/admin-tascott/Library/CloudStorage/Box-Box/eis_documents/enepa_repository/documents\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_URL = \"https://cdxapps.epa.gov/cdx-enepa-II/rest\"\n",
    "DOWNLOAD_ENDPOINT = f\"{BASE_URL}/public/v1/eis/document/download\"\n",
    "\n",
    "# Paths - relative to repository root\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "METADATA_DIR = REPO_ROOT / \"metadata\"\n",
    "DOCUMENTS_DIR = REPO_ROOT / \"documents\"  # May be a symlink to Box\n",
    "\n",
    "# Input file (from fetch_eis_records_api.ipynb)\n",
    "DOC_RECORD_FILE = METADATA_DIR / \"eis_document_record_api.pkl\"\n",
    "DOC_RECORD_PARQUET = METADATA_DIR / \"eis_document_record_api.parquet\"\n",
    "\n",
    "# Download tracking file\n",
    "DOWNLOAD_STATUS_FILE = METADATA_DIR / \"download_status_api.pkl\"\n",
    "\n",
    "# Rate limiting\n",
    "REQUEST_DELAY = 0.25  # seconds between requests\n",
    "MAX_WORKERS = 4  # parallel downloads (be respectful to the API)\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Documents directory: {DOCUMENTS_DIR}\")\n",
    "print(f\"Is symlink: {DOCUMENTS_DIR.is_symlink()}\")\n",
    "if DOCUMENTS_DIR.is_symlink():\n",
    "    print(f\"Symlink target: {DOCUMENTS_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize filename to match existing convention.\n",
    "    - Remove special characters: ( ) & , ~\n",
    "    - Replace spaces with underscores\n",
    "    - Normalize PDF extension\n",
    "    \n",
    "    Args:\n",
    "        filename: Original filename\n",
    "    \n",
    "    Returns:\n",
    "        Sanitized filename\n",
    "    \"\"\"\n",
    "    # Remove problematic characters\n",
    "    clean = re.sub(r'[()&,~\\/]', '', filename)\n",
    "    # Replace multiple spaces/underscores with single underscore\n",
    "    clean = re.sub(r'[\\s_]+', '_', clean)\n",
    "    # Normalize PDF extension\n",
    "    clean = re.sub(r'\\.PDF$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = re.sub(r'\\.pdf\\.pdf$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    # Remove leading/trailing underscores\n",
    "    clean = clean.strip('_')\n",
    "    return clean\n",
    "\n",
    "\n",
    "def get_year_from_eis_id(ceq_number) -> str:\n",
    "    \"\"\"\n",
    "    Extract year from CEQ Number (first 4 digits).\n",
    "    \n",
    "    Args:\n",
    "        ceq_number: CEQ Number (e.g., 20240001)\n",
    "    \n",
    "    Returns:\n",
    "        Year string (e.g., \"2024\")\n",
    "    \"\"\"\n",
    "    return str(ceq_number)[:4]\n",
    "\n",
    "\n",
    "def build_local_filename(ceq_number, original_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the local filename following existing convention.\n",
    "    Format: {CEQ_NUMBER}_{sanitized_filename}\n",
    "    \n",
    "    Note: This prepends the CEQ NUMBER even if it's already in the filename,\n",
    "    matching the existing behavior.\n",
    "    \n",
    "    Args:\n",
    "        ceq_number: CEQ NUMBER\n",
    "        original_filename: Original filename from API\n",
    "    \n",
    "    Returns:\n",
    "        Local filename\n",
    "    \"\"\"\n",
    "    sanitized = sanitize_filename(original_filename)\n",
    "    return f\"{ceq_number}_{sanitized}\"\n",
    "\n",
    "\n",
    "def get_existing_files(documents_dir: Path) -> set:\n",
    "    \"\"\"\n",
    "    Get set of all existing files (by filename only, not full path).\n",
    "    \n",
    "    Args:\n",
    "        documents_dir: Path to documents directory\n",
    "    \n",
    "    Returns:\n",
    "        Set of existing filenames\n",
    "    \"\"\"\n",
    "    existing = set()\n",
    "    \n",
    "    if not documents_dir.exists():\n",
    "        return existing\n",
    "    \n",
    "    for year_dir in documents_dir.iterdir():\n",
    "        if year_dir.is_dir() and year_dir.name.isdigit():\n",
    "            for file in year_dir.iterdir():\n",
    "                if file.is_file():\n",
    "                    existing.add(file.name)\n",
    "    \n",
    "    return existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_attachment(attachment_id: int, dest_path: Path, max_retries: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Download a single attachment by ID.\n",
    "    \n",
    "    Args:\n",
    "        attachment_id: The attachment ID from the API\n",
    "        dest_path: Full path where file should be saved\n",
    "        max_retries: Number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        Dict with download status: {success, size, error}\n",
    "    \"\"\"\n",
    "    params = {\"attachmentId\": attachment_id}\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                DOWNLOAD_ENDPOINT, \n",
    "                params=params, \n",
    "                timeout=120,\n",
    "                stream=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Ensure parent directory exists\n",
    "            dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Write file\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            file_size = dest_path.stat().st_size\n",
    "            \n",
    "            # Check for empty file\n",
    "            if file_size == 0:\n",
    "                dest_path.unlink()  # Remove empty file\n",
    "                return {\"success\": False, \"size\": 0, \"error\": \"Empty file received\"}\n",
    "            \n",
    "            return {\"success\": True, \"size\": file_size, \"error\": None}\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1}/{max_retries} failed for attachment {attachment_id}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                return {\"success\": False, \"size\": 0, \"error\": str(e)}\n",
    "    \n",
    "    return {\"success\": False, \"size\": 0, \"error\": \"Max retries exceeded\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Document Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_records():\n",
    "    \"\"\"\n",
    "    Load document records from the fetch notebook output.\n",
    "    \"\"\"\n",
    "    if DOC_RECORD_FILE.exists():\n",
    "        return pd.read_pickle(DOC_RECORD_FILE)\n",
    "    elif DOC_RECORD_PARQUET.exists():\n",
    "        return pd.read_parquet(DOC_RECORD_PARQUET)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Document records not found. Run fetch_eis_records_api.ipynb first.\\n\"\n",
    "            f\"Expected: {DOC_RECORD_FILE} or {DOC_RECORD_PARQUET}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_download_status():\n",
    "    \"\"\"\n",
    "    Load existing download status tracking.\n",
    "    \"\"\"\n",
    "    if DOWNLOAD_STATUS_FILE.exists():\n",
    "        return pd.read_pickle(DOWNLOAD_STATUS_FILE)\n",
    "    return pd.DataFrame(columns=['attachmentId', 'ceqNumber', 'filename', 'downloaded', 'size', 'error', 'timestamp'])\n",
    "\n",
    "\n",
    "def save_download_status(status_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Save download status tracking.\n",
    "    \"\"\"\n",
    "    status_df.to_pickle(DOWNLOAD_STATUS_FILE)\n",
    "    status_df.to_csv(METADATA_DIR / \"download_status_api.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45704 document records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eisId</th>\n",
       "      <th>ceqNumber</th>\n",
       "      <th>attachmentId</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>fileNameForDownload</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>sizeKb</th>\n",
       "      <th>pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>531723</td>\n",
       "      <td>20250186</td>\n",
       "      <td>544805</td>\n",
       "      <td>LoMo FRR Comprehensive Study Draft Report.pdf</td>\n",
       "      <td>LoMo FRR Comprehensive Study Draft Report</td>\n",
       "      <td>LoMo FRR Comprehensive Study Draft Report.pdf</td>\n",
       "      <td>EIS_Document</td>\n",
       "      <td>19550084</td>\n",
       "      <td>19092</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>531723</td>\n",
       "      <td>20250186</td>\n",
       "      <td>544810</td>\n",
       "      <td>LoMo System Plan - Basis of Estimate.pdf</td>\n",
       "      <td>LoMo System Plan - Basis of Estimate</td>\n",
       "      <td>LoMo System Plan - Basis of Estimate.pdf</td>\n",
       "      <td>EIS_Document</td>\n",
       "      <td>2235648</td>\n",
       "      <td>2184</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>531723</td>\n",
       "      <td>20250186</td>\n",
       "      <td>544815</td>\n",
       "      <td>Appendix A.1 LoMo FRM Past Performance Assessm...</td>\n",
       "      <td>Appendix A.1 LoMo FRM Past Performance Assessment</td>\n",
       "      <td>Appendix A.1 LoMo FRM Past Performance Assessm...</td>\n",
       "      <td>EIS_Document</td>\n",
       "      <td>4515420</td>\n",
       "      <td>4410</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>531723</td>\n",
       "      <td>20250186</td>\n",
       "      <td>544820</td>\n",
       "      <td>Appendix A.2.1 LoMo RAS Calibration Omaha Dist...</td>\n",
       "      <td>Appendix A.2.1 LoMo RAS Calibration Omaha Dist...</td>\n",
       "      <td>Appendix A.2.1 LoMo RAS Calibration Omaha Dist...</td>\n",
       "      <td>EIS_Document</td>\n",
       "      <td>21973806</td>\n",
       "      <td>21459</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>531723</td>\n",
       "      <td>20250186</td>\n",
       "      <td>545125</td>\n",
       "      <td>Appendix A.2.2 LoMo RAS Calibration Kansas Cit...</td>\n",
       "      <td>Appendix A.2.2 LoMo RAS Calibration Kansas Cit...</td>\n",
       "      <td>Appendix A.2.2 LoMo RAS Calibration Kansas Cit...</td>\n",
       "      <td>EIS_Document</td>\n",
       "      <td>13582918</td>\n",
       "      <td>13265</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eisId ceqNumber  attachmentId  \\\n",
       "0  531723  20250186        544805   \n",
       "1  531723  20250186        544810   \n",
       "2  531723  20250186        544815   \n",
       "3  531723  20250186        544820   \n",
       "4  531723  20250186        545125   \n",
       "\n",
       "                                                name  \\\n",
       "0      LoMo FRR Comprehensive Study Draft Report.pdf   \n",
       "1           LoMo System Plan - Basis of Estimate.pdf   \n",
       "2  Appendix A.1 LoMo FRM Past Performance Assessm...   \n",
       "3  Appendix A.2.1 LoMo RAS Calibration Omaha Dist...   \n",
       "4  Appendix A.2.2 LoMo RAS Calibration Kansas Cit...   \n",
       "\n",
       "                                               title  \\\n",
       "0          LoMo FRR Comprehensive Study Draft Report   \n",
       "1               LoMo System Plan - Basis of Estimate   \n",
       "2  Appendix A.1 LoMo FRM Past Performance Assessment   \n",
       "3  Appendix A.2.1 LoMo RAS Calibration Omaha Dist...   \n",
       "4  Appendix A.2.2 LoMo RAS Calibration Kansas Cit...   \n",
       "\n",
       "                                 fileNameForDownload          type      size  \\\n",
       "0      LoMo FRR Comprehensive Study Draft Report.pdf  EIS_Document  19550084   \n",
       "1           LoMo System Plan - Basis of Estimate.pdf  EIS_Document   2235648   \n",
       "2  Appendix A.1 LoMo FRM Past Performance Assessm...  EIS_Document   4515420   \n",
       "3  Appendix A.2.1 LoMo RAS Calibration Omaha Dist...  EIS_Document  21973806   \n",
       "4  Appendix A.2.2 LoMo RAS Calibration Kansas Cit...  EIS_Document  13582918   \n",
       "\n",
       "  sizeKb  pages  \n",
       "0  19092  383.0  \n",
       "1   2184   48.0  \n",
       "2   4410  124.0  \n",
       "3  21459  162.0  \n",
       "4  13265  126.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load document records\n",
    "doc_df = load_document_records()\n",
    "print(f\"Loaded {len(doc_df)} document records\")\n",
    "display(doc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents by year:\n",
      "year\n",
      "1987       2\n",
      "1988       1\n",
      "1990       2\n",
      "1991       4\n",
      "1992       3\n",
      "1993       3\n",
      "1994      10\n",
      "1995      20\n",
      "1996      59\n",
      "1997      82\n",
      "1998      58\n",
      "1999     202\n",
      "2000     369\n",
      "2001     481\n",
      "2002     596\n",
      "2003     562\n",
      "2004     631\n",
      "2005     755\n",
      "2006     712\n",
      "2007     912\n",
      "2008     911\n",
      "2009     894\n",
      "2010     661\n",
      "2011     406\n",
      "2012     991\n",
      "2013    3007\n",
      "2014    2918\n",
      "2015    4104\n",
      "2016    3683\n",
      "2017    2697\n",
      "2018    3239\n",
      "2019    2865\n",
      "2020    3280\n",
      "2021    2309\n",
      "2022    2206\n",
      "2023    1633\n",
      "2024    2498\n",
      "2025    1938\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Summary by year - use eisId if ceqNumber not available\n",
    "if 'ceqNumber' in doc_df.columns:\n",
    "    doc_df['year'] = doc_df['ceqNumber'].astype(str).str[:4]\n",
    "else:\n",
    "    doc_df['year'] = doc_df['eisId'].astype(str).str[:4]\n",
    "\n",
    "print(\"\\nDocuments by year:\")\n",
    "print(doc_df['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "  OVERWRITE: False\n",
      "  YEAR_FILTER: None\n",
      "  TYPE_FILTER: None\n",
      "  MAX_DOWNLOADS: None\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DOWNLOAD SETTINGS - MODIFY AS NEEDED\n",
    "# ============================================\n",
    "\n",
    "# Set to True to re-download all files, False to skip existing files\n",
    "OVERWRITE = False\n",
    "\n",
    "# Filter by year (set to None to download all years)\n",
    "# Example: YEAR_FILTER = [2023, 2024] to only download 2023-2024\n",
    "YEAR_FILTER = None\n",
    "\n",
    "# Filter by document type (set to None for all types)\n",
    "# Common types: 'pdf', 'PDF', etc.\n",
    "TYPE_FILTER = None  # e.g., ['pdf', 'PDF']\n",
    "\n",
    "# Maximum number of files to download (set to None for all)\n",
    "# Useful for testing\n",
    "MAX_DOWNLOADS = None  # e.g., 100\n",
    "\n",
    "print(f\"Settings:\")\n",
    "print(f\"  OVERWRITE: {OVERWRITE}\")\n",
    "print(f\"  YEAR_FILTER: {YEAR_FILTER}\")\n",
    "print(f\"  TYPE_FILTER: {TYPE_FILTER}\")\n",
    "print(f\"  MAX_DOWNLOADS: {MAX_DOWNLOADS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Download Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_download_queue(doc_df: pd.DataFrame, documents_dir: Path,\n",
    "                           overwrite: bool = False, year_filter: list = None,\n",
    "                           type_filter: list = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the download queue by determining which files need to be downloaded.\n",
    "    \n",
    "    Args:\n",
    "        doc_df: Document records DataFrame\n",
    "        documents_dir: Path to documents directory\n",
    "        overwrite: If True, include all files. If False, skip existing.\n",
    "        year_filter: Optional list of years to include\n",
    "        type_filter: Optional list of file types to include\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with download queue\n",
    "    \"\"\"\n",
    "    queue = doc_df.copy()\n",
    "    \n",
    "    # Add year column\n",
    "    queue['year'] = queue['eisId'].astype(str).str[:4]\n",
    "    \n",
    "    # Apply year filter\n",
    "    if year_filter:\n",
    "        year_filter_str = [str(y) for y in year_filter]\n",
    "        queue = queue[queue['year'].isin(year_filter_str)]\n",
    "        logger.info(f\"Filtered to years {year_filter}: {len(queue)} documents\")\n",
    "    \n",
    "    # Apply type filter\n",
    "    if type_filter:\n",
    "        # Check file extension\n",
    "        queue['extension'] = queue['name'].str.lower().str.split('.').str[-1]\n",
    "        type_filter_lower = [t.lower() for t in type_filter]\n",
    "        queue = queue[queue['extension'].isin(type_filter_lower)]\n",
    "        logger.info(f\"Filtered to types {type_filter}: {len(queue)} documents\")\n",
    "    \n",
    "    # Build local filenames\n",
    "    queue['localFilename'] = queue.apply(\n",
    "        lambda row: build_local_filename(row['eisId'], row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Build full paths\n",
    "    queue['localPath'] = queue.apply(\n",
    "        lambda row: documents_dir / row['year'] / row['localFilename'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Check for existing files\n",
    "    if not overwrite:\n",
    "        queue['exists'] = queue['localPath'].apply(lambda p: p.exists())\n",
    "        existing_count = queue['exists'].sum()\n",
    "        logger.info(f\"Found {existing_count} existing files\")\n",
    "        queue = queue[~queue['exists']]\n",
    "        logger.info(f\"Queue after removing existing: {len(queue)} documents\")\n",
    "    \n",
    "    # Remove any rows with missing attachment IDs\n",
    "    queue = queue[queue['attachmentId'].notna()]\n",
    "    \n",
    "    return queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 21:36:09,584 - INFO - Found 0 existing files\n",
      "2026-01-27 21:36:09,593 - INFO - Queue after removing existing: 45704 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download queue: 45704 files\n",
      "\n",
      "By year:\n",
      "year\n",
      "1377    194\n",
      "1520      3\n",
      "1521     10\n",
      "1523     55\n",
      "1525      6\n",
      "       ... \n",
      "8929    148\n",
      "8930     80\n",
      "8931    341\n",
      "8932    143\n",
      "8933    145\n",
      "Name: count, Length: 3020, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare the download queue\n",
    "download_queue = prepare_download_queue(\n",
    "    doc_df, \n",
    "    DOCUMENTS_DIR, \n",
    "    overwrite=OVERWRITE,\n",
    "    year_filter=YEAR_FILTER,\n",
    "    type_filter=TYPE_FILTER\n",
    ")\n",
    "\n",
    "# Apply max downloads limit\n",
    "if MAX_DOWNLOADS and len(download_queue) > MAX_DOWNLOADS:\n",
    "    download_queue = download_queue.head(MAX_DOWNLOADS)\n",
    "    logger.info(f\"Limited to {MAX_DOWNLOADS} downloads\")\n",
    "\n",
    "print(f\"\\nDownload queue: {len(download_queue)} files\")\n",
    "print(f\"\\nBy year:\")\n",
    "print(download_queue['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(queue: pd.DataFrame, parallel: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download all files in the queue.\n",
    "    \n",
    "    Args:\n",
    "        queue: Download queue DataFrame\n",
    "        parallel: If True, use parallel downloads (be careful with rate limits)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with download status\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    if parallel and MAX_WORKERS > 1:\n",
    "        # Parallel downloads\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = {}\n",
    "            for _, row in queue.iterrows():\n",
    "                future = executor.submit(\n",
    "                    download_attachment,\n",
    "                    int(row['attachmentId']),\n",
    "                    row['localPath']\n",
    "                )\n",
    "                futures[future] = row\n",
    "            \n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading\"):\n",
    "                row = futures[future]\n",
    "                result = future.result()\n",
    "                results.append({\n",
    "                    'attachmentId': row['attachmentId'],\n",
    "                    'eisId': row['eisId'],\n",
    "                    'filename': row['localFilename'],\n",
    "                    'downloaded': result['success'],\n",
    "                    'size': result['size'],\n",
    "                    'error': result['error'],\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "    else:\n",
    "        # Sequential downloads\n",
    "        for _, row in tqdm(queue.iterrows(), total=len(queue), desc=\"Downloading\"):\n",
    "            result = download_attachment(\n",
    "                int(row['attachmentId']),\n",
    "                row['localPath']\n",
    "            )\n",
    "            results.append({\n",
    "                'attachmentId': row['attachmentId'],\n",
    "                'eisId': row['eisId'],\n",
    "                'filename': row['localFilename'],\n",
    "                'downloaded': result['success'],\n",
    "                'size': result['size'],\n",
    "                'error': result['error'],\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the downloads\n",
    "if len(download_queue) > 0:\n",
    "    print(f\"Starting download of {len(download_queue)} files...\")\n",
    "    print(f\"Using {'parallel' if MAX_WORKERS > 1 else 'sequential'} downloads\")\n",
    "    \n",
    "    download_results = download_files(download_queue, parallel=(MAX_WORKERS > 1))\n",
    "    \n",
    "    # Merge with existing status\n",
    "    existing_status = load_download_status()\n",
    "    combined_status = pd.concat([existing_status, download_results], ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates (keep latest)\n",
    "    combined_status = combined_status.drop_duplicates(subset=['attachmentId'], keep='last')\n",
    "    \n",
    "    # Save status\n",
    "    save_download_status(combined_status)\n",
    "    \n",
    "    # Summary\n",
    "    success_count = download_results['downloaded'].sum()\n",
    "    fail_count = len(download_results) - success_count\n",
    "    total_size = download_results['size'].sum()\n",
    "    \n",
    "    print(f\"\\n=== Download Summary ===\")\n",
    "    print(f\"Successful: {success_count}\")\n",
    "    print(f\"Failed: {fail_count}\")\n",
    "    print(f\"Total size: {total_size / (1024**3):.2f} GB\")\n",
    "    \n",
    "    if fail_count > 0:\n",
    "        print(f\"\\nFailed downloads:\")\n",
    "        display(download_results[~download_results['downloaded']][['eisId', 'filename', 'error']])\n",
    "else:\n",
    "    print(\"No files to download. All files already exist or queue is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry Failed Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_failed_downloads():\n",
    "    \"\"\"\n",
    "    Retry any previously failed downloads.\n",
    "    \"\"\"\n",
    "    status_df = load_download_status()\n",
    "    failed = status_df[~status_df['downloaded']]\n",
    "    \n",
    "    if len(failed) == 0:\n",
    "        print(\"No failed downloads to retry.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Retrying {len(failed)} failed downloads...\")\n",
    "    \n",
    "    # Rebuild queue from failed records\n",
    "    doc_df_full = load_document_records()\n",
    "    retry_queue = doc_df_full[doc_df_full['attachmentId'].isin(failed['attachmentId'])].copy()\n",
    "    \n",
    "    # Add required columns\n",
    "    retry_queue['year'] = retry_queue['eisId'].astype(str).str[:4]\n",
    "    retry_queue['localFilename'] = retry_queue.apply(\n",
    "        lambda row: build_local_filename(row['eisId'], row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"),\n",
    "        axis=1\n",
    "    )\n",
    "    retry_queue['localPath'] = retry_queue.apply(\n",
    "        lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Download\n",
    "    retry_results = download_files(retry_queue, parallel=False)\n",
    "    \n",
    "    # Update status\n",
    "    status_df = status_df[~status_df['attachmentId'].isin(retry_results['attachmentId'])]\n",
    "    combined_status = pd.concat([status_df, retry_results], ignore_index=True)\n",
    "    save_download_status(combined_status)\n",
    "    \n",
    "    success_count = retry_results['downloaded'].sum()\n",
    "    print(f\"Retry complete: {success_count}/{len(retry_results)} successful\")\n",
    "\n",
    "# Uncomment to retry failed downloads:\n",
    "# retry_failed_downloads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Downloads Against Existing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_downloads():\n",
    "    \"\"\"\n",
    "    Compare expected documents against existing files.\n",
    "    \"\"\"\n",
    "    doc_df_full = load_document_records()\n",
    "    \n",
    "    # Build expected filenames\n",
    "    doc_df_full['year'] = doc_df_full['eisId'].astype(str).str[:4]\n",
    "    doc_df_full['expectedFilename'] = doc_df_full.apply(\n",
    "        lambda row: build_local_filename(row['eisId'], row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"),\n",
    "        axis=1\n",
    "    )\n",
    "    doc_df_full['expectedPath'] = doc_df_full.apply(\n",
    "        lambda row: DOCUMENTS_DIR / row['year'] / row['expectedFilename'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Check existence\n",
    "    doc_df_full['exists'] = doc_df_full['expectedPath'].apply(lambda p: p.exists())\n",
    "    \n",
    "    total = len(doc_df_full)\n",
    "    existing = doc_df_full['exists'].sum()\n",
    "    missing = total - existing\n",
    "    \n",
    "    print(f\"=== Download Verification ===\")\n",
    "    print(f\"Total expected documents: {total}\")\n",
    "    print(f\"Existing files: {existing} ({100*existing/total:.1f}%)\")\n",
    "    print(f\"Missing files: {missing} ({100*missing/total:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nBy year:\")\n",
    "    summary = doc_df_full.groupby('year').agg(\n",
    "        total=('exists', 'count'),\n",
    "        existing=('exists', 'sum')\n",
    "    )\n",
    "    summary['missing'] = summary['total'] - summary['existing']\n",
    "    summary['pct_complete'] = (100 * summary['existing'] / summary['total']).round(1)\n",
    "    display(summary)\n",
    "    \n",
    "    return doc_df_full\n",
    "\n",
    "verification_df = verify_downloads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility: Download ZIP Bundles (Alternative Method)\n",
    "\n",
    "If individual downloads are slow, you can use the ZIP bundle endpoints instead. This downloads all documents for a project at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_eis_zip(eis_id: int, set_number: int, dest_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Download EIS documents as a ZIP file.\n",
    "    \n",
    "    Args:\n",
    "        eis_id: EIS ID\n",
    "        set_number: Set number (documents are partitioned)\n",
    "        dest_dir: Directory to save ZIP\n",
    "    \n",
    "    Returns:\n",
    "        Dict with download status\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/public/v1/eis/document/download/zip/eis_document\"\n",
    "    params = {\"eisId\": eis_id, \"set\": set_number}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=300, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        dest_path = dest_dir / f\"{eis_id}_set{set_number}.zip\"\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(dest_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        return {\"success\": True, \"path\": dest_path, \"error\": None}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"path\": None, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def download_comment_letters_zip(eis_id: str, dest_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Download comment letters as a ZIP file.\n",
    "    \n",
    "    Args:\n",
    "        eis_id: EIS ID (as string)\n",
    "        dest_dir: Directory to save ZIP\n",
    "    \n",
    "    Returns:\n",
    "        Dict with download status\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/public/v1/eis/document/download/zip/comment_letter\"\n",
    "    params = {\"eisId\": eis_id}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=300, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        dest_path = dest_dir / f\"{eis_id}_comment_letters.zip\"\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(dest_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        return {\"success\": True, \"path\": dest_path, \"error\": None}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"path\": None, \"error\": str(e)}\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# result = download_eis_zip(20240001, 1, DOCUMENTS_DIR / \"zips\")\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
