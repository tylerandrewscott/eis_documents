{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Convert EIS Documents to Markdown using Marker\n",
    "\n",
    "This notebook converts PDF documents to markdown text using the `marker` library, which provides high-quality extraction with proper handling of:\n",
    "- Document structure (headings, lists)\n",
    "- Tables\n",
    "- Equations\n",
    "- Multi-column layouts\n",
    "\n",
    "**Key Features:**\n",
    "- Mirrors directory structure from `documents/` to `marker_conversions/`\n",
    "- Disables image extraction (excludes maps, figures, photos)\n",
    "- Automatically detects and applies OCR for scanned documents\n",
    "- Tracks conversion progress to allow resuming\n",
    "\n",
    "This replaces the text extraction previously done in `make_filter_text_tables.R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install marker-pdf pandas pyarrow tqdm pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import fitz  # pymupdf - for OCR detection\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "METADATA_DIR = REPO_ROOT / \"metadata\"\n",
    "DOCUMENTS_DIR = REPO_ROOT / \"documents\"  # Source PDFs (may be symlink to Box)\n",
    "OUTPUT_DIR = REPO_ROOT / \"marker_conversions\"  # Output markdown files\n",
    "\n",
    "# Input metadata file\n",
    "DOC_RECORD_FILE = METADATA_DIR / \"eis_document_record_api.parquet\"\n",
    "\n",
    "# Conversion tracking file\n",
    "CONVERSION_STATUS_FILE = METADATA_DIR / \"marker_conversion_status.pkl\"\n",
    "\n",
    "# OCR detection threshold: if extracted text has fewer chars per page than this, likely scanned\n",
    "OCR_THRESHOLD_CHARS_PER_PAGE = 100\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Documents directory: {DOCUMENTS_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Is documents symlink: {DOCUMENTS_DIR.is_symlink()}\")\n",
    "if DOCUMENTS_DIR.is_symlink():\n",
    "    print(f\"Symlink target: {DOCUMENTS_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize filename to match existing convention.\n",
    "    - Remove special characters: ( ) & , ~\n",
    "    - Replace spaces with underscores\n",
    "    - Normalize PDF extension\n",
    "    \"\"\"\n",
    "    clean = re.sub(r'[()&,~\\/]', '', filename)\n",
    "    clean = re.sub(r'[\\s_]+', '_', clean)\n",
    "    clean = re.sub(r'\\.PDF$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = re.sub(r'\\.pdf\\.pdf$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = clean.strip('_')\n",
    "    return clean\n",
    "\n",
    "\n",
    "def build_local_filename(ceq_number, original_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the local filename following existing convention.\n",
    "    Format: {CEQ_NUMBER}_{sanitized_filename}\n",
    "    \"\"\"\n",
    "    sanitized = sanitize_filename(original_filename)\n",
    "    return f\"{ceq_number}_{sanitized}\"\n",
    "\n",
    "\n",
    "def get_year_from_ceq(ceq_number) -> str:\n",
    "    \"\"\"Extract year from CEQ Number (first 4 digits).\"\"\"\n",
    "    return str(ceq_number)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def needs_ocr(pdf_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a PDF needs OCR by attempting to extract text.\n",
    "    \n",
    "    Returns True if:\n",
    "    - Text extraction yields very little text (likely scanned)\n",
    "    - The PDF contains mostly images\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "    \n",
    "    Returns:\n",
    "        True if OCR is needed, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_chars = 0\n",
    "        num_pages = len(doc)\n",
    "        \n",
    "        if num_pages == 0:\n",
    "            doc.close()\n",
    "            return True\n",
    "        \n",
    "        # Sample up to 10 pages to check for text\n",
    "        pages_to_check = min(num_pages, 10)\n",
    "        for i in range(pages_to_check):\n",
    "            page = doc[i]\n",
    "            text = page.get_text()\n",
    "            # Count non-whitespace characters\n",
    "            total_chars += len(text.strip())\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # Calculate average chars per page\n",
    "        avg_chars_per_page = total_chars / pages_to_check\n",
    "        \n",
    "        # If very little text, likely needs OCR\n",
    "        return avg_chars_per_page < OCR_THRESHOLD_CHARS_PER_PAGE\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error checking OCR need for {pdf_path}: {e}\")\n",
    "        # If we can't check, assume OCR might be needed\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_with_marker(pdf_path: Path, output_path: Path, force_ocr: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a PDF to markdown using marker.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to input PDF\n",
    "        output_path: Path for output markdown file\n",
    "        force_ocr: If True, force OCR processing\n",
    "    \n",
    "    Returns:\n",
    "        Dict with conversion status: {success, ocr_used, error, output_file}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Build marker command\n",
    "        # marker outputs to a directory, creating {filename}/{filename}.md\n",
    "        # We'll use a temp approach and rename\n",
    "        temp_output_dir = output_path.parent / f\"_temp_{output_path.stem}\"\n",
    "        \n",
    "        cmd = [\n",
    "            \"marker_single\",\n",
    "            str(pdf_path),\n",
    "            str(temp_output_dir),\n",
    "            \"--disable_image_extraction\"\n",
    "        ]\n",
    "        \n",
    "        if force_ocr:\n",
    "            cmd.append(\"--force_ocr\")\n",
    "        \n",
    "        # Run marker\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=600  # 10 minute timeout per file\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"ocr_used\": force_ocr,\n",
    "                \"error\": result.stderr[:500] if result.stderr else \"Unknown error\",\n",
    "                \"output_file\": None\n",
    "            }\n",
    "        \n",
    "        # Find the output markdown file\n",
    "        md_files = list(temp_output_dir.glob(\"**/*.md\"))\n",
    "        if not md_files:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"ocr_used\": force_ocr,\n",
    "                \"error\": \"No markdown file generated\",\n",
    "                \"output_file\": None\n",
    "            }\n",
    "        \n",
    "        # Move the markdown file to final location\n",
    "        source_md = md_files[0]\n",
    "        source_md.rename(output_path)\n",
    "        \n",
    "        # Clean up temp directory\n",
    "        import shutil\n",
    "        if temp_output_dir.exists():\n",
    "            shutil.rmtree(temp_output_dir)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"ocr_used\": force_ocr,\n",
    "            \"error\": None,\n",
    "            \"output_file\": str(output_path)\n",
    "        }\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"ocr_used\": force_ocr,\n",
    "            \"error\": \"Timeout (>10 minutes)\",\n",
    "            \"output_file\": None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"ocr_used\": force_ocr,\n",
    "            \"error\": str(e)[:500],\n",
    "            \"output_file\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Load Document Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_records():\n",
    "    \"\"\"Load document records from the API metadata.\"\"\"\n",
    "    if DOC_RECORD_FILE.exists():\n",
    "        return pd.read_parquet(DOC_RECORD_FILE)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Document records not found at {DOC_RECORD_FILE}.\\n\"\n",
    "            f\"Run fetch_eis_records_api.ipynb first.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_conversion_status():\n",
    "    \"\"\"Load existing conversion status tracking.\"\"\"\n",
    "    if CONVERSION_STATUS_FILE.exists():\n",
    "        return pd.read_pickle(CONVERSION_STATUS_FILE)\n",
    "    return pd.DataFrame(columns=[\n",
    "        'ceqNumber', 'attachmentId', 'source_file', 'output_file',\n",
    "        'converted', 'ocr_used', 'error', 'timestamp'\n",
    "    ])\n",
    "\n",
    "\n",
    "def save_conversion_status(status_df: pd.DataFrame):\n",
    "    \"\"\"Save conversion status tracking.\"\"\"\n",
    "    status_df.to_pickle(CONVERSION_STATUS_FILE)\n",
    "    # Also save CSV for easy inspection\n",
    "    status_df.to_csv(METADATA_DIR / \"marker_conversion_status.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document records\n",
    "doc_df = load_document_records()\n",
    "print(f\"Loaded {len(doc_df)} document records\")\n",
    "\n",
    "# Add helper columns\n",
    "doc_df['year'] = doc_df['ceqNumber'].astype(str).str[:4]\n",
    "doc_df['localFilename'] = doc_df.apply(\n",
    "    lambda row: build_local_filename(\n",
    "        row['ceqNumber'], \n",
    "        row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Build source and output paths\n",
    "doc_df['sourcePath'] = doc_df.apply(\n",
    "    lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n",
    "    axis=1\n",
    ")\n",
    "doc_df['outputPath'] = doc_df.apply(\n",
    "    lambda row: OUTPUT_DIR / row['year'] / row['localFilename'].replace('.pdf', '.md').replace('.PDF', '.md'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "display(doc_df[['ceqNumber', 'year', 'localFilename', 'sourcePath', 'outputPath']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which source files exist\n",
    "doc_df['sourceExists'] = doc_df['sourcePath'].apply(lambda p: p.exists())\n",
    "print(f\"Source files found: {doc_df['sourceExists'].sum()} / {len(doc_df)}\")\n",
    "\n",
    "# Check which have already been converted\n",
    "doc_df['alreadyConverted'] = doc_df['outputPath'].apply(lambda p: p.exists())\n",
    "print(f\"Already converted: {doc_df['alreadyConverted'].sum()}\")\n",
    "\n",
    "# Documents needing conversion\n",
    "to_convert = doc_df[doc_df['sourceExists'] & ~doc_df['alreadyConverted']].copy()\n",
    "print(f\"\\nDocuments to convert: {len(to_convert)}\")\n",
    "\n",
    "print(f\"\\nBy year:\")\n",
    "print(to_convert['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Conversion Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONVERSION SETTINGS - MODIFY AS NEEDED\n",
    "# ============================================\n",
    "\n",
    "# Filter by year (set to None to convert all years)\n",
    "YEAR_FILTER = None\n",
    "# YEAR_FILTER = [2024, 2025]  # Example: only recent years\n",
    "\n",
    "# Maximum number of files to convert (set to None for all)\n",
    "# Useful for testing\n",
    "MAX_CONVERSIONS = None\n",
    "# MAX_CONVERSIONS = 10  # Example: test with 10 files\n",
    "\n",
    "# Skip OCR detection and always/never force OCR\n",
    "# None = auto-detect, True = always OCR, False = never OCR\n",
    "FORCE_OCR_MODE = None\n",
    "\n",
    "print(f\"Settings:\")\n",
    "print(f\"  YEAR_FILTER: {YEAR_FILTER}\")\n",
    "print(f\"  MAX_CONVERSIONS: {MAX_CONVERSIONS}\")\n",
    "print(f\"  FORCE_OCR_MODE: {FORCE_OCR_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filters to conversion queue\n",
    "conversion_queue = to_convert.copy()\n",
    "\n",
    "if YEAR_FILTER:\n",
    "    year_filter_str = [str(y) for y in YEAR_FILTER]\n",
    "    conversion_queue = conversion_queue[conversion_queue['year'].isin(year_filter_str)]\n",
    "    print(f\"Filtered to years {YEAR_FILTER}: {len(conversion_queue)} documents\")\n",
    "\n",
    "if MAX_CONVERSIONS and len(conversion_queue) > MAX_CONVERSIONS:\n",
    "    conversion_queue = conversion_queue.head(MAX_CONVERSIONS)\n",
    "    print(f\"Limited to {MAX_CONVERSIONS} conversions\")\n",
    "\n",
    "print(f\"\\nFinal conversion queue: {len(conversion_queue)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Create Output Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory structure mirroring documents/\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get all year directories from documents\n",
    "year_dirs = [d for d in DOCUMENTS_DIR.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "print(f\"Found {len(year_dirs)} year directories in documents/\")\n",
    "\n",
    "# Create corresponding directories in marker_conversions/\n",
    "for year_dir in year_dirs:\n",
    "    output_year_dir = OUTPUT_DIR / year_dir.name\n",
    "    output_year_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created directory structure in {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Run Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversions(queue: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run marker conversions on all files in the queue.\n",
    "    \n",
    "    Args:\n",
    "        queue: DataFrame with sourcePath and outputPath columns\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with conversion results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(queue.iterrows(), total=len(queue), desc=\"Converting\"):\n",
    "        source_path = row['sourcePath']\n",
    "        output_path = row['outputPath']\n",
    "        \n",
    "        # Determine if OCR is needed\n",
    "        if FORCE_OCR_MODE is None:\n",
    "            use_ocr = needs_ocr(source_path)\n",
    "            if use_ocr:\n",
    "                logger.info(f\"OCR needed for {source_path.name}\")\n",
    "        else:\n",
    "            use_ocr = FORCE_OCR_MODE\n",
    "        \n",
    "        # Run conversion\n",
    "        result = convert_pdf_with_marker(source_path, output_path, force_ocr=use_ocr)\n",
    "        \n",
    "        results.append({\n",
    "            'ceqNumber': row['ceqNumber'],\n",
    "            'attachmentId': row['attachmentId'],\n",
    "            'source_file': str(source_path),\n",
    "            'output_file': result['output_file'],\n",
    "            'converted': result['success'],\n",
    "            'ocr_used': result['ocr_used'],\n",
    "            'error': result['error'],\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Save progress periodically (every 50 files)\n",
    "        if len(results) % 50 == 0:\n",
    "            temp_results = pd.DataFrame(results)\n",
    "            existing_status = load_conversion_status()\n",
    "            combined = pd.concat([existing_status, temp_results], ignore_index=True)\n",
    "            combined = combined.drop_duplicates(subset=['attachmentId'], keep='last')\n",
    "            save_conversion_status(combined)\n",
    "            logger.info(f\"Progress saved: {len(results)} files processed\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversions\n",
    "if len(conversion_queue) > 0:\n",
    "    print(f\"Starting conversion of {len(conversion_queue)} files...\")\n",
    "    print(f\"This may take a while. Progress is saved every 50 files.\")\n",
    "    \n",
    "    conversion_results = run_conversions(conversion_queue)\n",
    "    \n",
    "    # Merge with existing status\n",
    "    existing_status = load_conversion_status()\n",
    "    combined_status = pd.concat([existing_status, conversion_results], ignore_index=True)\n",
    "    combined_status = combined_status.drop_duplicates(subset=['attachmentId'], keep='last')\n",
    "    save_conversion_status(combined_status)\n",
    "    \n",
    "    # Summary\n",
    "    success_count = conversion_results['converted'].sum()\n",
    "    fail_count = len(conversion_results) - success_count\n",
    "    ocr_count = conversion_results['ocr_used'].sum()\n",
    "    \n",
    "    print(f\"\\n=== Conversion Summary ===\")\n",
    "    print(f\"Successful: {success_count}\")\n",
    "    print(f\"Failed: {fail_count}\")\n",
    "    print(f\"Used OCR: {ocr_count}\")\n",
    "    \n",
    "    if fail_count > 0:\n",
    "        print(f\"\\nFailed conversions:\")\n",
    "        display(conversion_results[~conversion_results['converted']][['ceqNumber', 'source_file', 'error']])\n",
    "else:\n",
    "    print(\"No files to convert. All files already converted or queue is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Verify Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_conversions():\n",
    "    \"\"\"\n",
    "    Compare expected conversions against existing output files.\n",
    "    \"\"\"\n",
    "    doc_df_full = load_document_records()\n",
    "    \n",
    "    # Build expected output paths\n",
    "    doc_df_full['year'] = doc_df_full['ceqNumber'].astype(str).str[:4]\n",
    "    doc_df_full['localFilename'] = doc_df_full.apply(\n",
    "        lambda row: build_local_filename(\n",
    "            row['ceqNumber'], \n",
    "            row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    doc_df_full['sourcePath'] = doc_df_full.apply(\n",
    "        lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n",
    "        axis=1\n",
    "    )\n",
    "    doc_df_full['outputPath'] = doc_df_full.apply(\n",
    "        lambda row: OUTPUT_DIR / row['year'] / row['localFilename'].replace('.pdf', '.md').replace('.PDF', '.md'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Check existence\n",
    "    doc_df_full['sourceExists'] = doc_df_full['sourcePath'].apply(lambda p: p.exists())\n",
    "    doc_df_full['outputExists'] = doc_df_full['outputPath'].apply(lambda p: p.exists())\n",
    "    \n",
    "    # Only count documents where source exists\n",
    "    with_source = doc_df_full[doc_df_full['sourceExists']]\n",
    "    \n",
    "    total = len(with_source)\n",
    "    converted = with_source['outputExists'].sum()\n",
    "    remaining = total - converted\n",
    "    \n",
    "    print(f\"=== Conversion Verification ===\")\n",
    "    print(f\"Total source documents: {total}\")\n",
    "    print(f\"Converted: {converted} ({100*converted/total:.1f}%)\")\n",
    "    print(f\"Remaining: {remaining} ({100*remaining/total:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nBy year:\")\n",
    "    summary = with_source.groupby('year').agg(\n",
    "        total=('outputExists', 'count'),\n",
    "        converted=('outputExists', 'sum')\n",
    "    )\n",
    "    summary['remaining'] = summary['total'] - summary['converted']\n",
    "    summary['pct_complete'] = (100 * summary['converted'] / summary['total']).round(1)\n",
    "    display(summary)\n",
    "    \n",
    "    return doc_df_full\n",
    "\n",
    "verification_df = verify_conversions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Retry Failed Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_failed_conversions():\n",
    "    \"\"\"\n",
    "    Retry any previously failed conversions.\n",
    "    \"\"\"\n",
    "    status_df = load_conversion_status()\n",
    "    failed = status_df[~status_df['converted']]\n",
    "    \n",
    "    if len(failed) == 0:\n",
    "        print(\"No failed conversions to retry.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Retrying {len(failed)} failed conversions...\")\n",
    "    \n",
    "    # Rebuild queue from failed records\n",
    "    doc_df_full = load_document_records()\n",
    "    retry_queue = doc_df_full[doc_df_full['attachmentId'].isin(failed['attachmentId'])].copy()\n",
    "    \n",
    "    # Add required columns\n",
    "    retry_queue['year'] = retry_queue['ceqNumber'].astype(str).str[:4]\n",
    "    retry_queue['localFilename'] = retry_queue.apply(\n",
    "        lambda row: build_local_filename(\n",
    "            row['ceqNumber'], \n",
    "            row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    retry_queue['sourcePath'] = retry_queue.apply(\n",
    "        lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n",
    "        axis=1\n",
    "    )\n",
    "    retry_queue['outputPath'] = retry_queue.apply(\n",
    "        lambda row: OUTPUT_DIR / row['year'] / row['localFilename'].replace('.pdf', '.md').replace('.PDF', '.md'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Run conversions\n",
    "    retry_results = run_conversions(retry_queue)\n",
    "    \n",
    "    # Update status\n",
    "    status_df = status_df[~status_df['attachmentId'].isin(retry_results['attachmentId'])]\n",
    "    combined_status = pd.concat([status_df, retry_results], ignore_index=True)\n",
    "    save_conversion_status(combined_status)\n",
    "    \n",
    "    success_count = retry_results['converted'].sum()\n",
    "    print(f\"Retry complete: {success_count}/{len(retry_results)} successful\")\n",
    "\n",
    "# Uncomment to retry failed conversions:\n",
    "# retry_failed_conversions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Sample Output Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample converted file\n",
    "def show_sample_output(n_chars: int = 2000):\n",
    "    \"\"\"\n",
    "    Display the beginning of a sample converted markdown file.\n",
    "    \"\"\"\n",
    "    md_files = list(OUTPUT_DIR.glob(\"**/*.md\"))\n",
    "    \n",
    "    if not md_files:\n",
    "        print(\"No converted files found yet.\")\n",
    "        return\n",
    "    \n",
    "    # Pick a random file\n",
    "    import random\n",
    "    sample_file = random.choice(md_files)\n",
    "    \n",
    "    print(f\"Sample file: {sample_file}\")\n",
    "    print(f\"File size: {sample_file.stat().st_size:,} bytes\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read(n_chars)\n",
    "    \n",
    "    print(content)\n",
    "    if len(content) == n_chars:\n",
    "        print(f\"\\n... [truncated at {n_chars} chars]\")\n",
    "\n",
    "# Uncomment to see a sample:\n",
    "# show_sample_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
