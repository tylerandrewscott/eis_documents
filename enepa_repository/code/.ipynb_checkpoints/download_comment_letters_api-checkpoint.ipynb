{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download EPA Comment Letters via API\n",
    "\n",
    "This notebook downloads EPA comment letters using the E-NEPA API.\n",
    "\n",
    "**Key Features:**\n",
    "- Downloads individual comment letter files using `attachmentId`\n",
    "- Also supports bulk ZIP download per project via `/public/v1/eis/document/download/zip/comment_letter`\n",
    "- `OVERWRITE` toggle to skip existing files or re-download\n",
    "- Stores all files in flat directory: `documents/comment_letters/`\n",
    "- Maintains naming convention: `{EIS_ID}_{filename}`\n",
    "\n",
    "**API Documentation:** https://cdxapps.epa.gov/cdx-enepa-II/apidocs/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install requests pandas tqdm pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import io\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_URL = \"https://cdxapps.epa.gov/cdx-enepa-II/rest\"\n",
    "DOWNLOAD_ENDPOINT = f\"{BASE_URL}/public/v1/eis/document/download\"\n",
    "ZIP_COMMENT_ENDPOINT = f\"{BASE_URL}/public/v1/eis/document/download/zip/comment_letter\"\n",
    "\n",
    "# Paths - relative to repository root\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "METADATA_DIR = REPO_ROOT / \"metadata\"\n",
    "DOCUMENTS_DIR = REPO_ROOT / \"documents\"\n",
    "COMMENT_LETTERS_DIR = DOCUMENTS_DIR / \"comment_letters\"  # Flat directory for all comment letters\n",
    "\n",
    "# Input file (from fetch_comment_letters_api.ipynb)\n",
    "COMMENT_LETTER_PKL = METADATA_DIR / \"comment_letter_record_api.pkl\"\n",
    "COMMENT_LETTER_PARQUET = METADATA_DIR / \"comment_letter_record_api.parquet\"\n",
    "\n",
    "# Alternative: use main document records\n",
    "DOC_RECORD_PKL = METADATA_DIR / \"eis_document_record_api.pkl\"\n",
    "\n",
    "# Download tracking file\n",
    "DOWNLOAD_STATUS_FILE = METADATA_DIR / \"comment_letter_download_status.pkl\"\n",
    "\n",
    "# Rate limiting\n",
    "REQUEST_DELAY = 0.25\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Comment letters directory: {COMMENT_LETTERS_DIR}\")\n",
    "print(f\"Documents directory is symlink: {DOCUMENTS_DIR.is_symlink()}\")\n",
    "if DOCUMENTS_DIR.is_symlink():\n",
    "    print(f\"Symlink target: {DOCUMENTS_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DOWNLOAD SETTINGS - MODIFY AS NEEDED\n",
    "# ============================================\n",
    "\n",
    "# Set to True to re-download all files, False to skip existing files\n",
    "OVERWRITE = False\n",
    "\n",
    "# Download method:\n",
    "# - \"individual\": Download each file separately (better for tracking, resumable)\n",
    "# - \"zip\": Download all comment letters for each project as ZIP (faster for bulk)\n",
    "DOWNLOAD_METHOD = \"individual\"\n",
    "\n",
    "# Filter by year (set to None to download all years)\n",
    "# Example: YEAR_FILTER = [2023, 2024]\n",
    "YEAR_FILTER = None\n",
    "\n",
    "# Maximum number of files to download (set to None for all)\n",
    "# Useful for testing\n",
    "MAX_DOWNLOADS = None  # e.g., 100\n",
    "\n",
    "print(f\"=== Download Configuration ===\")\n",
    "print(f\"  OVERWRITE: {OVERWRITE}\")\n",
    "print(f\"  DOWNLOAD_METHOD: {DOWNLOAD_METHOD}\")\n",
    "print(f\"  YEAR_FILTER: {YEAR_FILTER}\")\n",
    "print(f\"  MAX_DOWNLOADS: {MAX_DOWNLOADS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize filename to match existing convention.\n",
    "    \"\"\"\n",
    "    # Remove problematic characters\n",
    "    clean = re.sub(r'[()&,~\\/]', '', filename)\n",
    "    # Replace multiple spaces/underscores with single underscore\n",
    "    clean = re.sub(r'[\\s_]+', '_', clean)\n",
    "    # Normalize PDF extension\n",
    "    clean = re.sub(r'\\.PDF$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = re.sub(r'\\.pdf\\.pdf$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    # Remove leading/trailing underscores\n",
    "    clean = clean.strip('_')\n",
    "    return clean\n",
    "\n",
    "\n",
    "def build_local_filename(eis_id, original_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the local filename: {EIS_ID}_{sanitized_filename}\n",
    "    \"\"\"\n",
    "    sanitized = sanitize_filename(original_filename)\n",
    "    return f\"{eis_id}_{sanitized}\"\n",
    "\n",
    "\n",
    "def get_existing_files() -> set:\n",
    "    \"\"\"\n",
    "    Get set of existing files in comment_letters directory.\n",
    "    \"\"\"\n",
    "    existing = set()\n",
    "    if COMMENT_LETTERS_DIR.exists():\n",
    "        for file in COMMENT_LETTERS_DIR.iterdir():\n",
    "            if file.is_file():\n",
    "                existing.add(file.name)\n",
    "    return existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_attachment(attachment_id: int, dest_path: Path, max_retries: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Download a single attachment by ID.\n",
    "    \"\"\"\n",
    "    params = {\"attachmentId\": attachment_id}\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                DOWNLOAD_ENDPOINT, \n",
    "                params=params, \n",
    "                timeout=120,\n",
    "                stream=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Ensure parent directory exists\n",
    "            dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Write file\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            file_size = dest_path.stat().st_size\n",
    "            \n",
    "            if file_size == 0:\n",
    "                dest_path.unlink()\n",
    "                return {\"success\": False, \"size\": 0, \"error\": \"Empty file received\"}\n",
    "            \n",
    "            return {\"success\": True, \"size\": file_size, \"error\": None}\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1}/{max_retries} failed for attachment {attachment_id}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return {\"success\": False, \"size\": 0, \"error\": str(e)}\n",
    "    \n",
    "    return {\"success\": False, \"size\": 0, \"error\": \"Max retries exceeded\"}\n",
    "\n",
    "\n",
    "def download_comment_letters_zip(eis_id: str, dest_dir: Path, max_retries: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Download all comment letters for a project as ZIP and extract.\n",
    "    \n",
    "    Args:\n",
    "        eis_id: EIS ID (as string)\n",
    "        dest_dir: Directory to extract files to\n",
    "        max_retries: Number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        Dict with download status and list of extracted files\n",
    "    \"\"\"\n",
    "    params = {\"eisId\": eis_id}\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                ZIP_COMMENT_ENDPOINT,\n",
    "                params=params,\n",
    "                timeout=300,\n",
    "                stream=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Read ZIP into memory\n",
    "            zip_content = io.BytesIO()\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                zip_content.write(chunk)\n",
    "            zip_content.seek(0)\n",
    "            \n",
    "            # Extract files\n",
    "            dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "            extracted_files = []\n",
    "            \n",
    "            with zipfile.ZipFile(zip_content) as zf:\n",
    "                for member in zf.namelist():\n",
    "                    if member.endswith('/'):  # Skip directories\n",
    "                        continue\n",
    "                    \n",
    "                    # Build output filename with EIS ID prefix\n",
    "                    original_name = Path(member).name\n",
    "                    local_name = build_local_filename(eis_id, original_name)\n",
    "                    dest_path = dest_dir / local_name\n",
    "                    \n",
    "                    # Extract\n",
    "                    with zf.open(member) as src, open(dest_path, 'wb') as dst:\n",
    "                        dst.write(src.read())\n",
    "                    \n",
    "                    extracted_files.append({\n",
    "                        'filename': local_name,\n",
    "                        'size': dest_path.stat().st_size\n",
    "                    })\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"files\": extracted_files,\n",
    "                \"error\": None\n",
    "            }\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1}/{max_retries} failed for EIS {eis_id}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return {\"success\": False, \"files\": [], \"error\": str(e)}\n",
    "        except zipfile.BadZipFile as e:\n",
    "            return {\"success\": False, \"files\": [], \"error\": f\"Invalid ZIP: {e}\"}\n",
    "    \n",
    "    return {\"success\": False, \"files\": [], \"error\": \"Max retries exceeded\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Comment Letter Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comment_letter_records():\n",
    "    \"\"\"\n",
    "    Load comment letter records.\n",
    "    First tries dedicated comment letter file, then falls back to main doc records.\n",
    "    \"\"\"\n",
    "    # Try dedicated comment letter file first\n",
    "    if COMMENT_LETTER_PKL.exists():\n",
    "        return pd.read_pickle(COMMENT_LETTER_PKL)\n",
    "    elif COMMENT_LETTER_PARQUET.exists():\n",
    "        return pd.read_parquet(COMMENT_LETTER_PARQUET)\n",
    "    \n",
    "    # Fall back to main document records\n",
    "    if DOC_RECORD_PKL.exists():\n",
    "        doc_df = pd.read_pickle(DOC_RECORD_PKL)\n",
    "        # Filter to comment letters\n",
    "        return doc_df[doc_df['type'] == 'Comment_Letter'].copy()\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"Comment letter records not found.\\n\"\n",
    "        f\"Run fetch_comment_letters_api.ipynb or fetch_eis_records_api.ipynb first.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_download_status():\n",
    "    \"\"\"\n",
    "    Load existing download status.\n",
    "    \"\"\"\n",
    "    if DOWNLOAD_STATUS_FILE.exists():\n",
    "        return pd.read_pickle(DOWNLOAD_STATUS_FILE)\n",
    "    return pd.DataFrame(columns=['attachmentId', 'eisId', 'filename', 'downloaded', 'size', 'error', 'timestamp'])\n",
    "\n",
    "\n",
    "def save_download_status(status_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Save download status.\n",
    "    \"\"\"\n",
    "    status_df.to_pickle(DOWNLOAD_STATUS_FILE)\n",
    "    status_df.to_csv(METADATA_DIR / \"comment_letter_download_status.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load comment letter records\ncomment_df = load_comment_letter_records()\nprint(f\"Loaded {len(comment_df)} comment letter records\")\n\n# Add year column - use ceqNumber (format: YYYYNNNN) for year extraction\ncomment_df['year'] = comment_df['ceqNumber'].astype(str).str[:4]\n\nprint(f\"\\nComment letters by year:\")\nprint(comment_df['year'].value_counts().sort_index())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Download Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_download_queue(df: pd.DataFrame, overwrite: bool = False, \n                           year_filter: list = None) -> pd.DataFrame:\n    \"\"\"\n    Prepare the download queue.\n    \"\"\"\n    queue = df.copy()\n    \n    # Ensure year column exists - use ceqNumber for year extraction\n    if 'year' not in queue.columns:\n        queue['year'] = queue['ceqNumber'].astype(str).str[:4]\n    \n    # Apply year filter\n    if year_filter:\n        year_filter_str = [str(y) for y in year_filter]\n        queue = queue[queue['year'].isin(year_filter_str)]\n        logger.info(f\"Filtered to years {year_filter}: {len(queue)} letters\")\n    \n    # Build local filenames using ceqNumber as prefix (matches existing convention)\n    queue['localFilename'] = queue.apply(\n        lambda row: build_local_filename(\n            row['ceqNumber'], \n            row.get('name') or row.get('fileNameForDownload') or f\"{row['attachmentId']}.pdf\"\n        ),\n        axis=1\n    )\n    \n    # Build full paths (flat directory, not by year)\n    queue['localPath'] = queue['localFilename'].apply(lambda f: COMMENT_LETTERS_DIR / f)\n    \n    # Check for existing files\n    if not overwrite:\n        queue['exists'] = queue['localPath'].apply(lambda p: p.exists())\n        existing_count = queue['exists'].sum()\n        logger.info(f\"Found {existing_count} existing files\")\n        queue = queue[~queue['exists']]\n        logger.info(f\"Queue after removing existing: {len(queue)} letters\")\n    \n    # Remove rows with missing attachment IDs\n    queue = queue[queue['attachmentId'].notna()]\n    \n    return queue"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare download queue\n",
    "download_queue = prepare_download_queue(\n",
    "    comment_df,\n",
    "    overwrite=OVERWRITE,\n",
    "    year_filter=YEAR_FILTER\n",
    ")\n",
    "\n",
    "# Apply max downloads limit\n",
    "if MAX_DOWNLOADS and len(download_queue) > MAX_DOWNLOADS:\n",
    "    download_queue = download_queue.head(MAX_DOWNLOADS)\n",
    "    logger.info(f\"Limited to {MAX_DOWNLOADS} downloads\")\n",
    "\n",
    "print(f\"\\nDownload queue: {len(download_queue)} files\")\n",
    "if len(download_queue) > 0:\n",
    "    print(f\"\\nBy year:\")\n",
    "    print(download_queue['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_individual(queue: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download files individually using attachmentId.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    COMMENT_LETTERS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for _, row in tqdm(queue.iterrows(), total=len(queue), desc=\"Downloading\"):\n",
    "        result = download_attachment(\n",
    "            int(row['attachmentId']),\n",
    "            row['localPath']\n",
    "        )\n",
    "        results.append({\n",
    "            'attachmentId': row['attachmentId'],\n",
    "            'eisId': row['eisId'],\n",
    "            'filename': row['localFilename'],\n",
    "            'downloaded': result['success'],\n",
    "            'size': result['size'],\n",
    "            'error': result['error'],\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def download_via_zip(queue: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download files using ZIP endpoint (one ZIP per project).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get unique EIS IDs\n",
    "    unique_eis_ids = queue['eisId'].unique()\n",
    "    logger.info(f\"Downloading ZIPs for {len(unique_eis_ids)} projects\")\n",
    "    \n",
    "    for eis_id in tqdm(unique_eis_ids, desc=\"Downloading ZIPs\"):\n",
    "        result = download_comment_letters_zip(str(eis_id), COMMENT_LETTERS_DIR)\n",
    "        \n",
    "        if result['success']:\n",
    "            for file_info in result['files']:\n",
    "                results.append({\n",
    "                    'attachmentId': None,  # Not tracked in ZIP method\n",
    "                    'eisId': eis_id,\n",
    "                    'filename': file_info['filename'],\n",
    "                    'downloaded': True,\n",
    "                    'size': file_info['size'],\n",
    "                    'error': None,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        else:\n",
    "            # Record failure for all expected files from this EIS\n",
    "            eis_files = queue[queue['eisId'] == eis_id]\n",
    "            for _, row in eis_files.iterrows():\n",
    "                results.append({\n",
    "                    'attachmentId': row['attachmentId'],\n",
    "                    'eisId': eis_id,\n",
    "                    'filename': row['localFilename'],\n",
    "                    'downloaded': False,\n",
    "                    'size': 0,\n",
    "                    'error': result['error'],\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        time.sleep(REQUEST_DELAY)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run downloads\n",
    "if len(download_queue) > 0:\n",
    "    print(f\"Starting download of {len(download_queue)} comment letters...\")\n",
    "    print(f\"Method: {DOWNLOAD_METHOD}\")\n",
    "    print(f\"Destination: {COMMENT_LETTERS_DIR}\")\n",
    "    \n",
    "    if DOWNLOAD_METHOD == \"zip\":\n",
    "        download_results = download_via_zip(download_queue)\n",
    "    else:\n",
    "        download_results = download_individual(download_queue)\n",
    "    \n",
    "    # Merge with existing status\n",
    "    existing_status = load_download_status()\n",
    "    combined_status = pd.concat([existing_status, download_results], ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates (keep latest)\n",
    "    if 'attachmentId' in combined_status.columns:\n",
    "        combined_status = combined_status.drop_duplicates(subset=['filename'], keep='last')\n",
    "    \n",
    "    # Save status\n",
    "    save_download_status(combined_status)\n",
    "    \n",
    "    # Summary\n",
    "    success_count = download_results['downloaded'].sum()\n",
    "    fail_count = len(download_results) - success_count\n",
    "    total_size = download_results['size'].sum()\n",
    "    \n",
    "    print(f\"\\n=== Download Summary ===\")\n",
    "    print(f\"Successful: {success_count}\")\n",
    "    print(f\"Failed: {fail_count}\")\n",
    "    print(f\"Total size: {total_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    if fail_count > 0:\n",
    "        print(f\"\\nFailed downloads:\")\n",
    "        display(download_results[~download_results['downloaded']][['eisId', 'filename', 'error']])\n",
    "else:\n",
    "    print(\"No files to download. All comment letters already exist or queue is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def verify_downloads():\n    \"\"\"\n    Verify downloaded comment letters against expected records.\n    \"\"\"\n    comment_df_full = load_comment_letter_records()\n    \n    # Build expected filenames using ceqNumber as prefix\n    comment_df_full['expectedFilename'] = comment_df_full.apply(\n        lambda row: build_local_filename(\n            row['ceqNumber'],\n            row.get('name') or row.get('fileNameForDownload') or f\"{row['attachmentId']}.pdf\"\n        ),\n        axis=1\n    )\n    comment_df_full['expectedPath'] = comment_df_full['expectedFilename'].apply(\n        lambda f: COMMENT_LETTERS_DIR / f\n    )\n    \n    # Check existence\n    comment_df_full['exists'] = comment_df_full['expectedPath'].apply(lambda p: p.exists())\n    \n    total = len(comment_df_full)\n    existing = comment_df_full['exists'].sum()\n    missing = total - existing\n    \n    print(f\"=== Comment Letter Verification ===\")\n    print(f\"Total expected: {total}\")\n    print(f\"Existing files: {existing} ({100*existing/total:.1f}%)\")\n    print(f\"Missing files: {missing} ({100*missing/total:.1f}%)\")\n    \n    # By year - use ceqNumber for year extraction\n    comment_df_full['year'] = comment_df_full['ceqNumber'].astype(str).str[:4]\n    print(f\"\\nBy year:\")\n    summary = comment_df_full.groupby('year').agg(\n        total=('exists', 'count'),\n        existing=('exists', 'sum')\n    )\n    summary['missing'] = summary['total'] - summary['existing']\n    summary['pct_complete'] = (100 * summary['existing'] / summary['total']).round(1)\n    display(summary)\n    \n    return comment_df_full\n\nverification_df = verify_downloads()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry Failed Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def retry_failed_downloads():\n    \"\"\"\n    Retry any previously failed downloads.\n    \"\"\"\n    status_df = load_download_status()\n    failed = status_df[~status_df['downloaded']]\n    \n    if len(failed) == 0:\n        print(\"No failed downloads to retry.\")\n        return\n    \n    print(f\"Retrying {len(failed)} failed downloads...\")\n    \n    # Rebuild queue\n    comment_df_full = load_comment_letter_records()\n    \n    if 'attachmentId' in failed.columns and failed['attachmentId'].notna().any():\n        retry_queue = comment_df_full[comment_df_full['attachmentId'].isin(failed['attachmentId'])].copy()\n    else:\n        # Match by eisId + filename pattern\n        retry_queue = comment_df_full[comment_df_full['eisId'].isin(failed['eisId'])].copy()\n    \n    # Build filenames using ceqNumber as prefix\n    retry_queue['localFilename'] = retry_queue.apply(\n        lambda row: build_local_filename(\n            row['ceqNumber'],\n            row.get('name') or row.get('fileNameForDownload') or f\"{row['attachmentId']}.pdf\"\n        ),\n        axis=1\n    )\n    retry_queue['localPath'] = retry_queue['localFilename'].apply(lambda f: COMMENT_LETTERS_DIR / f)\n    \n    # Download\n    retry_results = download_individual(retry_queue)\n    \n    # Update status\n    status_df = status_df[~status_df['filename'].isin(retry_results['filename'])]\n    combined_status = pd.concat([status_df, retry_results], ignore_index=True)\n    save_download_status(combined_status)\n    \n    success_count = retry_results['downloaded'].sum()\n    print(f\"Retry complete: {success_count}/{len(retry_results)} successful\")\n\n# Uncomment to retry failed downloads:\n# retry_failed_downloads()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Downloaded Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in comment_letters directory\n",
    "if COMMENT_LETTERS_DIR.exists():\n",
    "    files = list(COMMENT_LETTERS_DIR.iterdir())\n",
    "    pdf_files = [f for f in files if f.suffix.lower() == '.pdf']\n",
    "    \n",
    "    print(f\"Files in {COMMENT_LETTERS_DIR}:\")\n",
    "    print(f\"  Total files: {len(files)}\")\n",
    "    print(f\"  PDF files: {len(pdf_files)}\")\n",
    "    \n",
    "    if pdf_files:\n",
    "        total_size = sum(f.stat().st_size for f in pdf_files)\n",
    "        print(f\"  Total size: {total_size / (1024**2):.2f} MB\")\n",
    "        \n",
    "        print(f\"\\nSample files:\")\n",
    "        for f in sorted(pdf_files)[:10]:\n",
    "            print(f\"  {f.name} ({f.stat().st_size / 1024:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"Directory does not exist yet: {COMMENT_LETTERS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}