{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Convert EIS Documents to Text using PyMuPDF\n\nThis notebook converts PDF documents to text using `pymupdf` (fitz), which provides fast extraction suitable for large document corpora.\n\n**Key Features:**\n- Mirrors directory structure from `documents/` to `text_conversions/`\n- Very fast: processes hundreds of pages per second\n- Handles both digital and scanned PDFs (with OCR fallback if needed)\n- Tracks conversion progress to allow resuming\n- Parallel processing support\n\n**Output format:** Plain text files (`.txt`) with page breaks indicated.\n\nThis replaces the text extraction previously done in `make_filter_text_tables.R`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages if needed\n# !pip install pymupdf pandas pyarrow tqdm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport re\nimport pandas as pd\nimport fitz  # pymupdf\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport logging\nfrom datetime import datetime\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport multiprocessing\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nREPO_ROOT = Path(\"../\").resolve()\nMETADATA_DIR = REPO_ROOT / \"metadata\"\nDOCUMENTS_DIR = REPO_ROOT / \"documents\"  # Source PDFs (may be symlink to Box)\nOUTPUT_DIR = REPO_ROOT / \"text_conversions\"  # Output text files\n\n# Input metadata file\nDOC_RECORD_FILE = METADATA_DIR / \"eis_document_record_api.parquet\"\n\n# Conversion tracking file\nCONVERSION_STATUS_FILE = METADATA_DIR / \"text_conversion_status.pkl\"\n\n# Parallel processing settings\nNUM_WORKERS = min(8, multiprocessing.cpu_count())  # Adjust based on your machine\n\nprint(f\"Repository root: {REPO_ROOT}\")\nprint(f\"Documents directory: {DOCUMENTS_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Parallel workers: {NUM_WORKERS}\")\nprint(f\"Is documents symlink: {DOCUMENTS_DIR.is_symlink()}\")\nif DOCUMENTS_DIR.is_symlink():\n    print(f\"Symlink target: {DOCUMENTS_DIR.resolve()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize filename to match existing convention.\n",
    "    - Remove special characters: ( ) & , ~\n",
    "    - Replace spaces with underscores\n",
    "    - Normalize PDF extension\n",
    "    \"\"\"\n",
    "    clean = re.sub(r'[()&,~\\/]', '', filename)\n",
    "    clean = re.sub(r'[\\s_]+', '_', clean)\n",
    "    clean = re.sub(r'\\.PDF$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = re.sub(r'\\.pdf\\.pdf$', '.pdf', clean, flags=re.IGNORECASE)\n",
    "    clean = clean.strip('_')\n",
    "    return clean\n",
    "\n",
    "\n",
    "def build_local_filename(ceq_number, original_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the local filename following existing convention.\n",
    "    Format: {CEQ_NUMBER}_{sanitized_filename}\n",
    "    \"\"\"\n",
    "    sanitized = sanitize_filename(original_filename)\n",
    "    return f\"{ceq_number}_{sanitized}\"\n",
    "\n",
    "\n",
    "def get_year_from_ceq(ceq_number) -> str:\n",
    "    \"\"\"Extract year from CEQ Number (first 4 digits).\"\"\"\n",
    "    return str(ceq_number)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "def extract_text_from_pdf(pdf_path: Path) -> tuple[str, int, bool]:\n    \"\"\"\n    Extract text from a PDF using pymupdf.\n    \n    Args:\n        pdf_path: Path to the PDF file\n    \n    Returns:\n        Tuple of (extracted_text, num_pages, has_text)\n    \"\"\"\n    try:\n        doc = fitz.open(pdf_path)\n        num_pages = len(doc)\n        \n        if num_pages == 0:\n            doc.close()\n            return \"\", 0, False\n        \n        text_parts = []\n        total_chars = 0\n        \n        for page_num, page in enumerate(doc):\n            page_text = page.get_text()\n            total_chars += len(page_text.strip())\n            \n            # Add page marker and text\n            text_parts.append(f\"\\n\\n--- PAGE {page_num + 1} ---\\n\\n\")\n            text_parts.append(page_text)\n        \n        doc.close()\n        \n        full_text = \"\".join(text_parts)\n        has_text = total_chars > (num_pages * 50)  # At least 50 chars per page average\n        \n        return full_text, num_pages, has_text\n        \n    except Exception as e:\n        return f\"ERROR: {str(e)}\", 0, False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "def convert_single_pdf(args: tuple) -> dict:\n    \"\"\"\n    Convert a single PDF to text. Designed to work with ProcessPoolExecutor.\n    \n    Args:\n        args: Tuple of (pdf_path, output_path, ceq_number, attachment_id)\n    \n    Returns:\n        Dict with conversion status\n    \"\"\"\n    pdf_path, output_path, ceq_number, attachment_id = args\n    pdf_path = Path(pdf_path)\n    output_path = Path(output_path)\n    \n    try:\n        # Extract text\n        text, num_pages, has_text = extract_text_from_pdf(pdf_path)\n        \n        if text.startswith(\"ERROR:\"):\n            return {\n                \"ceqNumber\": ceq_number,\n                \"attachmentId\": attachment_id,\n                \"source_file\": str(pdf_path),\n                \"output_file\": None,\n                \"converted\": False,\n                \"num_pages\": 0,\n                \"has_text\": False,\n                \"error\": text,\n                \"timestamp\": datetime.now().isoformat()\n            }\n        \n        # Ensure output directory exists\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        # Write text file\n        with open(output_path, 'w', encoding='utf-8') as f:\n            f.write(text)\n        \n        return {\n            \"ceqNumber\": ceq_number,\n            \"attachmentId\": attachment_id,\n            \"source_file\": str(pdf_path),\n            \"output_file\": str(output_path),\n            \"converted\": True,\n            \"num_pages\": num_pages,\n            \"has_text\": has_text,\n            \"error\": None,\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n    except Exception as e:\n        return {\n            \"ceqNumber\": ceq_number,\n            \"attachmentId\": attachment_id,\n            \"source_file\": str(pdf_path),\n            \"output_file\": None,\n            \"converted\": False,\n            \"num_pages\": 0,\n            \"has_text\": False,\n            \"error\": str(e)[:500],\n            \"timestamp\": datetime.now().isoformat()\n        }"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Load Document Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "def load_document_records():\n    \"\"\"Load document records from the API metadata.\"\"\"\n    if DOC_RECORD_FILE.exists():\n        return pd.read_parquet(DOC_RECORD_FILE)\n    else:\n        raise FileNotFoundError(\n            f\"Document records not found at {DOC_RECORD_FILE}.\\n\"\n            f\"Run fetch_eis_records_api.ipynb first.\"\n        )\n\n\ndef load_conversion_status():\n    \"\"\"Load existing conversion status tracking.\"\"\"\n    if CONVERSION_STATUS_FILE.exists():\n        return pd.read_pickle(CONVERSION_STATUS_FILE)\n    return pd.DataFrame(columns=[\n        'ceqNumber', 'attachmentId', 'source_file', 'output_file',\n        'converted', 'num_pages', 'has_text', 'error', 'timestamp'\n    ])\n\n\ndef save_conversion_status(status_df: pd.DataFrame):\n    \"\"\"Save conversion status tracking.\"\"\"\n    status_df.to_pickle(CONVERSION_STATUS_FILE)\n    # Also save CSV for easy inspection\n    status_df.to_csv(METADATA_DIR / \"text_conversion_status.csv\", index=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Load document records\ndoc_df = load_document_records()\nprint(f\"Loaded {len(doc_df)} document records\")\n\n# Add helper columns\ndoc_df['year'] = doc_df['ceqNumber'].astype(str).str[:4]\ndoc_df['localFilename'] = doc_df.apply(\n    lambda row: build_local_filename(\n        row['ceqNumber'], \n        row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n    ),\n    axis=1\n)\n\n# Build source and output paths\ndoc_df['sourcePath'] = doc_df.apply(\n    lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n    axis=1\n)\ndoc_df['outputPath'] = doc_df.apply(\n    lambda row: OUTPUT_DIR / row['year'] / (row['localFilename'].replace('.pdf', '.txt').replace('.PDF', '.txt')),\n    axis=1\n)\n\ndisplay(doc_df[['ceqNumber', 'year', 'localFilename', 'sourcePath', 'outputPath']].head())"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files found: 45559 / 45704\n",
      "Already converted: 0\n",
      "\n",
      "Documents to convert: 45559\n",
      "\n",
      "By year:\n",
      "year\n",
      "1987       2\n",
      "1988       1\n",
      "1990       2\n",
      "1991       4\n",
      "1992       3\n",
      "1993       3\n",
      "1994      10\n",
      "1995      20\n",
      "1996      59\n",
      "1997      82\n",
      "1998      58\n",
      "1999     202\n",
      "2000     369\n",
      "2001     481\n",
      "2002     596\n",
      "2003     562\n",
      "2004     631\n",
      "2005     755\n",
      "2006     712\n",
      "2007     912\n",
      "2008     911\n",
      "2009     894\n",
      "2010     661\n",
      "2011     406\n",
      "2012     991\n",
      "2013    3007\n",
      "2014    2918\n",
      "2015    3991\n",
      "2016    3651\n",
      "2017    2697\n",
      "2018    3239\n",
      "2019    2865\n",
      "2020    3280\n",
      "2021    2309\n",
      "2022    2206\n",
      "2023    1633\n",
      "2024    2498\n",
      "2025    1938\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check which source files exist\n",
    "doc_df['sourceExists'] = doc_df['sourcePath'].apply(lambda p: p.exists())\n",
    "print(f\"Source files found: {doc_df['sourceExists'].sum()} / {len(doc_df)}\")\n",
    "\n",
    "# Check which have already been converted\n",
    "doc_df['alreadyConverted'] = doc_df['outputPath'].apply(lambda p: p.exists())\n",
    "print(f\"Already converted: {doc_df['alreadyConverted'].sum()}\")\n",
    "\n",
    "# Documents needing conversion\n",
    "to_convert = doc_df[doc_df['sourceExists'] & ~doc_df['alreadyConverted']].copy()\n",
    "print(f\"\\nDocuments to convert: {len(to_convert)}\")\n",
    "\n",
    "print(f\"\\nBy year:\")\n",
    "print(to_convert['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Conversion Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CONVERSION SETTINGS - MODIFY AS NEEDED\n# ============================================\n\n# Filter by year (set to None to convert all years)\nYEAR_FILTER = None\n# YEAR_FILTER = [2024, 2025]  # Example: only recent years\n\n# Maximum number of files to convert (set to None for all)\n# Useful for testing\nMAX_CONVERSIONS = None\n# MAX_CONVERSIONS = 100  # Example: test with 100 files\n\nprint(f\"Settings:\")\nprint(f\"  YEAR_FILTER: {YEAR_FILTER}\")\nprint(f\"  MAX_CONVERSIONS: {MAX_CONVERSIONS}\")\nprint(f\"  NUM_WORKERS: {NUM_WORKERS}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to years [2024, 2025]: 4436 documents\n",
      "Limited to 10 conversions\n",
      "\n",
      "Final conversion queue: 10 files\n"
     ]
    }
   ],
   "source": [
    "# Apply filters to conversion queue\n",
    "conversion_queue = to_convert.copy()\n",
    "\n",
    "if YEAR_FILTER:\n",
    "    year_filter_str = [str(y) for y in YEAR_FILTER]\n",
    "    conversion_queue = conversion_queue[conversion_queue['year'].isin(year_filter_str)]\n",
    "    print(f\"Filtered to years {YEAR_FILTER}: {len(conversion_queue)} documents\")\n",
    "\n",
    "if MAX_CONVERSIONS and len(conversion_queue) > MAX_CONVERSIONS:\n",
    "    conversion_queue = conversion_queue.head(MAX_CONVERSIONS)\n",
    "    print(f\"Limited to {MAX_CONVERSIONS} conversions\")\n",
    "\n",
    "print(f\"\\nFinal conversion queue: {len(conversion_queue)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Create Output Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 year directories in documents/\n",
      "Created directory structure in /Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/marker_conversions\n"
     ]
    }
   ],
   "source": [
    "# Create output directory structure mirroring documents/\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get all year directories from documents\n",
    "year_dirs = [d for d in DOCUMENTS_DIR.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "print(f\"Found {len(year_dirs)} year directories in documents/\")\n",
    "\n",
    "# Create corresponding directories in marker_conversions/\n",
    "for year_dir in year_dirs:\n",
    "    output_year_dir = OUTPUT_DIR / year_dir.name\n",
    "    output_year_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created directory structure in {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Run Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "def run_conversions(queue: pd.DataFrame, num_workers: int = NUM_WORKERS) -> pd.DataFrame:\n    \"\"\"\n    Run text extraction on all files in the queue using parallel processing.\n    \n    Args:\n        queue: DataFrame with sourcePath and outputPath columns\n        num_workers: Number of parallel workers\n    \n    Returns:\n        DataFrame with conversion results\n    \"\"\"\n    # Prepare arguments for parallel processing\n    args_list = [\n        (str(row['sourcePath']), str(row['outputPath']), row['ceqNumber'], row['attachmentId'])\n        for _, row in queue.iterrows()\n    ]\n    \n    results = []\n    \n    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n        # Submit all tasks\n        futures = {executor.submit(convert_single_pdf, args): args for args in args_list}\n        \n        # Process completed tasks with progress bar\n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Converting\"):\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as e:\n                args = futures[future]\n                results.append({\n                    \"ceqNumber\": args[2],\n                    \"attachmentId\": args[3],\n                    \"source_file\": args[0],\n                    \"output_file\": None,\n                    \"converted\": False,\n                    \"num_pages\": 0,\n                    \"has_text\": False,\n                    \"error\": str(e)[:500],\n                    \"timestamp\": datetime.now().isoformat()\n                })\n            \n            # Save progress periodically (every 500 files)\n            if len(results) % 500 == 0:\n                temp_results = pd.DataFrame(results)\n                existing_status = load_conversion_status()\n                combined = pd.concat([existing_status, temp_results], ignore_index=True)\n                combined = combined.drop_duplicates(subset=['attachmentId'], keep='last')\n                save_conversion_status(combined)\n                logger.info(f\"Progress saved: {len(results)} files processed\")\n    \n    return pd.DataFrame(results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Run the conversions\nif len(conversion_queue) > 0:\n    print(f\"Starting conversion of {len(conversion_queue)} files with {NUM_WORKERS} workers...\")\n    print(f\"Progress is saved every 500 files.\")\n    \n    import time\n    start_time = time.time()\n    \n    conversion_results = run_conversions(conversion_queue)\n    \n    elapsed = time.time() - start_time\n    \n    # Merge with existing status\n    existing_status = load_conversion_status()\n    combined_status = pd.concat([existing_status, conversion_results], ignore_index=True)\n    combined_status = combined_status.drop_duplicates(subset=['attachmentId'], keep='last')\n    save_conversion_status(combined_status)\n    \n    # Summary\n    success_count = conversion_results['converted'].sum()\n    fail_count = len(conversion_results) - success_count\n    total_pages = conversion_results['num_pages'].sum()\n    low_text_count = (~conversion_results['has_text'] & conversion_results['converted']).sum()\n    \n    print(f\"\\n=== Conversion Summary ===\")\n    print(f\"Time elapsed: {elapsed/60:.1f} minutes\")\n    print(f\"Documents processed: {len(conversion_results)}\")\n    print(f\"Successful: {success_count}\")\n    print(f\"Failed: {fail_count}\")\n    print(f\"Total pages: {total_pages:,}\")\n    print(f\"Low/no text (may need OCR): {low_text_count}\")\n    print(f\"Rate: {len(conversion_results)/elapsed:.1f} docs/sec, {total_pages/elapsed:.1f} pages/sec\")\n    \n    if fail_count > 0:\n        print(f\"\\nFailed conversions:\")\n        display(conversion_results[~conversion_results['converted']][['ceqNumber', 'source_file', 'error']].head(20))\nelse:\n    print(\"No files to convert. All files already converted or queue is empty.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d6e30f1-d77f-47d6-acf1-69fa99624454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/admin-tascott/Documents/GitHub/eis_documents/enepa_repository/documents/2025/20250186_LoMo_FRR_Comprehensive_Study_Draft_Report.pdf'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversion_results['source_file'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Verify Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "def verify_conversions():\n    \"\"\"\n    Compare expected conversions against existing output files.\n    \"\"\"\n    doc_df_full = load_document_records()\n    \n    # Build expected output paths\n    doc_df_full['year'] = doc_df_full['ceqNumber'].astype(str).str[:4]\n    doc_df_full['localFilename'] = doc_df_full.apply(\n        lambda row: build_local_filename(\n            row['ceqNumber'], \n            row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n        ),\n        axis=1\n    )\n    doc_df_full['sourcePath'] = doc_df_full.apply(\n        lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n        axis=1\n    )\n    doc_df_full['outputPath'] = doc_df_full.apply(\n        lambda row: OUTPUT_DIR / row['year'] / (row['localFilename'].replace('.pdf', '.txt').replace('.PDF', '.txt')),\n        axis=1\n    )\n    \n    # Check existence\n    doc_df_full['sourceExists'] = doc_df_full['sourcePath'].apply(lambda p: p.exists())\n    doc_df_full['outputExists'] = doc_df_full['outputPath'].apply(lambda p: p.exists())\n    \n    # Only count documents where source exists\n    with_source = doc_df_full[doc_df_full['sourceExists']]\n    \n    total = len(with_source)\n    converted = with_source['outputExists'].sum()\n    remaining = total - converted\n    \n    print(f\"=== Conversion Verification ===\")\n    print(f\"Total source documents: {total}\")\n    print(f\"Converted: {converted} ({100*converted/total:.1f}%)\")\n    print(f\"Remaining: {remaining} ({100*remaining/total:.1f}%)\")\n    \n    print(f\"\\nBy year:\")\n    summary = with_source.groupby('year').agg(\n        total=('outputExists', 'count'),\n        converted=('outputExists', 'sum')\n    )\n    summary['remaining'] = summary['total'] - summary['converted']\n    summary['pct_complete'] = (100 * summary['converted'] / summary['total']).round(1)\n    display(summary)\n    \n    return doc_df_full\n\nverification_df = verify_conversions()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Retry Failed Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "def retry_failed_conversions():\n    \"\"\"\n    Retry any previously failed conversions.\n    \"\"\"\n    status_df = load_conversion_status()\n    failed = status_df[~status_df['converted']]\n    \n    if len(failed) == 0:\n        print(\"No failed conversions to retry.\")\n        return\n    \n    print(f\"Retrying {len(failed)} failed conversions...\")\n    \n    # Rebuild queue from failed records\n    doc_df_full = load_document_records()\n    retry_queue = doc_df_full[doc_df_full['attachmentId'].isin(failed['attachmentId'])].copy()\n    \n    # Add required columns\n    retry_queue['year'] = retry_queue['ceqNumber'].astype(str).str[:4]\n    retry_queue['localFilename'] = retry_queue.apply(\n        lambda row: build_local_filename(\n            row['ceqNumber'], \n            row['name'] or row['fileNameForDownload'] or f\"{row['attachmentId']}.pdf\"\n        ),\n        axis=1\n    )\n    retry_queue['sourcePath'] = retry_queue.apply(\n        lambda row: DOCUMENTS_DIR / row['year'] / row['localFilename'],\n        axis=1\n    )\n    retry_queue['outputPath'] = retry_queue.apply(\n        lambda row: OUTPUT_DIR / row['year'] / (row['localFilename'].replace('.pdf', '.txt').replace('.PDF', '.txt')),\n        axis=1\n    )\n    \n    # Run conversions\n    retry_results = run_conversions(retry_queue)\n    \n    # Update status\n    status_df = status_df[~status_df['attachmentId'].isin(retry_results['attachmentId'])]\n    combined_status = pd.concat([status_df, retry_results], ignore_index=True)\n    save_conversion_status(combined_status)\n    \n    success_count = retry_results['converted'].sum()\n    print(f\"Retry complete: {success_count}/{len(retry_results)} successful\")\n\n# Uncomment to retry failed conversions:\n# retry_failed_conversions()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Sample Output Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "# Inspect a sample converted file\ndef show_sample_output(n_chars: int = 2000):\n    \"\"\"\n    Display the beginning of a sample converted text file.\n    \"\"\"\n    txt_files = list(OUTPUT_DIR.glob(\"**/*.txt\"))\n    \n    if not txt_files:\n        print(\"No converted files found yet.\")\n        return\n    \n    # Pick a random file\n    import random\n    sample_file = random.choice(txt_files)\n    \n    print(f\"Sample file: {sample_file}\")\n    print(f\"File size: {sample_file.stat().st_size:,} bytes\")\n    print(\"=\" * 60)\n    \n    with open(sample_file, 'r', encoding='utf-8') as f:\n        content = f.read(n_chars)\n    \n    print(content)\n    if len(content) == n_chars:\n        print(f\"\\n... [truncated at {n_chars} chars]\")\n\n# Uncomment to see a sample:\n# show_sample_output()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}