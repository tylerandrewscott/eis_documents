{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch EIS Records via EPA E-NEPA API\n",
    "\n",
    "This notebook replaces the web scraping approach (`scrape_record_set_V2.R` and `scrape_record_details_V2.R`) with the official EPA E-NEPA API.\n",
    "\n",
    "**API Documentation:** https://cdxapps.epa.gov/cdx-enepa-II/apidocs/index.html\n",
    "\n",
    "## Outputs\n",
    "- `eis_record_api.parquet` - All EIS records with full metadata\n",
    "- `eis_document_record_api.parquet` - All document/attachment records\n",
    "\n",
    "These files are also saved as `.pkl` for easy Python reloading and can be converted to R-compatible formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install requests pandas pyarrow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_URL = \"https://cdxapps.epa.gov/cdx-enepa-II/rest\"\n",
    "SEARCH_ENDPOINT = f\"{BASE_URL}/public/v1/eis/search\"\n",
    "\n",
    "# Output paths - relative to repository root\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "METADATA_DIR = REPO_ROOT / \"metadata\"\n",
    "\n",
    "# Ensure metadata directory exists\n",
    "METADATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Output files\n",
    "EIS_RECORD_FILE = METADATA_DIR / \"eis_record_api.parquet\"\n",
    "EIS_RECORD_PKL = METADATA_DIR / \"eis_record_api.pkl\"\n",
    "DOC_RECORD_FILE = METADATA_DIR / \"eis_document_record_api.parquet\"\n",
    "DOC_RECORD_PKL = METADATA_DIR / \"eis_document_record_api.pkl\"\n",
    "\n",
    "# Year range for fetching records\n",
    "START_YEAR = 1987\n",
    "END_YEAR = datetime.now().year\n",
    "\n",
    "# Rate limiting\n",
    "REQUEST_DELAY = 0.5  # seconds between requests\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Metadata directory: {METADATA_DIR}\")\n",
    "print(f\"Will fetch records from {START_YEAR} to {END_YEAR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_eis_by_date_range(start_date: str, end_date: str, max_retries: int = 3) -> list:\n",
    "    \"\"\"\n",
    "    Search for EIS records within a date range.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date in MM/dd/yyyy format\n",
    "        end_date: End date in MM/dd/yyyy format\n",
    "        max_retries: Number of retry attempts on failure\n",
    "    \n",
    "    Returns:\n",
    "        List of EIS record dictionaries\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"startFRDate\": start_date,\n",
    "        \"endFRDate\": end_date\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(SEARCH_ENDPOINT, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1}/{max_retries} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                logger.error(f\"Failed to fetch records for {start_date} to {end_date}\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def search_eis_by_id(eis_id: str, max_retries: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Search for a specific EIS record by ID.\n",
    "    \n",
    "    Args:\n",
    "        eis_id: The EIS ID (8-digit number)\n",
    "        max_retries: Number of retry attempts on failure\n",
    "    \n",
    "    Returns:\n",
    "        EIS record dictionary or None\n",
    "    \"\"\"\n",
    "    params = {\"eisId\": eis_id}\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(SEARCH_ENDPOINT, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            results = response.json()\n",
    "            return results[0] if results else None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1}/{max_retries} failed for EIS {eis_id}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_eis_record(record: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Flatten a nested EIS record into a flat dictionary for DataFrame storage.\n",
    "    \n",
    "    Args:\n",
    "        record: Raw EIS record from API\n",
    "    \n",
    "    Returns:\n",
    "        Flattened dictionary\n",
    "    \"\"\"\n",
    "    flat = {\n",
    "        \"eisId\": record.get(\"eisId\"),\n",
    "        \"title\": record.get(\"title\"),\n",
    "        \"ceqNumber\": record.get(\"ceqNumber\"),\n",
    "        \"type\": record.get(\"type\"),\n",
    "        \"filedDate\": record.get(\"filedDate\"),\n",
    "        \"commentLetterDate\": record.get(\"commentLetterDate\"),\n",
    "        \"federalRegisterReportDate\": record.get(\"federalRegisterReportDate\"),\n",
    "        \"uniqueIdentificationNumber\": record.get(\"uniqueIdentificationNumber\"),\n",
    "        \"leadAgency\": record.get(\"leadAgency\"),\n",
    "        \"dueDate\": record.get(\"dueDate\"),\n",
    "        \"ammendedNoticeDate\": record.get(\"ammendedNoticeDate\"),\n",
    "        \"ammendedNoticeText\": record.get(\"ammendedNoticeText\"),\n",
    "        \"supplementalInformation\": record.get(\"supplementalInformation\"),\n",
    "        \"noticeOfIntent\": record.get(\"noticeOfIntent\"),\n",
    "        \"rating\": record.get(\"rating\"),\n",
    "        \"status\": record.get(\"status\"),\n",
    "    }\n",
    "    \n",
    "    # Extract states\n",
    "    states = record.get(\"states\", [])\n",
    "    flat[\"states\"] = \", \".join([s.get(\"name\", \"\") for s in states]) if states else None\n",
    "    flat[\"primaryState\"] = next((s.get(\"name\") for s in states if s.get(\"primary\")), None)\n",
    "    \n",
    "    # Extract cooperating agencies\n",
    "    coop_agencies = record.get(\"cooperatingAgency\", [])\n",
    "    flat[\"cooperatingAgencies\"] = \", \".join([a.get(\"name\", \"\") for a in coop_agencies]) if coop_agencies else None\n",
    "    \n",
    "    # Count attachments\n",
    "    attachments = record.get(\"attachments\", [])\n",
    "    flat[\"attachmentCount\"] = len(attachments)\n",
    "    \n",
    "    # Store raw JSON for attachments and zipLinkMetadata for later processing\n",
    "    flat[\"_attachments_json\"] = json.dumps(attachments) if attachments else None\n",
    "    flat[\"_zipLinkMetadata_json\"] = json.dumps(record.get(\"zipLinkMetadata\")) if record.get(\"zipLinkMetadata\") else None\n",
    "    \n",
    "    return flat\n",
    "\n",
    "\n",
    "def extract_attachments(record: dict) -> list:\n",
    "    \"\"\"\n",
    "    Extract attachment/document records from an EIS record.\n",
    "    \n",
    "    Args:\n",
    "        record: Raw EIS record from API\n",
    "    \n",
    "    Returns:\n",
    "        List of attachment dictionaries\n",
    "    \"\"\"\n",
    "    eis_id = record.get(\"eisId\")\n",
    "    attachments = record.get(\"attachments\", [])\n",
    "    \n",
    "    docs = []\n",
    "    for att in attachments:\n",
    "        doc = {\n",
    "            \"eisId\": eis_id,\n",
    "            \"attachmentId\": att.get(\"id\"),\n",
    "            \"name\": att.get(\"name\"),\n",
    "            \"title\": att.get(\"title\"),\n",
    "            \"fileNameForDownload\": att.get(\"fileNameForDownload\"),\n",
    "            \"type\": att.get(\"type\"),\n",
    "            \"size\": att.get(\"size\"),\n",
    "            \"sizeKb\": att.get(\"sizeKb\"),\n",
    "            \"pages\": att.get(\"pages\"),\n",
    "        }\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch All Records\n",
    "\n",
    "The API is queried by year to manage response sizes. Each year's results are accumulated and saved incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_records():\n",
    "    \"\"\"\n",
    "    Load existing records if available.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (eis_records_df, doc_records_df) or (None, None)\n",
    "    \"\"\"\n",
    "    eis_df = None\n",
    "    doc_df = None\n",
    "    \n",
    "    if EIS_RECORD_PKL.exists():\n",
    "        try:\n",
    "            eis_df = pd.read_pickle(EIS_RECORD_PKL)\n",
    "            logger.info(f\"Loaded {len(eis_df)} existing EIS records\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load existing EIS records: {e}\")\n",
    "    \n",
    "    if DOC_RECORD_PKL.exists():\n",
    "        try:\n",
    "            doc_df = pd.read_pickle(DOC_RECORD_PKL)\n",
    "            logger.info(f\"Loaded {len(doc_df)} existing document records\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load existing document records: {e}\")\n",
    "    \n",
    "    return eis_df, doc_df\n",
    "\n",
    "\n",
    "def save_records(eis_df: pd.DataFrame, doc_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Save records to both pickle and parquet formats.\n",
    "    \"\"\"\n",
    "    # Save pickle (fast Python serialization)\n",
    "    eis_df.to_pickle(EIS_RECORD_PKL)\n",
    "    doc_df.to_pickle(DOC_RECORD_PKL)\n",
    "    \n",
    "    # Save parquet (efficient, cross-platform)\n",
    "    eis_df.to_parquet(EIS_RECORD_FILE, index=False)\n",
    "    doc_df.to_parquet(DOC_RECORD_FILE, index=False)\n",
    "    \n",
    "    logger.info(f\"Saved {len(eis_df)} EIS records and {len(doc_df)} document records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_records(start_year: int = START_YEAR, end_year: int = END_YEAR, \n",
    "                      overwrite: bool = False, save_interval: int = 5):\n",
    "    \"\"\"\n",
    "    Fetch all EIS records from the API.\n",
    "    \n",
    "    Args:\n",
    "        start_year: First year to fetch\n",
    "        end_year: Last year to fetch\n",
    "        overwrite: If True, fetch all records regardless of existing data.\n",
    "                   If False, only fetch records not already in the database.\n",
    "        save_interval: Save progress every N years\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (eis_records_df, doc_records_df)\n",
    "    \"\"\"\n",
    "    # Load existing records\n",
    "    existing_eis_df, existing_doc_df = load_existing_records()\n",
    "    \n",
    "    if overwrite or existing_eis_df is None:\n",
    "        all_eis_records = []\n",
    "        all_doc_records = []\n",
    "        existing_ids = set()\n",
    "    else:\n",
    "        all_eis_records = existing_eis_df.to_dict('records')\n",
    "        all_doc_records = existing_doc_df.to_dict('records') if existing_doc_df is not None else []\n",
    "        existing_ids = set(existing_eis_df['eisId'].astype(str))\n",
    "        logger.info(f\"Starting with {len(existing_ids)} existing records\")\n",
    "    \n",
    "    years = list(range(end_year, start_year - 1, -1))  # Most recent first\n",
    "    \n",
    "    for i, year in enumerate(tqdm(years, desc=\"Fetching years\")):\n",
    "        start_date = f\"01/01/{year}\"\n",
    "        end_date = f\"12/31/{year}\"\n",
    "        \n",
    "        logger.info(f\"Fetching records for {year}...\")\n",
    "        records = search_eis_by_date_range(start_date, end_date)\n",
    "        \n",
    "        new_count = 0\n",
    "        for record in records:\n",
    "            eis_id = str(record.get(\"eisId\"))\n",
    "            \n",
    "            if not overwrite and eis_id in existing_ids:\n",
    "                continue\n",
    "            \n",
    "            # Flatten and store EIS record\n",
    "            flat_record = flatten_eis_record(record)\n",
    "            all_eis_records.append(flat_record)\n",
    "            existing_ids.add(eis_id)\n",
    "            \n",
    "            # Extract and store attachments\n",
    "            attachments = extract_attachments(record)\n",
    "            all_doc_records.extend(attachments)\n",
    "            \n",
    "            new_count += 1\n",
    "        \n",
    "        logger.info(f\"Year {year}: Found {len(records)} records, {new_count} new\")\n",
    "        \n",
    "        # Save progress periodically\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            eis_df = pd.DataFrame(all_eis_records)\n",
    "            doc_df = pd.DataFrame(all_doc_records)\n",
    "            save_records(eis_df, doc_df)\n",
    "        \n",
    "        time.sleep(REQUEST_DELAY)\n",
    "    \n",
    "    # Final save\n",
    "    eis_df = pd.DataFrame(all_eis_records)\n",
    "    doc_df = pd.DataFrame(all_doc_records)\n",
    "    save_records(eis_df, doc_df)\n",
    "    \n",
    "    return eis_df, doc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Fetch Configuration\n\nConfigure the fetch settings before running."
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# FETCH SETTINGS - MODIFY AS NEEDED\n# ============================================\n\n# Set to True to re-fetch ALL records from scratch (overwrites existing data)\n# Set to False to only fetch NEW records not already in the database (incremental update)\nOVERWRITE = False\n\n# Year range to fetch\n# Modify these to limit the fetch to specific years\nFETCH_START_YEAR = 1987  # Earliest year in the database\nFETCH_END_YEAR = datetime.now().year  # Current year\n\n# How often to save progress (every N years)\n# Lower values = more frequent saves, safer but slightly slower\nSAVE_INTERVAL = 5\n\nprint(f\"=== Fetch Configuration ===\")\nprint(f\"  OVERWRITE: {OVERWRITE}\")\nprint(f\"  Year range: {FETCH_START_YEAR} to {FETCH_END_YEAR}\")\nprint(f\"  Save interval: every {SAVE_INTERVAL} years\")\nprint()\nif OVERWRITE:\n    print(\"WARNING: OVERWRITE=True will re-fetch ALL records and replace existing data!\")\nelse:\n    print(\"Mode: Incremental update (will skip existing records)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run the Fetch\n\nExecute the cell below to start fetching records. Progress will be saved incrementally.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run the fetch using configuration settings above\neis_df, doc_df = fetch_all_records(\n    start_year=FETCH_START_YEAR,\n    end_year=FETCH_END_YEAR,\n    overwrite=OVERWRITE,\n    save_interval=SAVE_INTERVAL\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Total EIS records: {len(eis_df)}\")\n",
    "print(f\"Total document records: {len(doc_df)}\")\n",
    "print(f\"\\nEIS records by year:\")\n",
    "eis_df['year'] = eis_df['eisId'].astype(str).str[:4]\n",
    "print(eis_df['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview EIS records\n",
    "print(\"\\n=== EIS Records Sample ===\")\n",
    "display(eis_df[['eisId', 'title', 'leadAgency', 'states', 'federalRegisterReportDate', 'attachmentCount']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview document records\n",
    "print(\"\\n=== Document Records Sample ===\")\n",
    "display(doc_df[['eisId', 'attachmentId', 'name', 'type', 'sizeKb', 'pages']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV (Optional)\n",
    "\n",
    "For compatibility with R or other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "eis_df.to_csv(METADATA_DIR / \"eis_record_api.csv\", index=False)\n",
    "doc_df.to_csv(METADATA_DIR / \"eis_document_record_api.csv\", index=False)\n",
    "print(f\"Exported to CSV files in {METADATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility: Fetch Single Record by ID\n",
    "\n",
    "Useful for checking specific records or debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fetch a single record by EIS ID\n",
    "# record = search_eis_by_id(\"20240001\")\n",
    "# if record:\n",
    "#     print(json.dumps(record, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}